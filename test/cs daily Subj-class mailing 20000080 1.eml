Date: Thu, 7 Nov 2024 00:45:57 -0500
Message-Id: <202411070545.4A75jvOf1666827@lib-arxiv-039.serverfarm.cornell.edu>
From: send mail ONLY to cs <no-reply@arXiv.org>
Reply-To: cs@arXiv.org
To: cs daily title/abstract distribution <rabble@arXiv.org>
List-ID: <cs.daily.arXiv.org>
List-Help: <https://info.arxiv.org/help/subscribe.html>
List-Unsubscribe: <mailto:cs@arXiv.org?subject=cancel>
Precedence: bulk
Subject: cs daily Subj-class mailing 20000080 1

------------------------------------------------------------------------------
------------------------------------------------------------------------------
Send any comments regarding submissions directly to submitter.
------------------------------------------------------------------------------
Archives at http://arxiv.org/
To unsubscribe, e-mail To: cs@arXiv.org, Subject: cancel
------------------------------------------------------------------------------
 Submissions to:
Computer Vision and Pattern Recognition
Robotics
 received from  Tue  5 Nov 24 19:00:00 GMT  to  Wed  6 Nov 24 19:00:00 GMT
------------------------------------------------------------------------------
------------------------------------------------------------------------------
\\
arXiv:2411.03340
Date: Sat, 2 Nov 2024 00:16:29 GMT   (1531kb,D)

Title: Unlocking the Archives: Using Large Language Models to Transcribe
  Handwritten Historical Documents
Authors: Mark Humphries, Lianne C. Leddy, Quinn Downton, Meredith Legace, John
  McConnell, Isabella Murray, and Elizabeth Spence
Categories: cs.CV cs.CL cs.DL cs.LG
Comments: 29 Pages, 11 Tables, 2 Figures
\\
  This study demonstrates that Large Language Models (LLMs) can transcribe
historical handwritten documents with significantly higher accuracy than
specialized Handwritten Text Recognition (HTR) software, while being faster and
more cost-effective. We introduce an open-source software tool called
Transcription Pearl that leverages these capabilities to automatically
transcribe and correct batches of handwritten documents using commercially
available multimodal LLMs from OpenAI, Anthropic, and Google. In tests on a
diverse corpus of 18th/19th century English language handwritten documents,
LLMs achieved Character Error Rates (CER) of 5.7 to 7% and Word Error Rates
(WER) of 8.9 to 15.9%, improvements of 14% and 32% respectively over
specialized state-of-the-art HTR software like Transkribus. Most significantly,
when LLMs were then used to correct those transcriptions as well as texts
generated by conventional HTR software, they achieved near-human levels of
accuracy, that is CERs as low as 1.8% and WERs of 3.5%. The LLMs also completed
these tasks 50 times faster and at approximately 1/50th the cost of proprietary
HTR programs. These results demonstrate that when LLMs are incorporated into
software tools like Transcription Pearl, they provide an accessible, fast, and
highly accurate method for mass transcription of historical handwritten
documents, significantly streamlining the digitization process.
\\ ( https://arxiv.org/abs/2411.03340 ,  1531kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03359
Date: Tue, 5 Nov 2024 02:29:16 GMT   (395kb,D)

Title: Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution
  Detection
Authors: Geng Yu, Jianing Zhu, Jiangchao Yao, Bo Han
Categories: cs.CV cs.AI cs.LG
Comments: accepted by NeurIPS 2024
\\
  Out-of-distribution (OOD) detection is crucial for deploying reliable machine
learning models in open-world applications. Recent advances in CLIP-based OOD
detection have shown promising results via regularizing prompt tuning with OOD
features extracted from ID data. However, the irrelevant context mined from ID
data can be spurious due to the inaccurate foreground-background decomposition,
thus limiting the OOD detection performance. In this work, we propose a novel
framework, namely, Self-Calibrated Tuning (SCT), to mitigate this problem for
effective OOD detection with only the given few-shot ID data. Specifically, SCT
introduces modulating factors respectively on the two components of the
original learning objective. It adaptively directs the optimization process
between the two tasks during training on data with different prediction
uncertainty to calibrate the influence of OOD regularization, which is
compatible with many prompt tuning based OOD detection methods. Extensive
experiments and analyses have been conducted to characterize and demonstrate
the effectiveness of the proposed SCT. The code is publicly available.
\\ ( https://arxiv.org/abs/2411.03359 ,  395kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03403
Date: Tue, 5 Nov 2024 18:38:42 GMT   (32112kb,D)

Title: Enhancing Maritime Situational Awareness through End-to-End Onboard Raw
  Data Analysis
Authors: Roberto Del Prete, Manuel Salvoldi, Domenico Barretta, Nicolas
  Long\'ep\'e, Gabriele Meoni, Arnon Karnieli, Maria Daniela Graziano and
  Alfredo Renga
Categories: cs.CV
Comments: 38 pages
\\
  Satellite-based onboard data processing is crucial for time-sensitive
applications requiring timely and efficient rapid response. Advances in edge
artificial intelligence are shifting computational power from ground-based
centers to on-orbit platforms, transforming the
"sensing-communication-decision-feedback" cycle and reducing latency from
acquisition to delivery. The current research presents a framework addressing
the strict bandwidth, energy, and latency constraints of small satellites,
focusing on maritime monitoring. The study contributes three main innovations.
Firstly, it investigates the application of deep learning techniques for direct
ship detection and classification from raw satellite imagery. By simplifying
the onboard processing chain, our approach facilitates direct analyses without
requiring computationally intensive steps such as calibration and
ortho-rectification. Secondly, to address the scarcity of raw satellite data,
we introduce two novel datasets, VDS2Raw and VDV2Raw, which are derived from
raw data from Sentinel-2 and Vegetation and Environment Monitoring New Micro
Satellite (VENuS) missions, respectively, and enriched with Automatic
Identification System (AIS) records. Thirdly, we characterize the tasks'
optimal single and multiple spectral band combinations through statistical and
feature-based analyses validated on both datasets. In sum, we demonstrate the
feasibility of the proposed method through a proof-of-concept on CubeSat-like
hardware, confirming the models' potential for operational satellite-based
maritime monitoring.
\\ ( https://arxiv.org/abs/2411.03403 ,  32112kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03405
Date: Tue, 5 Nov 2024 18:39:25 GMT   (4462kb,D)

Title: Fine-Grained Spatial and Verbal Losses for 3D Visual Grounding
Authors: Sombit Dey, Ozan Unal, Christos Sakaridis, Luc Van Gool
Categories: cs.CV
Comments: Accepted at WACV 2025
\\
  3D visual grounding consists of identifying the instance in a 3D scene which
is referred by an accompanying language description. While several
architectures have been proposed within the commonly employed
grounding-by-selection framework, the utilized losses are comparatively
under-explored. In particular, most methods rely on a basic supervised
cross-entropy loss on the predicted distribution over candidate instances,
which fails to model both spatial relations between instances and the internal
fine-grained word-level structure of the verbal referral. Sparse attempts to
additionally supervise verbal embeddings globally by learning the class of the
referred instance from the description or employing verbo-visual contrast to
better separate instance embeddings do not fundamentally lift the
aforementioned limitations. Responding to these shortcomings, we introduce two
novel losses for 3D visual grounding: a visual-level offset loss on regressed
vector offsets from each instance to the ground-truth referred instance and a
language-related span loss on predictions for the word-level span of the
referred instance in the description. In addition, we equip the verbo-visual
fusion module of our new 3D visual grounding architecture AsphaltNet with a
top-down bidirectional attentive fusion block, which enables the supervisory
signals from our two losses to propagate to the respective converse branches of
the network and thus aid the latter to learn context-aware instance embeddings
and grounding-aware verbal embeddings. AsphaltNet proposes novel auxiliary
losses to aid 3D visual grounding with competitive results compared to the
state-of-the-art on the ReferIt3D benchmark.
\\ ( https://arxiv.org/abs/2411.03405 ,  4462kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03475
Date: Tue, 5 Nov 2024 19:59:40 GMT   (21248kb,D)

Title: Self Supervised Networks for Learning Latent Space Representations of
  Human Body Scans and Motions
Authors: Emmanuel Hartman, Nicolas Charon, Martin Bauer
Categories: cs.CV
Comments: 23 pages, 11 figures, 6 tables
\\
  This paper introduces self-supervised neural network models to tackle several
fundamental problems in the field of 3D human body analysis and processing.
First, we propose VariShaPE (Varifold Shape Parameter Estimator), a novel
architecture for the retrieval of latent space representations of body shapes
and poses. This network offers a fast and robust method to estimate the
embedding of arbitrary unregistered meshes into the latent space. Second, we
complement the estimation of latent codes with MoGeN (Motion Geometry Network)
a framework that learns the geometry on the latent space itself. This is
achieved by lifting the body pose parameter space into a higher dimensional
Euclidean space in which body motion mini-sequences from a training set of 4D
data can be approximated by simple linear interpolation. Using the SMPL latent
space representation we illustrate how the combination of these network models,
once trained, can be used to perform a variety of tasks with very limited
computational cost. This includes operations such as motion interpolation,
extrapolation and transfer as well as random shape and pose generation.
\\ ( https://arxiv.org/abs/2411.03475 ,  21248kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03480
Date: Tue, 5 Nov 2024 20:06:50 GMT   (6801kb,D)

Title: Rainfall regression from C-band Synthetic Aperture Radar using
  Multi-Task Generative Adversarial Networks
Authors: Aur\'elien Colin, Romain Husson
Categories: cs.CV
Comments: 36 pages, 13 figures
\\
  This paper introduces a data-driven approach to estimate precipitation rates
from Synthetic Aperture Radar (SAR) at a spatial resolution of 200 meters per
pixel. It addresses previous challenges related to the collocation of SAR and
weather radar data, specifically the misalignment in collocations and the
scarcity of rainfall examples under strong wind. To tackle these challenges,
the paper proposes a multi-objective formulation, introducing patch-level
components and an adversarial component. It exploits the full NEXRAD archive to
look for potential co-locations with Sentinel-1 data. With additional
enhancements to the training procedure and the incorporation of additional
inputs, the resulting model demonstrates improved accuracy in rainfall
estimates and the ability to extend its performance to scenarios up to 15 m/s.
\\ ( https://arxiv.org/abs/2411.03480 ,  6801kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03491
Date: Tue, 5 Nov 2024 20:16:15 GMT   (2927kb)

Title: An Application-Agnostic Automatic Target Recognition System Using Vision
  Language Models
Authors: Anthony Palladino, Dana Gajewski, Abigail Aronica, Patryk Deptula,
  Alexander Hamme, Seiyoung C. Lee, Jeff Muri, Todd Nelling, Michael A. Riley,
  Brian Wong, Margaret Duff
Categories: cs.CV
Comments: Accepted to the Thirty-Seventh Annual Conference on Innovative
  Applications of Artificial Intelligence (IAAI-25)
\\
  We present a novel Automatic Target Recognition (ATR) system using
open-vocabulary object detection and classification models. A primary advantage
of this approach is that target classes can be defined just before runtime by a
non-technical end user, using either a few natural language text descriptions
of the target, or a few image exemplars, or both. Nuances in the desired
targets can be expressed in natural language, which is useful for unique
targets with little or no training data. We also implemented a novel
combination of several techniques to improve performance, such as leveraging
the additional information in the sequence of overlapping frames to perform
tubelet identification (i.e., sequential bounding box matching), bounding box
re-scoring, and tubelet linking. Additionally, we developed a technique to
visualize the aggregate output of many overlapping frames as a mosaic of the
area scanned during the aerial surveillance or reconnaissance, and a kernel
density estimate (or heatmap) of the detected targets. We initially applied
this ATR system to the use case of detecting and clearing unexploded ordinance
on airfield runways and we are currently extending our research to other
real-world applications.
\\ ( https://arxiv.org/abs/2411.03491 ,  2927kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03505
Date: Tue, 5 Nov 2024 20:42:23 GMT   (6617kb,D)

Title: SynthSet: Generative Diffusion Model for Semantic Segmentation in
  Precision Agriculture
Authors: Andrew Heschl, Mauricio Murillo, Keyhan Najafian, Farhad Maleki
Categories: cs.CV
\\
  This paper introduces a methodology for generating synthetic annotated data
to address data scarcity in semantic segmentation tasks within the precision
agriculture domain. Utilizing Denoising Diffusion Probabilistic Models (DDPMs)
and Generative Adversarial Networks (GANs), we propose a dual diffusion model
architecture for synthesizing realistic annotated agricultural data, without
any human intervention. We employ super-resolution to enhance the phenotypic
characteristics of the synthesized images and their coherence with the
corresponding generated masks. We showcase the utility of the proposed method
for wheat head segmentation. The high quality of synthesized data underscores
the effectiveness of the proposed methodology in generating image-mask pairs.
Furthermore, models trained on our generated data exhibit promising performance
when tested on an external, diverse dataset of real wheat fields. The results
show the efficacy of the proposed methodology for addressing data scarcity for
semantic segmentation tasks. Moreover, the proposed approach can be readily
adapted for various segmentation tasks in precision agriculture and beyond.
\\ ( https://arxiv.org/abs/2411.03505 ,  6617kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03511
Date: Tue, 5 Nov 2024 21:08:19 GMT   (21000kb,D)

Title: Beyond Complete Shapes: A quantitative Evaluation of 3D Shape Matching
  Algorithms
Authors: Viktoria Ehm, Nafie El Amrani, Yizheng Xie, Lennart Bastian, Maolin
  Gao, Weikang Wang, Lu Sang, Dongliang Cao, Zorah L\"ahner, Daniel Cremers,
  Florian Bernard
Categories: cs.CV
\\
  Finding correspondences between 3D shapes is an important and long-standing
problem in computer vision, graphics and beyond. While approaches based on
machine learning dominate modern 3D shape matching, almost all existing
(learning-based) methods require that at least one of the involved shapes is
complete. In contrast, the most challenging and arguably most practically
relevant setting of matching partially observed shapes, is currently
underexplored. One important factor is that existing datasets contain only a
small number of shapes (typically below 100), which are unable to serve
data-hungry machine learning approaches, particularly in the unsupervised
regime. In addition, the type of partiality present in existing datasets is
often artificial and far from realistic. To address these limitations and to
encourage research on these relevant settings, we provide a generic and
flexible framework for the procedural generation of challenging partial shape
matching scenarios. Our framework allows for a virtually infinite generation of
partial shape matching instances from a finite set of shapes with complete
geometry. Further, we manually create cross-dataset correspondences between
seven existing (complete geometry) shape matching datasets, leading to a total
of 2543 shapes. Based on this, we propose several challenging partial benchmark
settings, for which we evaluate respective state-of-the-art methods as
baselines.
\\ ( https://arxiv.org/abs/2411.03511 ,  21000kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03531
Date: Tue, 5 Nov 2024 22:14:35 GMT   (7020kb,D)

Title: Personalized Video Summarization by Multimodal Video Understanding
Authors: Brian Chen, Xiangyuan Zhao, Yingnan Zhu
Categories: cs.CV cs.AI
Comments: In Proceedings of CIKM 2024 Applied Research Track
Journal-ref: 33rd ACM International Conference on Information and Knowledge
  Management (CIKM 2024)
DOI: 10.1145/3627673.3680011
\\
  Video summarization techniques have been proven to improve the overall user
experience when it comes to accessing and comprehending video content. If the
user's preference is known, video summarization can identify significant
information or relevant content from an input video, aiding them in obtaining
the necessary information or determining their interest in watching the
original video. Adapting video summarization to various types of video and user
preferences requires significant training data and expensive human labeling. To
facilitate such research, we proposed a new benchmark for video summarization
that captures various user preferences. Also, we present a pipeline called
Video Summarization with Language (VSL) for user-preferred video summarization
that is based on pre-trained visual language models (VLMs) to avoid the need to
train a video summarization system on a large training dataset. The pipeline
takes both video and closed captioning as input and performs semantic analysis
at the scene level by converting video frames into text. Subsequently, the
user's genre preference was used as the basis for selecting the pertinent
textual scenes. The experimental results demonstrate that our proposed pipeline
outperforms current state-of-the-art unsupervised video summarization models.
We show that our method is more adaptable across different datasets compared to
supervised query-based video summarization models. In the end, the runtime
analysis demonstrates that our pipeline is more suitable for practical use when
scaling up the number of user preferences and videos.
\\ ( https://arxiv.org/abs/2411.03531 ,  7020kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03554
Date: Tue, 5 Nov 2024 23:26:10 GMT   (29175kb,D)

Title: Benchmarking Vision Language Model Unlearning via Fictitious Facial
  Identity Dataset
Authors: Yingzi Ma, Jiongxiao Wang, Fei Wang, Siyuan Ma, Jiazhao Li, Xiujun Li,
  Furong Huang, Lichao Sun, Bo Li, Yejin Choi, Muhao Chen, Chaowei Xiao
Categories: cs.CV
\\
  Machine unlearning has emerged as an effective strategy for forgetting
specific information in the training data. However, with the increasing
integration of visual data, privacy concerns in Vision Language Models (VLMs)
remain underexplored. To address this, we introduce Facial Identity Unlearning
Benchmark (FIUBench), a novel VLM unlearning benchmark designed to robustly
evaluate the effectiveness of unlearning algorithms under the Right to be
Forgotten setting. Specifically, we formulate the VLM unlearning task via
constructing the Fictitious Facial Identity VQA dataset and apply a two-stage
evaluation pipeline that is designed to precisely control the sources of
information and their exposure levels. In terms of evaluation, since VLM
supports various forms of ways to ask questions with the same semantic meaning,
we also provide robust evaluation metrics including membership inference
attacks and carefully designed adversarial privacy attacks to evaluate the
performance of algorithms. Through the evaluation of four baseline VLM
unlearning algorithms within FIUBench, we find that all methods remain limited
in their unlearning performance, with significant trade-offs between model
utility and forget quality. Furthermore, our findings also highlight the
importance of privacy attacks for robust evaluations. We hope FIUBench will
drive progress in developing more effective VLM unlearning algorithms.
\\ ( https://arxiv.org/abs/2411.03554 ,  29175kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03555
Date: Tue, 5 Nov 2024 23:28:57 GMT   (1707kb,D)

Title: Object and Contact Point Tracking in Demonstrations Using 3D Gaussian
  Splatting
Authors: Michael B\"uttner, Jonathan Francis, Helge Rhodin, Andrew Melnik
Categories: cs.CV cs.RO
Comments: CoRL 2024, Workshop on Lifelong Learning for Home Robots, Munich,
  Germany
\\
  This paper introduces a method to enhance Interactive Imitation Learning
(IIL) by extracting touch interaction points and tracking object movement from
video demonstrations. The approach extends current IIL systems by providing
robots with detailed knowledge of both where and how to interact with objects,
particularly complex articulated ones like doors and drawers. By leveraging
cutting-edge techniques such as 3D Gaussian Splatting and FoundationPose for
tracking, this method allows robots to better understand and manipulate objects
in dynamic environments. The research lays the foundation for more effective
task learning and execution in autonomous robotic systems.
\\ ( https://arxiv.org/abs/2411.03555 ,  1707kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03561
Date: Tue, 5 Nov 2024 23:53:19 GMT   (28736kb,D)

Title: Estimating Ego-Body Pose from Doubly Sparse Egocentric Video Data
Authors: Seunggeun Chi, Pin-Hao Huang, Enna Sachdeva, Hengbo Ma, Karthik
  Ramani, and Kwonjoon Lee
Categories: cs.CV
Comments: Accepted at NeurIPS 2024
\\
  We study the problem of estimating the body movements of a camera wearer from
egocentric videos. Current methods for ego-body pose estimation rely on
temporally dense sensor data, such as IMU measurements from spatially sparse
body parts like the head and hands. However, we propose that even temporally
sparse observations, such as hand poses captured intermittently from egocentric
videos during natural or periodic hand movements, can effectively constrain
overall body motion. Naively applying diffusion models to generate full-body
pose from head pose and sparse hand pose leads to suboptimal results. To
overcome this, we develop a two-stage approach that decomposes the problem into
temporal completion and spatial completion. First, our method employs masked
autoencoders to impute hand trajectories by leveraging the spatiotemporal
correlations between the head pose sequence and intermittent hand poses,
providing uncertainty estimates. Subsequently, we employ conditional diffusion
models to generate plausible full-body motions based on these temporally dense
trajectories of the head and hands, guided by the uncertainty estimates from
the imputation. The effectiveness of our method was rigorously tested and
validated through comprehensive experiments conducted on various HMD setup with
AMASS and Ego-Exo4D datasets.
\\ ( https://arxiv.org/abs/2411.03561 ,  28736kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03576
Date: Wed, 6 Nov 2024 00:34:26 GMT   (1879kb,D)

Title: Hybrid Attention for Robust RGB-T Pedestrian Detection in Real-World
  Conditions
Authors: Arunkumar Rathinam, Leo Pauly, Abd El Rahman Shabayek, Wassim
  Rharbaoui, Anis Kacem, Vincent Gaudilli\`ere, Djamila Aouada
Categories: cs.CV cs.AI
Comments: Accepted for publication in IEEE Robotics and Automation Letters,
  October 2024
\\
  Multispectral pedestrian detection has gained significant attention in recent
years, particularly in autonomous driving applications. To address the
challenges posed by adversarial illumination conditions, the combination of
thermal and visible images has demonstrated its advantages. However, existing
fusion methods rely on the critical assumption that the RGB-Thermal (RGB-T)
image pairs are fully overlapping. These assumptions often do not hold in
real-world applications, where only partial overlap between images can occur
due to sensors configuration. Moreover, sensor failure can cause loss of
information in one modality. In this paper, we propose a novel module called
the Hybrid Attention (HA) mechanism as our main contribution to mitigate
performance degradation caused by partial overlap and sensor failure, i.e. when
at least part of the scene is acquired by only one sensor. We propose an
improved RGB-T fusion algorithm, robust against partial overlap and sensor
failure encountered during inference in real-world applications. We also
leverage a mobile-friendly backbone to cope with resource constraints in
embedded systems. We conducted experiments by simulating various partial
overlap and sensor failure scenarios to evaluate the performance of our
proposed method. The results demonstrate that our approach outperforms
state-of-the-art methods, showcasing its superiority in handling real-world
challenges.
\\ ( https://arxiv.org/abs/2411.03576 ,  1879kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03628
Date: Wed, 6 Nov 2024 02:50:30 GMT   (45225kb,D)

Title: StreamingBench: Assessing the Gap for MLLMs to Achieve Streaming Video
  Understanding
Authors: Junming Lin, Zheng Fang, Chi Chen, Zihao Wan, Fuwen Luo, Peng Li, Yang
  Liu, Maosong Sun
Categories: cs.CV cs.AI
\\
  The rapid development of Multimodal Large Language Models (MLLMs) has
expanded their capabilities from image comprehension to video understanding.
However, most of these MLLMs focus primarily on offline video comprehension,
necessitating extensive processing of all video frames before any queries can
be made. This presents a significant gap compared to the human ability to
watch, listen, think, and respond to streaming inputs in real time,
highlighting the limitations of current MLLMs. In this paper, we introduce
StreamingBench, the first comprehensive benchmark designed to evaluate the
streaming video understanding capabilities of MLLMs. StreamingBench assesses
three core aspects of streaming video understanding: (1) real-time visual
understanding, (2) omni-source understanding, and (3) contextual understanding.
The benchmark consists of 18 tasks, featuring 900 videos and 4,500
human-curated QA pairs. Each video features five questions presented at
different time points to simulate a continuous streaming scenario. We conduct
experiments on StreamingBench with 13 open-source and proprietary MLLMs and
find that even the most advanced proprietary MLLMs like Gemini 1.5 Pro and
GPT-4o perform significantly below human-level streaming video understanding
capabilities. We hope our work can facilitate further advancements for MLLMs,
empowering them to approach human-level video comprehension and interaction in
more realistic scenarios.
\\ ( https://arxiv.org/abs/2411.03628 ,  45225kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03637
Date: Wed, 6 Nov 2024 03:28:06 GMT   (6969kb,D)

Title: Structure Consistent Gaussian Splatting with Matching Prior for Few-shot
  Novel View Synthesis
Authors: Rui Peng, Wangze Xu, Luyang Tang, Liwei Liao, Jianbo Jiao, Ronggang
  Wang
Categories: cs.CV
Comments: NeurIPS 2024 Accepted
\\
  Despite the substantial progress of novel view synthesis, existing methods,
either based on the Neural Radiance Fields (NeRF) or more recently 3D Gaussian
Splatting (3DGS), suffer significant degradation when the input becomes sparse.
Numerous efforts have been introduced to alleviate this problem, but they still
struggle to synthesize satisfactory results efficiently, especially in the
large scene. In this paper, we propose SCGaussian, a Structure Consistent
Gaussian Splatting method using matching priors to learn 3D consistent scene
structure. Considering the high interdependence of Gaussian attributes, we
optimize the scene structure in two folds: rendering geometry and, more
importantly, the position of Gaussian primitives, which is hard to be directly
constrained in the vanilla 3DGS due to the non-structure property. To achieve
this, we present a hybrid Gaussian representation. Besides the ordinary
non-structure Gaussian primitives, our model also consists of ray-based
Gaussian primitives that are bound to matching rays and whose optimization of
their positions is restricted along the ray. Thus, we can utilize the matching
correspondence to directly enforce the position of these Gaussian primitives to
converge to the surface points where rays intersect. Extensive experiments on
forward-facing, surrounding, and complex large scenes show the effectiveness of
our approach with state-of-the-art performance and high efficiency. Code is
available at https://github.com/prstrive/SCGaussian.
\\ ( https://arxiv.org/abs/2411.03637 ,  6969kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03638
Date: Wed, 6 Nov 2024 03:30:46 GMT   (1696kb,D)

Title: Adaptive Stereo Depth Estimation with Multi-Spectral Images Across All
  Lighting Conditions
Authors: Zihan Qin, Jialei Xu, Wenbo Zhao, Junjun Jiang, Xianming Liu
Categories: cs.CV cs.AI
\\
  Depth estimation under adverse conditions remains a significant challenge.
Recently, multi-spectral depth estimation, which integrates both visible light
and thermal images, has shown promise in addressing this issue. However,
existing algorithms struggle with precise pixel-level feature matching,
limiting their ability to fully exploit geometric constraints across different
spectra. To address this, we propose a novel framework incorporating stereo
depth estimation to enforce accurate geometric constraints. In particular, we
treat the visible light and thermal images as a stereo pair and utilize a
Cross-modal Feature Matching (CFM) Module to construct a cost volume for
pixel-level matching. To mitigate the effects of poor lighting on stereo
matching, we introduce Degradation Masking, which leverages robust monocular
thermal depth estimation in degraded regions. Our method achieves
state-of-the-art (SOTA) performance on the Multi-Spectral Stereo (MS2) dataset,
with qualitative evaluations demonstrating high-quality depth maps under
varying lighting conditions.
\\ ( https://arxiv.org/abs/2411.03638 ,  1696kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03670
Date: Wed, 6 Nov 2024 05:09:34 GMT   (22410kb,D)

Title: Touchstone Benchmark: Are We on the Right Way for Evaluating AI
  Algorithms for Medical Segmentation?
Authors: Pedro R. A. S. Bassi, Wenxuan Li, Yucheng Tang, Fabian Isensee, Zifu
  Wang, Jieneng Chen, Yu-Cheng Chou, Yannick Kirchhoff, Maximilian Rokuss,
  Ziyan Huang, Jin Ye, Junjun He, Tassilo Wald, Constantin Ulrich, Michael
  Baumgartner, Saikat Roy, Klaus H. Maier-Hein, Paul Jaeger, Yiwen Ye, Yutong
  Xie, Jianpeng Zhang, Ziyang Chen, Yong Xia, Zhaohu Xing, Lei Zhu, Yousef
  Sadegheih, Afshin Bozorgpour, Pratibha Kumari, Reza Azad, Dorit Merhof,
  Pengcheng Shi, Ting Ma, Yuxin Du, Fan Bai, Tiejun Huang, Bo Zhao, Haonan
  Wang, Xiaomeng Li, Hanxue Gu, Haoyu Dong, Jichen Yang, Maciej A. Mazurowski,
  Saumya Gupta, Linshan Wu, Jiaxin Zhuang, Hao Chen, Holger Roth, Daguang Xu,
  Matthew B. Blaschko, Sergio Decherchi, Andrea Cavalli, Alan L. Yuille,
  Zongwei Zhou
Categories: cs.CV cs.AI
Comments: Accepted to NeurIPS-2024
\\
  How can we test AI performance? This question seems trivial, but it isn't.
Standard benchmarks often have problems such as in-distribution and small-size
test sets, oversimplified metrics, unfair comparisons, and short-term outcome
pressure. As a consequence, good performance on standard benchmarks does not
guarantee success in real-world scenarios. To address these problems, we
present Touchstone, a large-scale collaborative segmentation benchmark of 9
types of abdominal organs. This benchmark is based on 5,195 training CT scans
from 76 hospitals around the world and 5,903 testing CT scans from 11
additional hospitals. This diverse test set enhances the statistical
significance of benchmark results and rigorously evaluates AI algorithms across
various out-of-distribution scenarios. We invited 14 inventors of 19 AI
algorithms to train their algorithms, while our team, as a third party,
independently evaluated these algorithms on three test sets. In addition, we
also evaluated pre-existing AI frameworks--which, differing from algorithms,
are more flexible and can support different algorithms--including MONAI from
NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are
committed to expanding this benchmark to encourage more innovation of AI
algorithms for the medical domain.
\\ ( https://arxiv.org/abs/2411.03670 ,  22410kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03672
Date: Wed, 6 Nov 2024 05:11:25 GMT   (1611kb)

Title: Towards 3D Semantic Scene Completion for Autonomous Driving: A
  Meta-Learning Framework Empowered by Deformable Large-Kernel Attention and
  Mamba Model
Authors: Yansong Qu, Zilin Huang, Zihao Sheng, Tiantian Chen, Sikai Chen
Categories: cs.CV cs.AI
\\
  Semantic scene completion (SSC) is essential for achieving comprehensive
perception in autonomous driving systems. However, existing SSC methods often
overlook the high deployment costs in real-world applications. Traditional
architectures, such as 3D Convolutional Neural Networks (3D CNNs) and
self-attention mechanisms, face challenges in efficiently capturing long-range
dependencies within 3D voxel grids, limiting their effectiveness. To address
these issues, we introduce MetaSSC, a novel meta-learning-based framework for
SSC that leverages deformable convolution, large-kernel attention, and the
Mamba (D-LKA-M) model. Our approach begins with a voxel-based semantic
segmentation (SS) pretraining task, aimed at exploring the semantics and
geometry of incomplete regions while acquiring transferable meta-knowledge.
Using simulated cooperative perception datasets, we supervise the perception
training of a single vehicle using aggregated sensor data from multiple nearby
connected autonomous vehicles (CAVs), generating richer and more comprehensive
labels. This meta-knowledge is then adapted to the target domain through a
dual-phase training strategy that does not add extra model parameters, enabling
efficient deployment. To further enhance the model's capability in capturing
long-sequence relationships within 3D voxel grids, we integrate Mamba blocks
with deformable convolution and large-kernel attention into the backbone
network. Extensive experiments demonstrate that MetaSSC achieves
state-of-the-art performance, significantly outperforming competing models
while also reducing deployment costs.
\\ ( https://arxiv.org/abs/2411.03672 ,  1611kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03688
Date: Wed, 6 Nov 2024 06:14:24 GMT   (6799kb,D)

Title: Where Do We Stand with Implicit Neural Representations? A Technical and
  Performance Survey
Authors: Amer Essakine, Yanqi Cheng, Chun-Wun Cheng, Lipei Zhang, Zhongying
  Deng, Lei Zhu, Carola-Bibiane Sch\"onlieb, Angelica I Aviles-Rivero
Categories: cs.CV
\\
  Implicit Neural Representations (INRs) have emerged as a paradigm in
knowledge representation, offering exceptional flexibility and performance
across a diverse range of applications. INRs leverage multilayer perceptrons
(MLPs) to model data as continuous implicit functions, providing critical
advantages such as resolution independence, memory efficiency, and
generalisation beyond discretised data structures. Their ability to solve
complex inverse problems makes them particularly effective for tasks including
audio reconstruction, image representation, 3D object reconstruction, and
high-dimensional data synthesis. This survey provides a comprehensive review of
state-of-the-art INR methods, introducing a clear taxonomy that categorises
them into four key areas: activation functions, position encoding, combined
strategies, and network structure optimisation. We rigorously analyse their
critical properties, such as full differentiability, smoothness, compactness,
and adaptability to varying resolutions while also examining their strengths
and limitations in addressing locality biases and capturing fine details. Our
experimental comparison offers new insights into the trade-offs between
different approaches, showcasing the capabilities and challenges of the latest
INR techniques across various tasks. In addition to identifying areas where
current methods excel, we highlight key limitations and potential avenues for
improvement, such as developing more expressive activation functions, enhancing
positional encoding mechanisms, and improving scalability for complex,
high-dimensional data. This survey serves as a roadmap for researchers,
offering practical guidance for future exploration in the field of INRs. We aim
to foster new methodologies by outlining promising research directions for INRs
and applications.
\\ ( https://arxiv.org/abs/2411.03688 ,  6799kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03695
Date: Wed, 6 Nov 2024 06:33:55 GMT   (5250kb,D)

Title: AMNCutter: Affinity-Attention-Guided Multi-View Normalized Cutter for
  Unsupervised Surgical Instrument Segmentation
Authors: Mingyu Sheng, Jianan Fan, Dongnan Liu, Ron Kikinis, Weidong Cai
Categories: cs.CV
Comments: This paper was accepted by the 2025 IEEE Winter Conference on
  Applications of Computer Vision (WACV)
\\
  Surgical instrument segmentation (SIS) is pivotal for robotic-assisted
minimally invasive surgery, assisting surgeons by identifying surgical
instruments in endoscopic video frames. Recent unsupervised surgical instrument
segmentation (USIS) methods primarily rely on pseudo-labels derived from
low-level features such as color and optical flow, but these methods show
limited effectiveness and generalizability in complex and unseen endoscopic
scenarios. In this work, we propose a label-free unsupervised model featuring a
novel module named Multi-View Normalized Cutter (m-NCutter). Different from
previous USIS works, our model is trained using a graph-cutting loss function
that leverages patch affinities for supervision, eliminating the need for
pseudo-labels. The framework adaptively determines which affinities from which
levels should be prioritized. Therefore, the low- and high-level features and
their affinities are effectively integrated to train a label-free unsupervised
model, showing superior effectiveness and generalization ability. We conduct
comprehensive experiments across multiple SIS datasets to validate our
approach's state-of-the-art (SOTA) performance, robustness, and exceptional
potential as a pre-trained model. Our code is released at
https://github.com/MingyuShengSMY/AMNCutter.
\\ ( https://arxiv.org/abs/2411.03695 ,  5250kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03696
Date: Wed, 6 Nov 2024 06:34:27 GMT   (5141kb,D)

Title: OccLoff: Learning Optimized Feature Fusion for 3D Occupancy Prediction
Authors: Ji Zhang, Yiran Ding, Zixin Liu
Categories: cs.CV
\\
  3D semantic occupancy prediction is crucial for finely representing the
surrounding environment, which is essential for ensuring the safety in
autonomous driving. Existing fusion-based occupancy methods typically involve
performing a 2D-to-3D view transformation on image features, followed by
computationally intensive 3D operations to fuse these with LiDAR features,
leading to high computational costs and reduced accuracy. Moreover, current
research on occupancy prediction predominantly focuses on designing specific
network architectures, often tailored to particular models, with limited
attention given to the more fundamental aspect of semantic feature learning.
This gap hinders the development of more transferable methods that could
enhance the performance of various occupancy models. To address these
challenges, we propose OccLoff, a framework that Learns to Optimize Feature
Fusion for 3D occupancy prediction. Specifically, we introduce a sparse fusion
encoder with entropy masks that directly fuses 3D and 2D features, improving
model accuracy while reducing computational overhead. Additionally, we propose
a transferable proxy-based loss function and an adaptive hard sample weighting
algorithm, which enhance the performance of several state-of-the-art methods.
Extensive evaluations on the nuScenes and SemanticKITTI benchmarks demonstrate
the superiority of our framework, and ablation studies confirm the
effectiveness of each proposed module.
\\ ( https://arxiv.org/abs/2411.03696 ,  5141kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03702
Date: Wed, 6 Nov 2024 06:58:17 GMT   (3083kb,D)

Title: Graph-Based Multi-Modal Sensor Fusion for Autonomous Driving
Authors: Depanshu Sani and Saket Anand
Categories: cs.CV cs.RO
Comments: An extended abstract accepted at Young Researchers' Symposium, ICVGIP
  '24. This extended abstract contains the following: 1. Short summary of our
  work, SAGA-KF, accepted at ICPR'24. 2. A proposal that was awarded the
  Qualcomm Innovation Fellowship'24
\\
  The growing demand for robust scene understanding in mobile robotics and
autonomous driving has highlighted the importance of integrating multiple
sensing modalities. By combining data from diverse sensors like cameras and
LIDARs, fusion techniques can overcome the limitations of individual sensors,
enabling a more complete and accurate perception of the environment. We
introduce a novel approach to multi-modal sensor fusion, focusing on developing
a graph-based state representation that supports critical decision-making
processes in autonomous driving. We present a Sensor-Agnostic Graph-Aware
Kalman Filter [3], the first online state estimation technique designed to fuse
multi-modal graphs derived from noisy multi-sensor data. The estimated
graph-based state representations serve as a foundation for advanced
applications like Multi-Object Tracking (MOT), offering a comprehensive
framework for enhancing the situational awareness and safety of autonomous
systems. We validate the effectiveness of our proposed framework through
extensive experiments conducted on both synthetic and real-world driving
datasets (nuScenes). Our results showcase an improvement in MOTA and a
reduction in estimated position errors (MOTP) and identity switches (IDS) for
tracked objects using the SAGA-KF. Furthermore, we highlight the capability of
such a framework to develop methods that can leverage heterogeneous information
(like semantic objects and geometric structures) from various sensing
modalities, enabling a more holistic approach to scene understanding and
enhancing the safety and effectiveness of autonomous systems.
\\ ( https://arxiv.org/abs/2411.03702 ,  3083kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03706
Date: Wed, 6 Nov 2024 07:08:41 GMT   (31088kb,D)

Title: 3DGS-CD: 3D Gaussian Splatting-based Change Detection for Physical
  Object Rearrangement
Authors: Ziqi Lu, Jianbo Ye, John Leonard
Categories: cs.CV cs.RO
\\
  We present 3DGS-CD, the first 3D Gaussian Splatting (3DGS)-based method for
detecting physical object rearrangements in 3D scenes. Our approach estimates
3D object-level changes by comparing two sets of unaligned images taken at
different times. Leveraging 3DGS's novel view rendering and EfficientSAM's
zero-shot segmentation capabilities, we detect 2D object-level changes, which
are then associated and fused across views to estimate 3D changes. Our method
can detect changes in cluttered environments using sparse post-change images
within as little as 18s, using as few as a single new image. It does not rely
on depth input, user instructions, object classes, or object models -- An
object is recognized simply if it has been re-arranged. Our approach is
evaluated on both public and self-collected real-world datasets, achieving up
to 14% higher accuracy and three orders of magnitude faster performance
compared to the state-of-the-art radiance-field-based change detection method.
This significant performance boost enables a broad range of downstream
applications, where we highlight three key use cases: object reconstruction,
robot workspace reset, and 3DGS model update. Our code and data will be made
available at https://github.com/520xyxyzq/3DGS-CD.
\\ ( https://arxiv.org/abs/2411.03706 ,  31088kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03707
Date: Wed, 6 Nov 2024 07:11:15 GMT   (742kb)

Title: Fine-Tuning Vision-Language Model for Automated Engineering Drawing
  Information Extraction
Authors: Muhammad Tayyab Khan, Lequn Chen, Ye Han Ng, Wenhe Feng, Nicholas Yew
  Jin Tan, Seung Ki Moon
Categories: cs.CV cs.AI
Comments: Paper has been submitted to the 9th International Conference on
  Innovation in Artificial Intelligence (ICIAI 2025)
\\
  Geometric Dimensioning and Tolerancing (GD&T) plays a critical role in
manufacturing by defining acceptable variations in part features to ensure
component quality and functionality. However, extracting GD&T information from
2D engineering drawings is a time-consuming and labor-intensive task, often
relying on manual efforts or semi-automated tools. To address these challenges,
this study proposes an automated and computationally efficient GD&T extraction
method by fine-tuning Florence-2, an open-source vision-language model (VLM).
The model is trained on a dataset of 400 drawings with ground truth annotations
provided by domain experts. For comparison, two state-of-the-art closed-source
VLMs, GPT-4o and Claude-3.5-Sonnet, are evaluated on the same dataset. All
models are assessed using precision, recall, F1-score, and hallucination
metrics. Due to the computational cost and impracticality of fine-tuning large
closed-source VLMs for domain-specific tasks, GPT-4o and Claude-3.5-Sonnet are
evaluated in a zero-shot setting. In contrast, Florence-2, a smaller model with
0.23 billion parameters, is optimized through full-parameter fine-tuning across
three distinct experiments, each utilizing datasets augmented to different
levels. The results show that Florence-2 achieves a 29.95% increase in
precision, a 37.75% increase in recall, a 52.40% improvement in F1-score, and a
43.15% reduction in hallucination rate compared to the best-performing
closed-source model. These findings highlight the effectiveness of fine-tuning
smaller, open-source VLMs like Florence-2, offering a practical and efficient
solution for automated GD&T extraction to support downstream manufacturing
tasks.
\\ ( https://arxiv.org/abs/2411.03707 ,  742kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03714
Date: Wed, 6 Nov 2024 07:28:57 GMT   (5526kb,D)

Title: Explaining Human Activity Recognition with SHAP: Validating Insights
  with Perturbation and Quantitative Measures
Authors: Felix Tempel, Espen Alexander F. Ihlen, Lars Adde, Inga Str\"umke
Categories: cs.CV
\\
  In Human Activity Recognition (HAR), understanding the intricacy of body
movements within high-risk applications is essential. This study uses SHapley
Additive exPlanations (SHAP) to explain the decision-making process of Graph
Convolution Networks (GCNs) when classifying activities with skeleton data. We
employ SHAP to explain two real-world datasets: one for cerebral palsy (CP)
classification and the widely used NTU RGB+D 60 action recognition dataset. To
test the explanation, we introduce a novel perturbation approach that modifies
the model's edge importance matrix, allowing us to evaluate the impact of
specific body key points on prediction outcomes. To assess the fidelity of our
explanations, we employ informed perturbation, targeting body key points
identified as important by SHAP and comparing them against random perturbation
as a control condition. This perturbation enables a judgment on whether the
body key points are truly influential or non-influential based on the SHAP
values. Results on both datasets show that body key points identified as
important through SHAP have the largest influence on the accuracy, specificity,
and sensitivity metrics. Our findings highlight that SHAP can provide granular
insights into the input feature contribution to the prediction outcome of GCNs
in HAR tasks. This demonstrates the potential for more interpretable and
trustworthy models in high-stakes applications like healthcare or
rehabilitation.
\\ ( https://arxiv.org/abs/2411.03714 ,  5526kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03717
Date: Wed, 6 Nov 2024 07:30:34 GMT   (10454kb,D)

Title: These Maps Are Made by Propagation: Adapting Deep Stereo Networks to
  Road Scenarios with Decisive Disparity Diffusion
Authors: Chuang-Wei Liu, Yikang Zhang, Qijun Chen, Ioannis Pitas, and Rui Fan
Categories: cs.CV
Comments: 13 pages, 7 figures
\\
  Stereo matching has emerged as a cost-effective solution for road surface 3D
reconstruction, garnering significant attention towards improving both
computational efficiency and accuracy. This article introduces decisive
disparity diffusion (D3Stereo), marking the first exploration of dense deep
feature matching that adapts pre-trained deep convolutional neural networks
(DCNNs) to previously unseen road scenarios. A pyramid of cost volumes is
initially created using various levels of learned representations.
Subsequently, a novel recursive bilateral filtering algorithm is employed to
aggregate these costs. A key innovation of D3Stereo lies in its alternating
decisive disparity diffusion strategy, wherein intra-scale diffusion is
employed to complete sparse disparity images, while inter-scale inheritance
provides valuable prior information for higher resolutions. Extensive
experiments conducted on our created UDTIRI-Stereo and Stereo-Road datasets
underscore the effectiveness of D3Stereo strategy in adapting pre-trained DCNNs
and its superior performance compared to all other explicit programming-based
algorithms designed specifically for road surface 3D reconstruction. Additional
experiments conducted on the Middlebury dataset with backbone DCNNs pre-trained
on the ImageNet database further validate the versatility of D3Stereo strategy
in tackling general stereo matching problems.
\\ ( https://arxiv.org/abs/2411.03717 ,  10454kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03724
Date: Wed, 6 Nov 2024 07:43:40 GMT   (16377kb,D)

Title: Estimation of Psychosocial Work Environment Exposures Through Video
  Object Detection. Proof of Concept Using CCTV Footage
Authors: Claus D. Hansen, Thuy Hai Le and David Campos
Categories: cs.CV
Comments: 11 pages, 9 figures, presented at IWOAR 9th International Workshop on
  Sensor-Based Activity Recognition and Artificial Intelligence, September
  26-27, Potsdam, Germany
\\
  This paper examines the use of computer vision algorithms to estimate aspects
of the psychosocial work environment using CCTV footage. We present a proof of
concept for a methodology that detects and tracks people in video footage and
estimates interactions between customers and employees by estimating their
poses and calculating the duration of their encounters. We propose a pipeline
that combines existing object detection and tracking algorithms (YOLOv8 and
DeepSORT) with pose estimation algorithms (BlazePose) to estimate the number of
customers and employees in the footage as well as the duration of their
encounters. We use a simple rule-based approach to classify the interactions as
positive, neutral or negative based on three different criteria: distance,
duration and pose. The proposed methodology is tested on a small dataset of
CCTV footage. While the data is quite limited in particular with respect to the
quality of the footage, we have chosen this case as it represents a typical
setting where the method could be applied. The results show that the object
detection and tracking part of the pipeline has a reasonable performance on the
dataset with a high degree of recall and reasonable accuracy. At this stage,
the pose estimation is still limited to fully detect the type of interactions
due to difficulties in tracking employees in the footage. We conclude that the
method is a promising alternative to self-reported measures of the psychosocial
work environment and could be used in future studies to obtain external
observations of the work environment.
\\ ( https://arxiv.org/abs/2411.03724 ,  16377kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03725
Date: Wed, 6 Nov 2024 07:44:04 GMT   (7801kb,D)

Title: PX2Tooth: Reconstructing the 3D Point Cloud Teeth from a Single
  Panoramic X-ray
Authors: Wen Ma, Huikai Wu, Zikai Xiao, Yang Feng, Jian Wu, and Zuozhu Liu
Categories: cs.CV
Comments: Ma W, Wu H, Xiao Z, et al. PX2Tooth: Reconstructing the 3D Point
  Cloud Teeth from a Single Panoramic X-Ray[C]//International Conference on
  Medical Image Computing and Computer-Assisted Intervention. Cham: Springer
  Nature Switzerland, 2024: 411-421
\\
  Reconstructing the 3D anatomical structures of the oral cavity, which
originally reside in the cone-beam CT (CBCT), from a single 2D Panoramic
X-ray(PX) remains a critical yet challenging task, as it can effectively reduce
radiation risks and treatment costs during the diagnostic in digital dentistry.
However, current methods are either error-prone or only trained/evaluated on
small-scale datasets (less than 50 cases), resulting in compromised
trustworthiness. In this paper, we propose PX2Tooth, a novel approach to
reconstruct 3D teeth using a single PX image with a two-stage framework. First,
we design the PXSegNet to segment the permanent teeth from the PX images,
providing clear positional, morphological, and categorical information for each
tooth. Subsequently, we design a novel tooth generation network (TGNet) that
learns to transform random point clouds into 3D teeth. TGNet integrates the
segmented patch information and introduces a Prior Fusion Module (PFM) to
enhance the generation quality, especially in the root apex region. Moreover,
we construct a dataset comprising 499 pairs of CBCT and Panoramic X-rays.
Extensive experiments demonstrate that PX2Tooth can achieve an Intersection
over Union (IoU) of 0.793, significantly surpassing previous methods,
underscoring the great potential of artificial intelligence in digital
dentistry.
\\ ( https://arxiv.org/abs/2411.03725 ,  7801kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03728
Date: Wed, 6 Nov 2024 07:46:34 GMT   (10532kb,D)

Title: Efficient Fourier Filtering Network with Contrastive Learning for
  UAV-based Unaligned Bi-modal Salient Object Detection
Authors: Pengfei Lyu, Pak-Hei Yeung, Xiufei Cheng, Xiaosheng Yu, Chengdong Wu,
  and Jagath C. Rajapakse
Categories: cs.CV
Comments: 11 pages, 7 figures
\\
  Unmanned aerial vehicle (UAV)-based bi-modal salient object detection (BSOD)
aims to segment salient objects in a scene utilizing complementary cues in
unaligned RGB and thermal image pairs. However, the high computational expense
of existing UAV-based BSOD models limits their applicability to real-world UAV
devices. To address this problem, we propose an efficient Fourier filter
network with contrastive learning that achieves both real-time and accurate
performance. Specifically, we first design a semantic contrastive alignment
loss to align the two modalities at the semantic level, which facilitates
mutual refinement in a parameter-free way. Second, inspired by the fast Fourier
transform that obtains global relevance in linear complexity, we propose
synchronized alignment fusion, which aligns and fuses bi-modal features in the
channel and spatial dimensions by a hierarchical filtering mechanism. Our
proposed model, AlignSal, reduces the number of parameters by 70.0%, decreases
the floating point operations by 49.4%, and increases the inference speed by
152.5% compared to the cutting-edge BSOD model (i.e., MROS). Extensive
experiments on the UAV RGB-T 2400 and three weakly aligned datasets demonstrate
that AlignSal achieves both real-time inference speed and better performance
and generalizability compared to sixteen state-of-the-art BSOD models across
most evaluation metrics. In addition, our ablation studies further verify
AlignSal's potential in boosting the performance of existing aligned BSOD
models on UAV-based unaligned data. The code is available at:
https://github.com/JoshuaLPF/AlignSal.
\\ ( https://arxiv.org/abs/2411.03728 ,  10532kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03729
Date: Wed, 6 Nov 2024 07:48:30 GMT   (23891kb,D)

Title: Relation Learning and Aggregate-attention for Multi-person Motion
  Prediction
Authors: Kehua Qu, Rui Ding, Jin Tang
Categories: cs.CV cs.AI
Comments: Submitted to IEEE Transactions on Multimedia
\\
  Multi-person motion prediction is an emerging and intricate task with broad
real-world applications. Unlike single person motion prediction, it considers
not just the skeleton structures or human trajectories but also the
interactions between others. Previous methods use various networks to achieve
impressive predictions but often overlook that the joints relations within an
individual (intra-relation) and interactions among groups (inter-relation) are
distinct types of representations. These methods often lack explicit
representation of inter&intra-relations, and inevitably introduce undesired
dependencies. To address this issue, we introduce a new collaborative framework
for multi-person motion prediction that explicitly modeling these relations:a
GCN-based network for intra-relations and a novel reasoning network for
inter-relations.Moreover, we propose a novel plug-and-play aggregation module
called the Interaction Aggregation Module (IAM), which employs an
aggregate-attention mechanism to seamlessly integrate these relations.
Experiments indicate that the module can also be applied to other dual-path
models. Extensive experiments on the 3DPW, 3DPW-RC, CMU-Mocap, MuPoTS-3D, as
well as synthesized datasets Mix1 & Mix2 (9 to 15 persons), demonstrate that
our method achieves state-of-the-art performance.
\\ ( https://arxiv.org/abs/2411.03729 ,  23891kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03745
Date: Wed, 6 Nov 2024 08:22:00 GMT   (1320kb,D)

Title: Homotopy Continuation Made Easy: Regression-based Online Simulation of
  Starting Problem-Solution Pairs
Authors: Xinyue Zhang, Zijia Dai, Wanting Xu, Laurent Kneip
Categories: cs.CV
\\
  While automatically generated polynomial elimination templates have sparked
great progress in the field of 3D computer vision, there remain many problems
for which the degree of the constraints or the number of unknowns leads to
intractability. In recent years, homotopy continuation has been introduced as a
plausible alternative. However, the method currently depends on expensive
parallel tracking of all possible solutions in the complex domain, or a
classification network for starting problem-solution pairs trained over a
limited set of real-world examples. Our innovation consists of employing a
regression network trained in simulation to directly predict a solution from
input correspondences, followed by an online simulator that invents a
consistent problem-solution pair. Subsequently, homotopy continuation is
applied to track that single solution back to the original problem. We apply
this elegant combination to generalized camera resectioning, and also introduce
a new solution to the challenging generalized relative pose and scale problem.
As demonstrated, the proposed method successfully compensates the raw error
committed by the regressor alone, and leads to state-of-the-art efficiency and
success rates while running on CPU resources, only.
\\ ( https://arxiv.org/abs/2411.03745 ,  1320kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03794
Date: Wed, 6 Nov 2024 09:39:25 GMT   (39416kb,D)

Title: Harmformer: Harmonic Networks Meet Transformers for Continuous
  Roto-Translation Equivariance
Authors: Tom\'a\v{s} Karella, Adam Harmanec, Jan Kotera, Jan Bla\v{z}ek, Filip
  \v{S}roubek
Categories: cs.CV
Comments: Appears in NeurIPS 2024 Workshop on Symmetry and Geometry in Neural
  Representations
\\
  CNNs exhibit inherent equivariance to image translation, leading to efficient
parameter and data usage, faster learning, and improved robustness. The concept
of translation equivariant networks has been successfully extended to rotation
transformation using group convolution for discrete rotation groups and
harmonic functions for the continuous rotation group encompassing $360^\circ$.
We explore the compatibility of the SA mechanism with full rotation
equivariance, in contrast to previous studies that focused on discrete
rotation. We introduce the Harmformer, a harmonic transformer with a
convolutional stem that achieves equivariance for both translation and
continuous rotation. Accompanied by an end-to-end equivariance proof, the
Harmformer not only outperforms previous equivariant transformers, but also
demonstrates inherent stability under any continuous rotation, even without
seeing rotated samples during training.
\\ ( https://arxiv.org/abs/2411.03794 ,  39416kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03795
Date: Wed, 6 Nov 2024 09:39:52 GMT   (3918kb,D)

Title: VQA$^2$:Visual Question Answering for Video Quality Assessment
Authors: Ziheng Jia, Zicheng Zhang, Jiaying Qian, Haoning Wu, Wei Sun, Chunyi
  Li, Xiaohong Liu, Weisi Lin, Guangtao Zhai, Xiongkuo Min
Categories: cs.CV cs.AI
Comments: 10 pages 3 figures
\\
  The advent and proliferation of large multi-modal models (LMMs) have
introduced a new paradigm to video-related computer vision fields, including
training and inference methods based on visual question answering (VQA). These
methods enable models to handle multiple downstream tasks robustly. Video
Quality Assessment (VQA), a classic field in low-level visual quality
evaluation, originally focused on quantitative video quality scoring. However,
driven by advances in LMMs, it is now evolving towards more comprehensive
visual quality understanding tasks. Visual question answering has significantly
improved low-level visual evaluation within the image domain recently. However,
related work is almost nonexistent in the video domain, leaving substantial
room for improvement. To address this gap, we introduce the VQA2 Instruction
Dataset the first visual question answering instruction dataset entirely
focuses on video quality assessment, and based on it, we propose the VQA2
series models The VQA2 Instruction Dataset consists of three stages and covers
various video types, containing 157,735 instruction question-answer pairs,
including both manually annotated and synthetic data. We conduct extensive
experiments on both video quality scoring and video quality understanding
tasks. Results demonstrate that the VQA2 series models achieve state-of-the-art
(SOTA) performance in quality scoring tasks, and their performance in visual
quality question answering surpasses the renowned GPT-4o. Additionally, our
final model, the VQA2-Assistant, performs well across both scoring and
question-answering tasks, validating its versatility.
\\ ( https://arxiv.org/abs/2411.03795 ,  3918kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03807
Date: Wed, 6 Nov 2024 10:07:46 GMT   (4406kb,D)

Title: GS2Pose: Tow-stage 6D Object Pose Estimation Guided by Gaussian
  Splatting
Authors: Jilan Mei, Junbo Li, Cai Meng
Categories: cs.CV cs.AI
\\
  This paper proposes a new method for accurate and robust 6D pose estimation
of novel objects, named GS2Pose. By introducing 3D Gaussian splatting, GS2Pose
can utilize the reconstruction results without requiring a high-quality CAD
model, which means it only requires segmented RGBD images as input.
Specifically, GS2Pose employs a two-stage structure consisting of coarse
estimation followed by refined estimation. In the coarse stage, a lightweight
U-Net network with a polarization attention mechanism, called Pose-Net, is
designed. By using the 3DGS model for supervised training, Pose-Net can
generate NOCS images to compute a coarse pose. In the refinement stage, GS2Pose
formulates a pose regression algorithm following the idea of reprojection or
Bundle Adjustment (BA), referred to as GS-Refiner. By leveraging Lie algebra to
extend 3DGS, GS-Refiner obtains a pose-differentiable rendering pipeline that
refines the coarse pose by comparing the input images with the rendered images.
GS-Refiner also selectively updates parameters in the 3DGS model to achieve
environmental adaptation, thereby enhancing the algorithm's robustness and
flexibility to illuminative variation, occlusion, and other challenging
disruptive factors. GS2Pose was evaluated through experiments conducted on the
LineMod dataset, where it was compared with similar algorithms, yielding highly
competitive results. The code for GS2Pose will soon be released on GitHub.
\\ ( https://arxiv.org/abs/2411.03807 ,  4406kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03819
Date: Wed, 6 Nov 2024 10:39:00 GMT   (2342kb,D)

Title: SA3DIP: Segment Any 3D Instance with Potential 3D Priors
Authors: Xi Yang, Xu Gu, Xingyilang Yin, Xinbo Gao
Categories: cs.CV
\\
  The proliferation of 2D foundation models has sparked research into adapting
them for open-world 3D instance segmentation. Recent methods introduce a
paradigm that leverages superpoints as geometric primitives and incorporates 2D
multi-view masks from Segment Anything model (SAM) as merging guidance,
achieving outstanding zero-shot instance segmentation results. However, the
limited use of 3D priors restricts the segmentation performance. Previous
methods calculate the 3D superpoints solely based on estimated normal from
spatial coordinates, resulting in under-segmentation for instances with similar
geometry. Besides, the heavy reliance on SAM and hand-crafted algorithms in 2D
space suffers from over-segmentation due to SAM's inherent part-level
segmentation tendency. To address these issues, we propose SA3DIP, a novel
method for Segmenting Any 3D Instances via exploiting potential 3D Priors.
Specifically, on one hand, we generate complementary 3D primitives based on
both geometric and textural priors, which reduces the initial errors that
accumulate in subsequent procedures. On the other hand, we introduce
supplemental constraints from the 3D space by using a 3D detector to guide a
further merging process. Furthermore, we notice a considerable portion of
low-quality ground truth annotations in ScanNetV2 benchmark, which affect the
fair evaluations. Thus, we present ScanNetV2-INS with complete ground truth
labels and supplement additional instances for 3D class-agnostic instance
segmentation. Experimental evaluations on various 2D-3D datasets demonstrate
the effectiveness and robustness of our approach. Our code and proposed
ScanNetV2-INS dataset are available HERE.
\\ ( https://arxiv.org/abs/2411.03819 ,  2342kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03823
Date: Wed, 6 Nov 2024 10:44:15 GMT   (833kb,D)

Title: Both Text and Images Leaked! A Systematic Analysis of Multimodal LLM
  Data Contamination
Authors: Dingjie Song, Sicheng Lai, Shunian Chen, Lichao Sun, Benyou Wang
Categories: cs.CV cs.AI
\\
  The rapid progression of multimodal large language models (MLLMs) has
demonstrated superior performance on various multimodal benchmarks. However,
the issue of data contamination during training creates challenges in
performance evaluation and comparison. While numerous methods exist for
detecting dataset contamination in large language models (LLMs), they are less
effective for MLLMs due to their various modalities and multiple training
phases. In this study, we introduce a multimodal data contamination detection
framework, MM-Detect, designed for MLLMs. Our experimental results indicate
that MM-Detect is sensitive to varying degrees of contamination and can
highlight significant performance improvements due to leakage of the training
set of multimodal benchmarks. Furthermore, We also explore the possibility of
contamination originating from the pre-training phase of LLMs used by MLLMs and
the fine-tuning phase of MLLMs, offering new insights into the stages at which
contamination may be introduced.
\\ ( https://arxiv.org/abs/2411.03823 ,  833kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03829
Date: Wed, 6 Nov 2024 11:03:02 GMT   (18708kb,D)

Title: Generalize or Detect? Towards Robust Semantic Segmentation Under
  Multiple Distribution Shifts
Authors: Zhitong Gao, Bingnan Li, Mathieu Salzmann, Xuming He
Categories: cs.CV
Comments: Published in NeurIPS 2024
\\
  In open-world scenarios, where both novel classes and domains may exist, an
ideal segmentation model should detect anomaly classes for safety and
generalize to new domains. However, existing methods often struggle to
distinguish between domain-level and semantic-level distribution shifts,
leading to poor out-of-distribution (OOD) detection or domain generalization
performance. In this work, we aim to equip the model to generalize effectively
to covariate-shift regions while precisely identifying semantic-shift regions.
To achieve this, we design a novel generative augmentation method to produce
coherent images that incorporate both anomaly (or novel) objects and various
covariate shifts at both image and object levels. Furthermore, we introduce a
training strategy that recalibrates uncertainty specifically for semantic
shifts and enhances the feature extractor to align features associated with
domain shifts. We validate the effectiveness of our method across benchmarks
featuring both semantic and domain shifts. Our method achieves state-of-the-art
performance across all benchmarks for both OOD detection and domain
generalization. Code is available at
https://github.com/gaozhitong/MultiShiftSeg.
\\ ( https://arxiv.org/abs/2411.03829 ,  18708kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03831
Date: Wed, 6 Nov 2024 11:03:34 GMT   (291kb)

Title: An Enhancement of Haar Cascade Algorithm Applied to Face Recognition for
  Gate Pass Security
Authors: Clarence A. Antipona, Romeo R. Magsino, Raymund M. Dioses, Khatalyn E.
  Mata
Categories: cs.CV
\\
  This study is focused on enhancing the Haar Cascade Algorithm to decrease the
false positive and false negative rate in face matching and face detection to
increase the accuracy rate even under challenging conditions. The face
recognition library was implemented with Haar Cascade Algorithm in which the
128-dimensional vectors representing the unique features of a face are encoded.
A subprocess was applied where the grayscale image from Haar Cascade was
converted to RGB to improve the face encoding. Logical process and face
filtering are also used to decrease non-face detection. The Enhanced Haar
Cascade Algorithm produced a 98.39% accuracy rate (21.39% increase), 63.59%
precision rate, 98.30% recall rate, and 72.23% in F1 Score. In comparison, the
Haar Cascade Algorithm achieved a 46.70% to 77.00% accuracy rate, 44.15%
precision rate, 98.61% recall rate, and 47.01% in F1 Score. Both algorithms
used the Confusion Matrix Test with 301,950 comparisons using the same dataset
of 550 images. The 98.39% accuracy rate shows a significant decrease in false
positive and false negative rates in facial recognition. Face matching and face
detection are more accurate in images with complex backgrounds, lighting
variations, and occlusions, or even those with similar attributes.
\\ ( https://arxiv.org/abs/2411.03831 ,  291kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03835
Date: Wed, 6 Nov 2024 11:14:49 GMT   (1277kb,D)

Title: An Edge Computing-Based Solution for Real-Time Leaf Disease
  Classification using Thermal Imaging
Authors: P\'ublio Elon Correa da Silva and Jurandy Almeida
Categories: cs.CV
Journal-ref: IEEE Geoscience and Remote Sensing Letters (2024)
DOI: 10.1109/LGRS.2024.3456637
\\
  Deep learning (DL) technologies can transform agriculture by improving crop
health monitoring and management, thus improving food safety. In this paper, we
explore the potential of edge computing for real-time classification of leaf
diseases using thermal imaging. We present a thermal image dataset for plant
disease classification and evaluate deep learning models, including
InceptionV3, MobileNetV1, MobileNetV2, and VGG-16, on resource-constrained
devices like the Raspberry Pi 4B. Using pruning and quantization-aware
training, these models achieve inference times up to 1.48x faster on Edge TPU
Max for VGG16, and up to 2.13x faster with precision reduction on Intel NCS2
for MobileNetV1, compared to high-end GPUs like the RTX 3090, while maintaining
state-of-the-art accuracy.
\\ ( https://arxiv.org/abs/2411.03835 ,  1277kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03861
Date: Wed, 6 Nov 2024 12:14:11 GMT   (631kb,D)

Title: FedRISE: Rating Induced Sign Election of Gradients for Byzantine
  Tolerant Federated Aggregation
Authors: Joseph Geo Benjamin, Mothilal Asokan, Mohammad Yaqub, Karthik
  Nandakumar
Categories: cs.CV cs.CR
Comments: This is a work under submission/review process
\\
  One of the most common defense strategies against model poisoning in
federated learning is to employ a robust aggregator mechanism that makes the
training more resilient. Many of the existing Byzantine robust aggregators
provide theoretical guarantees and are empirically effective against certain
categories of attacks. However, we observe that certain high-strength attacks
can subvert the aggregator and collapse the training. In addition, most
aggregators require identifying tolerant settings to converge. Impact of
attacks becomes more pronounced when the number of Byzantines is near-majority,
and becomes harder to evade if the attacker is omniscient with access to data,
honest updates and aggregation methods. Motivated by these observations, we
develop a robust aggregator called FedRISE for cross-silo FL that is consistent
and less susceptible to poisoning updates by an omniscient attacker. The
proposed method explicitly determines the optimal direction of each gradient
through a sign-voting strategy that uses variance-reduced sparse gradients. We
argue that vote weighting based on the cosine similarity of raw gradients is
misleading, and we introduce a sign-based gradient valuation function that
ignores the gradient magnitude. We compare our method against 8 robust
aggregators under 6 poisoning attacks on 3 datasets and architectures. Our
results show that existing robust aggregators collapse for at least some
attacks under severe settings, while FedRISE demonstrates better robustness
because of a stringent gradient inclusion formulation.
\\ ( https://arxiv.org/abs/2411.03861 ,  631kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03862
Date: Wed, 6 Nov 2024 12:14:23 GMT   (2518kb,D)

Title: ROBIN: Robust and Invisible Watermarks for Diffusion Models with
  Adversarial Optimization
Authors: Huayang Huang, Yu Wu, Qian Wang
Categories: cs.CV cs.AI cs.CR
Comments: Accept to NeurIPS 2024
\\
  Watermarking generative content serves as a vital tool for authentication,
ownership protection, and mitigation of potential misuse. Existing watermarking
methods face the challenge of balancing robustness and concealment. They
empirically inject a watermark that is both invisible and robust and passively
achieve concealment by limiting the strength of the watermark, thus reducing
the robustness. In this paper, we propose to explicitly introduce a watermark
hiding process to actively achieve concealment, thus allowing the embedding of
stronger watermarks. To be specific, we implant a robust watermark in an
intermediate diffusion state and then guide the model to hide the watermark in
the final generated image. We employ an adversarial optimization algorithm to
produce the optimal hiding prompt guiding signal for each watermark. The prompt
embedding is optimized to minimize artifacts in the generated image, while the
watermark is optimized to achieve maximum strength. The watermark can be
verified by reversing the generation process. Experiments on various diffusion
models demonstrate the watermark remains verifiable even under significant
image tampering and shows superior invisibility compared to other
state-of-the-art robust watermarking methods.
\\ ( https://arxiv.org/abs/2411.03862 ,  2518kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03924
Date: Wed, 6 Nov 2024 13:54:26 GMT   (3408kb,D)

Title: Self-supervised Representation Learning for Cell Event Recognition
  through Time Arrow Prediction
Authors: Cangxiong Chen, Vinay P. Namboodiri, Julia E. Sero
Categories: cs.CV
\\
  The spatio-temporal nature of live-cell microscopy data poses challenges in
the analysis of cell states which is fundamental in bioimaging. Deep-learning
based segmentation or tracking methods rely on large amount of high quality
annotations to work effectively. In this work, we explore an alternative
solution: using feature maps obtained from self-supervised representation
learning (SSRL) on time arrow prediction (TAP) for the downstream supervised
task of cell event recognition. We demonstrate through extensive experiments
and analysis that this approach can achieve better performance with limited
annotation compared to models trained from end to end using fully supervised
approach. Our analysis also provides insight into applications of the SSRL
using TAP in live-cell microscopy.
\\ ( https://arxiv.org/abs/2411.03924 ,  3408kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03926
Date: Wed, 6 Nov 2024 13:57:53 GMT   (3150kb,D)

Title: Act in Collusion: A Persistent Distributed Multi-Target Backdoor in
  Federated Learning
Authors: Tao Liu, Wu Yang, Chen Xu, Jiguang Lv, Huanran Wang, Yuhang Zhang,
  Shuchun Xu, Dapeng Man
Categories: cs.CV
\\
  Federated learning, a novel paradigm designed to protect data privacy, is
vulnerable to backdoor attacks due to its distributed nature. Current research
often designs attacks based on a single attacker with a single backdoor,
overlooking more realistic and complex threats in federated learning. We
propose a more practical threat model for federated learning: the distributed
multi-target backdoor. In this model, multiple attackers control different
clients, embedding various triggers and targeting different classes,
collaboratively implanting backdoors into the global model via central
aggregation. Empirical validation shows that existing methods struggle to
maintain the effectiveness of multiple backdoors in the global model. Our key
insight is that similar backdoor triggers cause parameter conflicts and
injecting new backdoors disrupts gradient directions, significantly weakening
some backdoors performance. To solve this, we propose a Distributed
Multi-Target Backdoor Attack (DMBA), ensuring efficiency and persistence of
backdoors from different malicious clients. To avoid parameter conflicts, we
design a multi-channel dispersed frequency trigger strategy to maximize trigger
differences. To mitigate gradient interference, we introduce backdoor replay in
local training to neutralize conflicting gradients. Extensive validation shows
that 30 rounds after the attack, Attack Success Rates of three different
backdoors from various clients remain above 93%. The code will be made publicly
available after the review period.
\\ ( https://arxiv.org/abs/2411.03926 ,  3150kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03959
Date: Wed, 6 Nov 2024 14:45:16 GMT   (15294kb,D)

Title: Energy Score-based Pseudo-Label Filtering and Adaptive Loss for
  Imbalanced Semi-supervised SAR target recognition
Authors: Xinzheng Zhang, Yuqing Luo, Guopeng Li
Categories: cs.CV cs.AI
\\
  Automatic target recognition (ATR) is an important use case for synthetic
aperture radar (SAR) image interpretation. Recent years have seen significant
advancements in SAR ATR technology based on semi-supervised learning. However,
existing semi-supervised SAR ATR algorithms show low recognition accuracy in
the case of class imbalance. This work offers a non-balanced semi-supervised
SAR target recognition approach using dynamic energy scores and adaptive loss.
First, an energy score-based method is developed to dynamically select
unlabeled samples near to the training distribution as pseudo-labels during
training, assuring pseudo-label reliability in long-tailed distribution
circumstances. Secondly, loss functions suitable for class imbalances are
proposed, including adaptive margin perception loss and adaptive hard triplet
loss, the former offsets inter-class confusion of classifiers, alleviating the
imbalance issue inherent in pseudo-label generation. The latter effectively
tackles the model's preference for the majority class by focusing on complex
difficult samples during training. Experimental results on extremely imbalanced
SAR datasets demonstrate that the proposed method performs well under the dual
constraints of scarce labels and data imbalance, effectively overcoming the
model bias caused by data imbalance and achieving high-precision target
recognition.
\\ ( https://arxiv.org/abs/2411.03959 ,  15294kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03960
Date: Wed, 6 Nov 2024 14:45:41 GMT   (38752kb,D)

Title: Face Reconstruction from Face Embeddings using Adapter to a Face
  Foundation Model
Authors: Hatef Otroshi Shahreza, Anjith George, S\'ebastien Marcel
Categories: cs.CV
\\
  Face recognition systems extract embedding vectors from face images and use
these embeddings to verify or identify individuals. Face reconstruction attack
(also known as template inversion) refers to reconstructing face images from
face embeddings and using the reconstructed face image to enter a face
recognition system. In this paper, we propose to use a face foundation model to
reconstruct face images from the embeddings of a blackbox face recognition
model. The foundation model is trained with 42M images to generate face images
from the facial embeddings of a fixed face recognition model. We propose to use
an adapter to translate target embeddings into the embedding space of the
foundation model. The generated images are evaluated on different face
recognition models and different datasets, demonstrating the effectiveness of
our method to translate embeddings of different face recognition models. We
also evaluate the transferability of reconstructed face images when attacking
different face recognition models. Our experimental results show that our
reconstructed face images outperform previous reconstruction attacks against
face recognition models.
\\ ( https://arxiv.org/abs/2411.03960 ,  38752kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03976
Date: Wed, 6 Nov 2024 15:13:31 GMT   (2111kb,D)

Title: HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion
  Segmentation
Authors: Ziyuan Ding, Yixiong Liang, Shichao Kan, Qing Liu
Categories: cs.CV
Comments: 11 pages, 3 figures, accepted by MICCAI 2024, the revised version
Journal-ref: MICCAI 2024, LNCS 15009, pp. 1-11, 2024
DOI: 10.1007/978-3-031-72114-4_32
\\
  High resolution is crucial for precise segmentation in fundus images, yet
handling high-resolution inputs incurs considerable GPU memory costs, with
diminishing performance gains as overhead increases. To address this issue
while tackling the challenge of segmenting tiny objects, recent studies have
explored local-global fusion methods. These methods preserve fine details using
local regions and capture long-range context information from downscaled global
images. However, the necessity of multiple forward passes inevitably incurs
significant computational overhead, adversely affecting inference speed. In
this paper, we propose HRDecoder, a simple High-Resolution Decoder network for
fundus lesion segmentation. It integrates a high-resolution representation
learning module to capture fine-grained local features and a high-resolution
fusion module to fuse multi-scale predictions. Our method effectively improves
the overall segmentation accuracy of fundus lesions while consuming reasonable
memory and computational overhead, and maintaining satisfying inference speed.
Experimental results on the IDRID and DDR datasets demonstrate the
effectiveness of our method. Code is available at
https://github.com/CVIU-CSU/HRDecoder.
\\ ( https://arxiv.org/abs/2411.03976 ,  2111kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03982
Date: Wed, 6 Nov 2024 15:19:24 GMT   (47598kb,D)

Title: ReEdit: Multimodal Exemplar-Based Image Editing with Diffusion Models
Authors: Ashutosh Srivastava, Tarun Ram Menta, Abhinav Java, Avadhoot Jadhav,
  Silky Singh, Surgan Jandial, Balaji Krishnamurthy
Categories: cs.CV
Comments: First three authors contributed equally to this work
\\
  Modern Text-to-Image (T2I) Diffusion models have revolutionized image editing
by enabling the generation of high-quality photorealistic images. While the de
facto method for performing edits with T2I models is through text instructions,
this approach non-trivial due to the complex many-to-many mapping between
natural language and images. In this work, we address exemplar-based image
editing -- the task of transferring an edit from an exemplar pair to a content
image(s). We propose ReEdit, a modular and efficient end-to-end framework that
captures edits in both text and image modalities while ensuring the fidelity of
the edited image. We validate the effectiveness of ReEdit through extensive
comparisons with state-of-the-art baselines and sensitivity analyses of key
design choices. Our results demonstrate that ReEdit consistently outperforms
contemporary approaches both qualitatively and quantitatively. Additionally,
ReEdit boasts high practical applicability, as it does not require any
task-specific optimization and is four times faster than the next best
baseline.
\\ ( https://arxiv.org/abs/2411.03982 ,  47598kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03993
Date: Wed, 6 Nov 2024 15:34:57 GMT   (28518kb,D)

Title: Local vs distributed representations: What is the right basis for
  interpretability?
Authors: Julien Colin, Lore Goetschalckx, Thomas Fel, Victor Boutin, Jay Gopal,
  Thomas Serre, Nuria Oliver
Categories: cs.CV
\\
  Much of the research on the interpretability of deep neural networks has
focused on studying the visual features that maximally activate individual
neurons. However, recent work has cast doubts on the usefulness of such local
representations for understanding the behavior of deep neural networks because
individual neurons tend to respond to multiple unrelated visual patterns, a
phenomenon referred to as "superposition". A promising alternative to
disentangle these complex patterns is learning sparsely distributed vector
representations from entire network layers, as the resulting basis vectors
seemingly encode single identifiable visual patterns consistently. Thus, one
would expect the resulting code to align better with human perceivable visual
patterns, but supporting evidence remains, at best, anecdotal. To fill this
gap, we conducted three large-scale psychophysics experiments collected from a
pool of 560 participants. Our findings provide (i) strong evidence that
features obtained from sparse distributed representations are easier to
interpret by human observers and (ii) that this effect is more pronounced in
the deepest layers of a neural network. Complementary analyses also reveal that
(iii) features derived from sparse distributed representations contribute more
to the model's decision. Overall, our results highlight that distributed
representations constitute a superior basis for interpretability, underscoring
a need for the field to move beyond the interpretation of local neural codes in
favor of sparsely distributed ones.
\\ ( https://arxiv.org/abs/2411.03993 ,  28518kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04008
Date: Wed, 6 Nov 2024 15:47:18 GMT   (14061kb,D)

Title: Aligning Characteristic Descriptors with Images for Human-Expert-like
  Explainability
Authors: Bharat Chandra Yalavarthi, Nalini Ratha
Categories: cs.CV cs.AI
\\
  In mission-critical domains such as law enforcement and medical diagnosis,
the ability to explain and interpret the outputs of deep learning models is
crucial for ensuring user trust and supporting informed decision-making.
Despite advancements in explainability, existing methods often fall short in
providing explanations that mirror the depth and clarity of those given by
human experts. Such expert-level explanations are essential for the dependable
application of deep learning models in law enforcement and medical contexts.
Additionally, we recognize that most explanations in real-world scenarios are
communicated primarily through natural language. Addressing these needs, we
propose a novel approach that utilizes characteristic descriptors to explain
model decisions by identifying their presence in images, thereby generating
expert-like explanations. Our method incorporates a concept bottleneck layer
within the model architecture, which calculates the similarity between image
and descriptor encodings to deliver inherent and faithful explanations. Through
experiments in face recognition and chest X-ray diagnosis, we demonstrate that
our approach offers a significant contrast over existing techniques, which are
often limited to the use of saliency maps. We believe our approach represents a
significant step toward making deep learning systems more accountable,
transparent, and trustworthy in the critical domains of face recognition and
medical diagnosis.
\\ ( https://arxiv.org/abs/2411.04008 ,  14061kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04059
Date: Wed, 6 Nov 2024 17:11:44 GMT   (1238kb,D)

Title: Pseudo-labeling with Keyword Refining for Few-Supervised Video
  Captioning
Authors: Ping Li and Tao Wang and Xinkui Zhao and Xianghua Xu and Mingli Song
Categories: cs.CV
Comments: 12 figures, Accepted in Pattern Recognition
\\
  Video captioning generate a sentence that describes the video content.
Existing methods always require a number of captions (\eg, 10 or 20) per video
to train the model, which is quite costly. In this work, we explore the
possibility of using only one or very few ground-truth sentences, and introduce
a new task named few-supervised video captioning. Specifically, we propose a
few-supervised video captioning framework that consists of lexically
constrained pseudo-labeling module and keyword-refined captioning module.
Unlike the random sampling in natural language processing that may cause
invalid modifications (\ie, edit words), the former module guides the model to
edit words using some actions (\eg, copy, replace, insert, and delete) by a
pretrained token-level classifier, and then fine-tunes candidate sentences by a
pretrained language model. Meanwhile, the former employs the repetition
penalized sampling to encourage the model to yield concise pseudo-labeled
sentences with less repetition, and selects the most relevant sentences upon a
pretrained video-text model. Moreover, to keep semantic consistency between
pseudo-labeled sentences and video content, we develop the transformer-based
keyword refiner with the video-keyword gated fusion strategy to emphasize more
on relevant words. Extensive experiments on several benchmarks demonstrate the
advantages of the proposed approach in both few-supervised and fully-supervised
scenarios. The code implementation is available at
https://github.com/mlvccn/PKG_VidCap
\\ ( https://arxiv.org/abs/2411.04059 ,  1238kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04077
Date: Wed, 6 Nov 2024 17:55:37 GMT   (2608kb,D)

Title: H-POPE: Hierarchical Polling-based Probing Evaluation of Hallucinations
  in Large Vision-Language Models
Authors: Nhi Pham, Michael Schott
Categories: cs.CV
Comments: Poster at https://sites.google.com/berkeley.edu/bb-stat/home
\\
  By leveraging both texts and images, large vision language models (LVLMs)
have shown significant progress in various multi-modal tasks. Nevertheless,
these models often suffer from hallucinations, e.g., they exhibit
inconsistencies between the visual input and the textual output. To address
this, we propose H-POPE, a coarse-to-fine-grained benchmark that systematically
assesses hallucination in object existence and attributes. Our evaluation shows
that models are prone to hallucinations on object existence, and even more so
on fine-grained attributes. We further investigate whether these models rely on
visual input to formulate the output texts.
\\ ( https://arxiv.org/abs/2411.04077 ,  2608kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04079
Date: Wed, 6 Nov 2024 17:57:43 GMT   (4895kb,D)

Title: Textual Decomposition Then Sub-motion-space Scattering for
  Open-Vocabulary Motion Generation
Authors: Ke Fan, Jiangning Zhang, Ran Yi, Jingyu Gong, Yabiao Wang, Yating
  Wang, Xin Tan, Chengjie Wang and Lizhuang Ma
Categories: cs.CV
Comments: project page: https://vankouf.github.io/DSONet/
\\
  Text-to-motion generation is a crucial task in computer vision, which
generates the target 3D motion by the given text. The existing annotated
datasets are limited in scale, resulting in most existing methods overfitting
to the small datasets and unable to generalize to the motions of the open
domain. Some methods attempt to solve the open-vocabulary motion generation
problem by aligning to the CLIP space or using the Pretrain-then-Finetuning
paradigm. However, the current annotated dataset's limited scale only allows
them to achieve mapping from sub-text-space to sub-motion-space, instead of
mapping between full-text-space and full-motion-space (full mapping), which is
the key to attaining open-vocabulary motion generation. To this end, this paper
proposes to leverage the atomic motion (simple body part motions over a short
time period) as an intermediate representation, and leverage two orderly
coupled steps, i.e., Textual Decomposition and Sub-motion-space Scattering, to
address the full mapping problem. For Textual Decomposition, we design a
fine-grained description conversion algorithm, and combine it with the
generalization ability of a large language model to convert any given motion
text into atomic texts. Sub-motion-space Scattering learns the compositional
process from atomic motions to the target motions, to make the learned
sub-motion-space scattered to form the full-motion-space. For a given motion of
the open domain, it transforms the extrapolation into interpolation and thereby
significantly improves generalization. Our network, $DSO$-Net, combines textual
$d$ecomposition and sub-motion-space $s$cattering to solve the
$o$pen-vocabulary motion generation. Extensive experiments demonstrate that our
DSO-Net achieves significant improvements over the state-of-the-art methods on
open-vocabulary motion generation. Code is available at
https://vankouf.github.io/DSONet/.
\\ ( https://arxiv.org/abs/2411.04079 ,  4895kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04097
Date: Wed, 6 Nov 2024 18:25:00 GMT   (12078kb,D)

Title: RaVL: Discovering and Mitigating Spurious Correlations in Fine-Tuned
  Vision-Language Models
Authors: Maya Varma, Jean-Benoit Delbrouck, Zhihong Chen, Akshay Chaudhari,
  Curtis Langlotz
Categories: cs.CV cs.AI
Comments: NeurIPS 2024
\\
  Fine-tuned vision-language models (VLMs) often capture spurious correlations
between image features and textual attributes, resulting in degraded zero-shot
performance at test time. Existing approaches for addressing spurious
correlations (i) primarily operate at the global image-level rather than
intervening directly on fine-grained image features and (ii) are predominantly
designed for unimodal settings. In this work, we present RaVL, which takes a
fine-grained perspective on VLM robustness by discovering and mitigating
spurious correlations using local image features rather than operating at the
global image level. Given a fine-tuned VLM, RaVL first discovers spurious
correlations by leveraging a region-level clustering approach to identify
precise image features contributing to zero-shot classification errors. Then,
RaVL mitigates the identified spurious correlation with a novel region-aware
loss function that enables the VLM to focus on relevant regions and ignore
spurious relationships during fine-tuning. We evaluate RaVL on 654 VLMs with
various model architectures, data domains, and learned spurious correlations.
Our results show that RaVL accurately discovers (191% improvement over the
closest baseline) and mitigates (8.2% improvement on worst-group image
classification accuracy) spurious correlations. Qualitative evaluations on
general-domain and medical-domain VLMs confirm our findings.
\\ ( https://arxiv.org/abs/2411.04097 ,  12078kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04125
Date: Wed, 6 Nov 2024 18:59:41 GMT   (6839kb,D)

Title: Community Forensics: Using Thousands of Generators to Train Fake Image
  Detectors
Authors: Jeongsoo Park, Andrew Owens
Categories: cs.CV
Comments: 15 pages
\\
  One of the key challenges of detecting AI-generated images is spotting images
that have been created by previously unseen generative models. We argue that
the limited diversity of the training data is a major obstacle to addressing
this problem, and we propose a new dataset that is significantly larger and
more diverse than prior work. As part of creating this dataset, we
systematically download thousands of text-to-image latent diffusion models and
sample images from them. We also collect images from dozens of popular open
source and commercial models. The resulting dataset contains 2.7M images that
have been sampled from 4803 different models. These images collectively capture
a wide range of scene content, generator architectures, and image processing
settings. Using this dataset, we study the generalization abilities of fake
image detectors. Our experiments suggest that detection performance improves as
the number of models in the training set increases, even when these models have
similar architectures. We also find that detection performance improves as the
diversity of the models increases, and that our trained detectors generalize
better than those trained on other datasets.
\\ ( https://arxiv.org/abs/2411.04125 ,  6839kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03315
Date: Tue, 8 Oct 2024 11:01:12 GMT   (3431kb,D)

Title: Learning Force Distribution Estimation for the GelSight Mini Optical
  Tactile Sensor Based on Finite Element Analysis
Authors: Erik Helmut, Luca Dziarski, Niklas Funk, Boris Belousov, Jan Peters
Categories: cs.RO cs.LG
\\
  Contact-rich manipulation remains a major challenge in robotics. Optical
tactile sensors like GelSight Mini offer a low-cost solution for contact
sensing by capturing soft-body deformations of the silicone gel. However,
accurately inferring shear and normal force distributions from these gel
deformations has yet to be fully addressed. In this work, we propose a machine
learning approach using a U-net architecture to predict force distributions
directly from the sensor's raw images. Our model, trained on force
distributions inferred from Finite Element Analysis (FEA), demonstrates
promising accuracy in predicting normal and shear force distributions. It also
shows potential for generalization across sensors of the same type and for
enabling real-time application. The codebase, dataset and models are
open-sourced and available at https://feats-ai.github.io .
\\ ( https://arxiv.org/abs/2411.03315 ,  3431kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03328
Date: Tue, 22 Oct 2024 15:32:43 GMT   (2322kb,D)

Title: Foundation Models for Rapid Autonomy Validation
Authors: Alec Farid, Peter Schleede, Aaron Huang, Christoffer Heckman
Categories: cs.RO cs.LG
\\
  We are motivated by the problem of autonomous vehicle performance validation.
A key challenge is that an autonomous vehicle requires testing in every kind of
driving scenario it could encounter, including rare events, to provide a strong
case for safety and show there is no edge-case pathological behavior.
Autonomous vehicle companies rely on potentially millions of miles driven in
realistic simulation to expose the driving stack to enough miles to estimate
rates and severity of collisions. To address scalability and coverage, we
propose the use of a behavior foundation model, specifically a masked
autoencoder (MAE), trained to reconstruct driving scenarios. We leverage the
foundation model in two complementary ways: we (i) use the learned embedding
space to group qualitatively similar scenarios together and (ii) fine-tune the
model to label scenario difficulty based on the likelihood of a collision upon
re-simulation. We use the difficulty scoring as importance weighting for the
groups of scenarios. The result is an approach which can more rapidly estimate
the rates and severity of collisions by prioritizing hard scenarios while
ensuring exposure to every kind of driving scenario.
\\ ( https://arxiv.org/abs/2411.03328 ,  2322kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03408
Date: Tue, 5 Nov 2024 18:47:22 GMT   (4573kb,D)

Title: Learning Few-Shot Object Placement with Intra-Category Transfer
Authors: Adrian R\"ofer, Russell Buchanan, Max Argus, Sethu Vijayakumar, and
  Abhinav Valada
Categories: cs.RO
Comments: 8 pages, 7 figures, 2 tables, submitted to RA-L
\\
  Efficient learning from demonstration for long-horizon tasks remains an open
challenge in robotics. While significant effort has been directed toward
learning trajectories, a recent resurgence of object-centric approaches has
demonstrated improved sample efficiency, enabling transferable robotic skills.
Such approaches model tasks as a sequence of object poses over time. In this
work, we propose a scheme for transferring observed object arrangements to
novel object instances by learning these arrangements on canonical class
frames. We then employ this scheme to enable a simple yet effective approach
for training models from as few as five demonstrations to predict arrangements
of a wide range of objects including tableware, cutlery, furniture, and desk
spaces. We propose a method for optimizing the learned models to enables
efficient learning of tasks such as setting a table or tidying up an office
with intra-category transfer, even in the presence of distractors. We present
extensive experimental results in simulation and on a real robotic system for
table setting which, based on human evaluations, scored 73.3% compared to a
human baseline. We make the code and trained models publicly available at
http://oplict.cs.uni-freiburg.de.
\\ ( https://arxiv.org/abs/2411.03408 ,  4573kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03409
Date: Tue, 5 Nov 2024 18:48:12 GMT   (5865kb,D)

Title: STEER: Flexible Robotic Manipulation via Dense Language Grounding
Authors: Laura Smith, Alex Irpan, Montserrat Gonzalez Arenas, Sean Kirmani,
  Dmitry Kalashnikov, Dhruv Shah, Ted Xiao
Categories: cs.RO cs.AI
Comments: Project website: https://lauramsmith.github.io/steer/
\\
  The complexity of the real world demands robotic systems that can
intelligently adapt to unseen situations. We present STEER, a robot learning
framework that bridges high-level, commonsense reasoning with precise, flexible
low-level control. Our approach translates complex situational awareness into
actionable low-level behavior through training language-grounded policies with
dense annotation. By structuring policy training around fundamental, modular
manipulation skills expressed in natural language, STEER exposes an expressive
interface for humans or Vision-Language Models (VLMs) to intelligently
orchestrate the robot's behavior by reasoning about the task and context. Our
experiments demonstrate the skills learned via STEER can be combined to
synthesize novel behaviors to adapt to new situations or perform completely new
tasks without additional data collection or training.
\\ ( https://arxiv.org/abs/2411.03409 ,  5865kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03416
Date: Tue, 5 Nov 2024 18:57:45 GMT   (3213kb,D)

Title: Accelerating Gaussian Variational Inference for Motion Planning Under
  Uncertainty
Authors: Zinuo Chang, Hongzhe Yu, Patricio Vela, and Yongxin Chen
Categories: cs.RO
Comments: 7 pages
\\
  This work addresses motion planning under uncertainty as a stochastic optimal
control problem. The path distribution induced by the optimal controller
corresponds to a posterior path distribution with a known form. To approximate
this posterior, we frame an optimization problem in the space of Gaussian
distributions, which aligns with the Gaussian Variational Inference Motion
Planning (GVIMP) paradigm introduced in \cite{yu2023gaussian}. In this
framework, the computation bottleneck lies in evaluating the expectation of
collision costs over a dense discretized trajectory and computing the marginal
covariances. This work exploits the sparse motion planning factor graph, which
allows for parallel computing collision costs and Gaussian Belief Propagation
(GBP) marginal covariance computation, to introduce a computationally efficient
approach to solving GVIMP. We term the novel paradigm as the Parallel Gaussian
Variational Inference Motion Planning (P-GVIMP). We validate the proposed
framework on various robotic systems, demonstrating significant speed
acceleration achieved by leveraging Graphics Processing Units (GPUs) for
parallel computation. An open-sourced implementation is presented at
https://github.com/hzyu17/VIMP.
\\ ( https://arxiv.org/abs/2411.03416 ,  3213kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03481
Date: Tue, 5 Nov 2024 20:07:27 GMT   (40287kb,D)

Title: Chance-Constrained Convex MPC for Robust Quadruped Locomotion Under
  Parametric and Additive Uncertainties
Authors: Ananya Trivedi, Sarvesh Prajapati, Mark Zolotas, Michael Everett and
  Taskin Padir
Categories: cs.RO cs.SY eess.SY
Comments: Under review for Robotics and Automation Letters
\\
  Recent advances in quadrupedal locomotion have focused on improving stability
and performance across diverse environments. However, existing methods often
lack adequate safety analysis and struggle to adapt to varying payloads and
complex terrains, typically requiring extensive tuning. To overcome these
challenges, we propose a Chance-Constrained Model Predictive Control (CCMPC)
framework that explicitly models payload and terrain variability as
distributions of parametric and additive disturbances within the single rigid
body dynamics (SRBD) model. Our approach ensures safe and consistent
performance under uncertain dynamics by expressing the model friction cone
constraints, which define the feasible set of ground reaction forces, as chance
constraints. Moreover, we solve the resulting stochastic control problem using
a computationally efficient quadratic programming formulation. Extensive Monte
Carlo simulations of quadrupedal locomotion across varying payloads and complex
terrains demonstrate that CCMPC significantly outperforms two competitive
benchmarks: Linear MPC (LMPC) and MPC with hand-tuned safety margins to
maintain stability, reduce foot slippage, and track the center of mass.
Hardware experiments on the Unitree Go1 robot show successful locomotion across
various indoor and outdoor terrains with unknown loads exceeding 50% of the
robot body weight, despite no additional parameter tuning. A video of the
results and accompanying code can be found at: https://cc-mpc.github.io/.
\\ ( https://arxiv.org/abs/2411.03481 ,  40287kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03483
Date: Tue, 5 Nov 2024 20:07:47 GMT   (18812kb,D)

Title: Augmented-Reality Enabled Crop Monitoring with Robot Assistance
Authors: Caio Mucchiani, Dimitrios Chatziparaschis and Konstantinos Karydis
Categories: cs.RO
\\
  The integration of augmented reality (AR), extended reality (XR), and virtual
reality (VR) technologies in agriculture has shown significant promise in
enhancing various agricultural practices. Mobile robots have also been adopted
as assessment tools in precision agriculture, improving economic efficiency and
productivity, and minimizing undesired effects such as weeds and pests. Despite
considerable work on both fronts, the combination of a versatile User Interface
(UI) provided by an AR headset with the integration and direct interaction and
control of a mobile field robot has not yet been fully explored or
standardized. This work aims to address this gap by providing real-time data
input and control output of a mobile robot for precision agriculture through a
virtual environment enabled by an AR headset interface. The system leverages
open-source computational tools and off-the-shelf hardware for effective
integration. Distinctive case studies are presented where growers or
technicians can interact with a legged robot via an AR headset and a UI. Users
can teleoperate the robot to gather information in an area of interest, request
real-time graphed status of an area, or have the robot autonomously navigate to
selected areas for measurement updates. The proposed system utilizes a custom
local navigation method with a fixed holographic coordinate system in
combination with QR codes. This step toward fusing AR and robotics in
agriculture aims to provide practical solutions for real-time data management
and control enabled by human-robot interaction. The implementation can be
extended to various robot applications in agriculture and beyond, promoting a
unified framework for on-demand and autonomous robot operation in the field.
\\ ( https://arxiv.org/abs/2411.03483 ,  18812kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03487
Date: Tue, 5 Nov 2024 20:11:41 GMT   (6268kb,D)

Title: Enhancing Exploratory Capability of Visual Navigation Using Uncertainty
  of Implicit Scene Representation
Authors: Yichen Wang, Qiming Liu, Zhe Liu, and Hesheng Wang
Categories: cs.RO
\\
  In the context of visual navigation in unknown scenes, both "exploration" and
"exploitation" are equally crucial. Robots must first establish environmental
cognition through exploration and then utilize the cognitive information to
accomplish target searches. However, most existing methods for image-goal
navigation prioritize target search over the generation of exploratory
behavior. To address this, we propose the Navigation with Uncertainty-driven
Exploration (NUE) pipeline, which uses an implicit and compact scene
representation, NeRF, as a cognitive structure. We estimate the uncertainty of
NeRF and augment the exploratory ability by the uncertainty to in turn
facilitate the construction of implicit representation. Simultaneously, we
extract memory information from NeRF to enhance the robot's reasoning ability
for determining the location of the target. Ultimately, we seamlessly combine
the two generated abilities to produce navigational actions. Our pipeline is
end-to-end, with the environmental cognitive structure being constructed
online. Extensive experimental results on image-goal navigation demonstrate the
capability of our pipeline to enhance exploratory behaviors, while also
enabling a natural transition from the exploration to exploitation phase. This
enables our model to outperform existing memory-based cognitive navigation
structures in terms of navigation performance.
\\ ( https://arxiv.org/abs/2411.03487 ,  6268kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03494
Date: Tue, 5 Nov 2024 20:18:29 GMT   (40321kb,D)

Title: An Open-source Sim2Real Approach for Sensor-independent Robot Navigation
  in a Grid
Authors: Murad Mehrab Abrar, Souryadeep Mondal, Michelle Hickner
Categories: cs.RO cs.LG
Comments: Accepted for publication at the 9th IEEE International Conference on
  Robotics and Automation Engineering (IEEE ICRAE 2024), Singapore
\\
  This paper presents a Sim2Real (Simulation to Reality) approach to bridge the
gap between a trained agent in a simulated environment and its real-world
implementation in navigating a robot in a similar setting. Specifically, we
focus on navigating a quadruped robot in a real-world grid-like environment
inspired by the Gymnasium Frozen Lake -- a highly user-friendly and free
Application Programming Interface (API) to develop and test Reinforcement
Learning (RL) algorithms. We detail the development of a pipeline to transfer
motion policies learned in the Frozen Lake simulation to a physical quadruped
robot, thus enabling autonomous navigation and obstacle avoidance in a grid
without relying on expensive localization and mapping sensors. The work
involves training an RL agent in the Frozen Lake environment and utilizing the
resulting Q-table to control a 12 Degrees-of-Freedom (DOF) quadruped robot. In
addition to detailing the RL implementation, inverse kinematics-based quadruped
gaits, and the transfer policy pipeline, we open-source the project on GitHub
and include a demonstration video of our Sim2Real transfer approach. This work
provides an accessible, straightforward, and low-cost framework for
researchers, students, and hobbyists to explore and implement RL-based robot
navigation in real-world grid environments.
\\ ( https://arxiv.org/abs/2411.03494 ,  40321kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03532
Date: Tue, 5 Nov 2024 22:15:28 GMT   (6989kb,D)

Title: A Behavior Architecture for Fast Humanoid Robot Door Traversals
Authors: Duncan Calvert, Luigi Penco, Dexton Anderson, Tomasz Bialek, Arghya
  Chatterjee, Bhavyansh Mishra, Geoffrey Clark, Sylvain Bertrand and Robert
  Griffin
Categories: cs.RO
Comments: 15 pages, 23 figure, for submission to Elsevier RAS
\\
  Towards the role of humanoid robots as squad mates in urban operations and
other domains, we identified doors as a major area lacking capability
development. In this paper, we focus on the ability of humanoid robots to
navigate and deal with doors. Human-sized doors are ubiquitous in many
environment domains and the humanoid form factor is uniquely suited to operate
and traverse them. We present an architecture which incorporates GPU
accelerated perception and a tree based interactive behavior coordination
system with a whole body motion and walking controller. Our system is capable
of performing door traversals on a variety of door types. It supports rapid
authoring of behaviors for unseen door types and techniques to achieve
re-usability of those authored behaviors. The behaviors are modelled using
trees and feature logical reactivity and action sequences that can be executed
with layered concurrency to increase speed. Primitive actions are built on top
of our existing whole body controller which supports manipulation while
walking. We include a perception system using both neural networks and
classical computer vision for door mechanism detection outside of the lab
environment. We present operator-robot interdependence analysis charts to
explore how human cognition is combined with artificial intelligence to produce
complex robot behavior. Finally, we present and discuss real robot performances
of fast door traversals on our Nadia humanoid robot. Videos online at
https://www.youtube.com/playlist?list=PLXuyT8w3JVgMPaB5nWNRNHtqzRK8i68dy.
\\ ( https://arxiv.org/abs/2411.03532 ,  6989kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03540
Date: Tue, 5 Nov 2024 22:42:41 GMT   (1344kb,D)

Title: VLA-3D: A Dataset for 3D Semantic Scene Understanding and Navigation
Authors: Haochen Zhang, Nader Zantout, Pujith Kachana, Zongyuan Wu, Ji Zhang,
  Wenshan Wang
Categories: cs.RO
Comments: Accepted and presented at the 1st Workshop on Semantic Reasoning and
  Goal Understanding in Robotics (SemRob), Robotics Science and Systems
  Conference (RSS 2024)
\\
  With the recent rise of Large Language Models (LLMs), Vision-Language Models
(VLMs), and other general foundation models, there is growing potential for
multimodal, multi-task embodied agents that can operate in diverse environments
given only natural language as input. One such application area is indoor
navigation using natural language instructions. However, despite recent
progress, this problem remains challenging due to the spatial reasoning and
semantic understanding required, particularly in arbitrary scenes that may
contain many objects belonging to fine-grained classes. To address this
challenge, we curate the largest real-world dataset for Vision and
Language-guided Action in 3D Scenes (VLA-3D), consisting of over 11.5K scanned
3D indoor rooms from existing datasets, 23.5M heuristically generated semantic
relations between objects, and 9.7M synthetically generated referential
statements. Our dataset consists of processed 3D point clouds, semantic object
and room annotations, scene graphs, navigable free space annotations, and
referential language statements that specifically focus on view-independent
spatial relations for disambiguating objects. The goal of these features is to
aid the downstream task of navigation, especially on real-world systems where
some level of robustness must be guaranteed in an open world of changing scenes
and imperfect language. We benchmark our dataset with current state-of-the-art
models to obtain a performance baseline. All code to generate and visualize the
dataset is publicly released, see https://github.com/HaochenZ11/VLA-3D. With
the release of this dataset, we hope to provide a resource for progress in
semantic 3D scene understanding that is robust to changes and one which will
aid the development of interactive indoor navigation systems.
\\ ( https://arxiv.org/abs/2411.03540 ,  1344kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03556
Date: Tue, 5 Nov 2024 23:34:27 GMT   (3125kb,D)

Title: VQ-ACE: Efficient Policy Search for Dexterous Robotic Manipulation via
  Action Chunking Embedding
Authors: Chenyu Yang, Davide Liconti, Robert K. Katzschmann
Categories: cs.RO
\\
  Dexterous robotic manipulation remains a significant challenge due to the
high dimensionality and complexity of hand movements required for tasks like
in-hand manipulation and object grasping. This paper addresses this issue by
introducing Vector Quantized Action Chunking Embedding (VQ-ACE), a novel
framework that compresses human hand motion into a quantized latent space,
significantly reducing the action space's dimensionality while preserving key
motion characteristics. By integrating VQ-ACE with both Model Predictive
Control (MPC) and Reinforcement Learning (RL), we enable more efficient
exploration and policy learning in dexterous manipulation tasks using a
biomimetic robotic hand. Our results show that latent space sampling with MPC
produces more human-like behavior in tasks such as Ball Rolling and Object
Picking, leading to higher task success rates and reduced control costs. For
RL, action chunking accelerates learning and improves exploration, demonstrated
through faster convergence in tasks like cube stacking and in-hand cube
reorientation. These findings suggest that VQ-ACE offers a scalable and
effective solution for robotic manipulation tasks involving complex,
high-dimensional state spaces, contributing to more natural and adaptable
robotic systems.
\\ ( https://arxiv.org/abs/2411.03556 ,  3125kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03581
Date: Wed, 6 Nov 2024 00:40:53 GMT   (43892kb,D)

Title: Can Robotic Cues Manipulate Human Decisions? Exploring Consensus
  Building via Bias-Controlled Non-linear Opinion Dynamics and Robotic Eye Gaze
  Mediated Interaction in Human-Robot Teaming
Authors: Rajul Kumar, Adam Bhatti, Ningshi Yao
Categories: cs.RO cs.SY eess.SY
Comments: 35 pages, 14 figures
\\
  Although robots are becoming more advanced with human-like anthropomorphic
features and decision-making abilities to improve collaboration, the active
integration of humans into this process remains under-explored. This article
presents the first experimental study exploring decision-making interactions
between humans and robots with visual cues from robotic eyes, which can
dynamically influence human opinion formation. The cues generated by robotic
eyes gradually guide human decisions towards alignment with the robot's
choices. Both human and robot decision-making processes are modeled as
non-linear opinion dynamics with evolving biases. To examine these opinion
dynamics under varying biases, we conduct numerical parametric and equilibrium
continuation analyses using tuned parameters designed explicitly for the
presented human-robot interaction experiment. Furthermore, to facilitate the
transition from disagreement to agreement, we introduced a human opinion
observation algorithm integrated with the formation of the robot's opinion,
where the robot's behavior is controlled based on its formed opinion. The
algorithms developed aim to enhance human involvement in consensus building,
fostering effective collaboration between humans and robots. Experiments with
51 participants (N = 51) show that human-robot teamwork can be improved by
guiding human decisions using robotic cues. Finally, we provide detailed
insights on the effects of trust, cognitive load, and participant demographics
on decision-making based on user feedback and post-experiment interviews.
\\ ( https://arxiv.org/abs/2411.03581 ,  43892kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03591
Date: Wed, 6 Nov 2024 01:11:39 GMT   (11932kb,D)

Title: vMF-Contact: Uncertainty-aware Evidential Learning for Probabilistic
  Contact-grasp in Noisy Clutter
Authors: Yitian Shi, Edgar Welte, Maximilian Gilles, Rania Rayyes
Categories: cs.RO
\\
  Grasp learning in noisy environments, such as occlusions, sensor noise, and
out-of-distribution (OOD) objects, poses significant challenges. Recent
learning-based approaches focus primarily on capturing aleatoric uncertainty
from inherent data noise. The epistemic uncertainty, which represents the OOD
recognition, is often addressed by ensembles with multiple forward paths,
limiting real-time application. In this paper, we propose an uncertainty-aware
approach for 6-DoF grasp detection using evidential learning to comprehensively
capture both uncertainties in real-world robotic grasping. As a key
contribution, we introduce vMF-Contact, a novel architecture for learning
hierarchical contact grasp representations with probabilistic modeling of
directional uncertainty as von Mises-Fisher (vMF) distribution. To achieve
this, we derive and analyze the theoretical formulation of the second-order
objective on the posterior parametrization, providing formal guarantees for the
model's ability to quantify uncertainty and improve grasp prediction
performance. Moreover, we enhance feature expressiveness by applying partial
point reconstructions as an auxiliary task, improving the comprehension of
uncertainty quantification as well as the generalization to unseen objects. In
the real-world experiments, our method demonstrates a significant improvement
by 39% in the overall clearance rate compared to the baselines. Video is under
https://www.youtube.com/watch?v=4aQsrDgdV8Y&t=12s
\\ ( https://arxiv.org/abs/2411.03591 ,  11932kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03610
Date: Wed, 6 Nov 2024 02:05:44 GMT   (8111kb,D)

Title: LCP-Fusion: A Neural Implicit SLAM with Enhanced Local Constraints and
  Computable Prior
Authors: Jiahui Wang, Yinan Deng, Yi Yang and Yufeng Yue
Categories: cs.RO cs.CV
Comments: Accepted by 2024 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2024)
\\
  Recently the dense Simultaneous Localization and Mapping (SLAM) based on
neural implicit representation has shown impressive progress in hole filling
and high-fidelity mapping. Nevertheless, existing methods either heavily rely
on known scene bounds or suffer inconsistent reconstruction due to drift in
potential loop-closure regions, or both, which can be attributed to the
inflexible representation and lack of local constraints. In this paper, we
present LCP-Fusion, a neural implicit SLAM system with enhanced local
constraints and computable prior, which takes the sparse voxel octree structure
containing feature grids and SDF priors as hybrid scene representation,
enabling the scalability and robustness during mapping and tracking. To enhance
the local constraints, we propose a novel sliding window selection strategy
based on visual overlap to address the loop-closure, and a practical warping
loss to constrain relative poses. Moreover, we estimate SDF priors as coarse
initialization for implicit features, which brings additional explicit
constraints and robustness, especially when a light but efficient adaptive
early ending is adopted. Experiments demonstrate that our method achieve better
localization accuracy and reconstruction consistency than existing RGB-D
implicit SLAM, especially in challenging real scenes (ScanNet) as well as
self-captured scenes with unknown scene bounds. The code is available at
https://github.com/laliwang/LCP-Fusion.
\\ ( https://arxiv.org/abs/2411.03610 ,  8111kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03614
Date: Wed, 6 Nov 2024 02:16:26 GMT   (1735kb,D)

Title: Robot Swarming over the internet
Authors: Will Ferenc and Hannah Kastein and Lauren Lieu and Ryan Wilson and
  Yuan Rick Huang and Jerome Gilles and Andrea L. Bertozzi and Balaji R. Sharma
  and Baisravan HomChaudhuri and Subramanian Ramakrishnan and Manish Kumar
Categories: cs.RO
Journal-ref: 012 American Control Conference, Montreal, Canada, June 2012
DOI: 10.1109/ACC.2012.6315420
\\
  This paper considers cooperative control of robots involving two different
testbed systems in remote locations with communication on the internet. This
provides us the capability to exchange robots status like positions, velocities
and directions needed for the swarming algorithm. The results show that all
robots properly follow some leader defined one of the testbeds. Measurement of
data exchange rates show no loss of packets, and average transfer delays stay
within tolerance limits for practical applications. In our knowledge, the
novelty of this paper concerns this kind of control over a large network like
internet.
\\ ( https://arxiv.org/abs/2411.03614 ,  1735kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03619
Date: Wed, 6 Nov 2024 02:24:27 GMT   (3774kb,D)

Title: Real-Time Safe Bipedal Robot Navigation using Linear Discrete Control
  Barrier Functions
Authors: Chengyang Peng, Victor Paredes, Guillermo A. Castillo and Ayonga
  Hereid
Categories: cs.RO cs.SY eess.SY
Comments: 7 pages, 10 figures
\\
  Safe navigation in real-time is an essential task for humanoid robots in
real-world deployment. Since humanoid robots are inherently underactuated
thanks to unilateral ground contacts, a path is considered safe if it is
obstacle-free and respects the robot's physical limitations and underlying
dynamics. Existing approaches often decouple path planning from gait control
due to the significant computational challenge caused by the full-order robot
dynamics. In this work, we develop a unified, safe path and gait planning
framework that can be evaluated online in real-time, allowing the robot to
navigate clustered environments while sustaining stable locomotion. Our
approach uses the popular Linear Inverted Pendulum (LIP) model as a template
model to represent walking dynamics. It incorporates heading angles in the
model to evaluate kinematic constraints essential for physically feasible gaits
properly. In addition, we leverage discrete control barrier functions (DCBF)
for obstacle avoidance, ensuring that the subsequent foot placement provides a
safe navigation path within clustered environments. To guarantee real-time
computation, we use a novel approximation of the DCBF to produce linear DCBF
(LDCBF) constraints. We validate the proposed approach in simulation using a
Digit robot in randomly generated environments. The results demonstrate that
our approach can generate safe gaits for a non-trivial humanoid robot to
navigate environments with randomly generated obstacles in real-time.
\\ ( https://arxiv.org/abs/2411.03619 ,  3774kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03660
Date: Wed, 6 Nov 2024 04:41:58 GMT   (9933kb,D)

Title: Development of a Practical Articulated Wheeled In-pipe Robot for Both
  3-4 in Force Main Inspection of Sewer Pipes
Authors: Kenya Murata and Atsushi Kakogawa
Categories: cs.RO
Comments: The Twenty-Ninth International Symposium on Artificial Life and
  Robotics 2024 (AROB 29th 2024), The Ninth International Symposium on
  BioComplexity 2024 (ISBC 9th 2024), The Seventh International Symposium on
  Swarm Behavior and Bio-Inspired Robotics 2024 (SWARM 7th 2024) B-Con Plaza,
  Beppu, Japan and ONLINE, January 24-26, 2024
\\
  This paper reports a practical articulated wheeled in-pipe inspection robot
"AIRo-7.1" which is waterproof and dustproof, and can adapt to 3 to 4 in inner
diameters. The joint torque can be adjusted by a PWM open-loop control. The
middle joint angle can be controlled by a position feedback control system
while the other two joints are bent by torsional springs. Thanks to this simple
and high-density design, not only downsizing of the robot but also wide range
of the adaptive inner diameter were achieved. However, the relationship between
the actual middle joint torque value and the PWM duty ratio should be pre-known
because the reducer used in AIRo-7.1 was designed by ourselves. Therefore,
preliminary experiments were conducted to clarify the relationship between
them. To examine the adaptive movement, experiments in both 3 in and 4 in pipes
with vertical, bend, and diameter change sections. Finally, field experiment
was also conducted. From the results, high adaptability to different inner
diameters of pipes and slippery environments were confirmed although waterproof
and dustproof were not perfectly working.
\\ ( https://arxiv.org/abs/2411.03660 ,  9933kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03669
Date: Wed, 6 Nov 2024 05:08:49 GMT   (10942kb,D)

Title: Imagined Potential Games: A Framework for Simulating, Learning and
  Evaluating Interactive Behaviors
Authors: Lingfeng Sun, Yixiao Wang, Pin-Yun Hung, Changhao Wang, Xiang Zhang,
  Zhuo Xu, Masayoshi Tomizuka
Categories: cs.RO
Comments: 13 pages, 10 figures. arXiv admin note: substantial text overlap with
  arXiv:2310.01614
\\
  Interacting with human agents in complex scenarios presents a significant
challenge for robotic navigation, particularly in environments that necessitate
both collision avoidance and collaborative interaction, such as indoor spaces.
Unlike static or predictably moving obstacles, human behavior is inherently
complex and unpredictable, stemming from dynamic interactions with other
agents. Existing simulation tools frequently fail to adequately model such
reactive and collaborative behaviors, impeding the development and evaluation
of robust social navigation strategies. This paper introduces a novel framework
utilizing distributed potential games to simulate human-like interactions in
highly interactive scenarios. Within this framework, each agent imagines a
virtual cooperative game with others based on its estimation. We demonstrate
this formulation can facilitate the generation of diverse and realistic
interaction patterns in a configurable manner across various scenarios.
Additionally, we have developed a gym-like environment leveraging our
interactive agent model to facilitate the learning and evaluation of
interactive navigation algorithms.
\\ ( https://arxiv.org/abs/2411.03669 ,  10942kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03682
Date: Wed, 6 Nov 2024 06:06:07 GMT   (23281kb,D)

Title: LEGATO: Cross-Embodiment Imitation Using a Grasping Tool
Authors: Mingyo Seo, H. Andy Park, Shenli Yuan, Yuke Zhu, Luis Sentis
Categories: cs.RO
Comments: Submitted to RA-L
\\
  Cross-embodiment imitation learning enables policies trained on specific
embodiments to transfer across different robots, unlocking the potential for
large-scale imitation learning that is both cost-effective and highly reusable.
This paper presents LEGATO, a cross-embodiment imitation learning framework for
visuomotor skill transfer across varied kinematic morphologies. We introduce a
handheld gripper that unifies action and observation spaces, allowing tasks to
be defined consistently across robots. Using this gripper, we train visuomotor
policies via imitation learning, applying a motion-invariant transformation to
compute the training loss. Gripper motions are then retargeted into
high-degree-of-freedom whole-body motions using inverse kinematics for
deployment across diverse embodiments. Our evaluations in simulation and
real-robot experiments highlight the framework's effectiveness in learning and
transferring visuomotor skills across various robots. More information can be
found at the project page: https://ut-hcrl.github.io/LEGATO.
\\ ( https://arxiv.org/abs/2411.03682 ,  23281kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03747
Date: Wed, 6 Nov 2024 08:22:34 GMT   (1235kb,D)

Title: Observability-Aware Control for Cooperatively Localizing Quadrotor UAVs
Authors: H S Helson Go, Ching Lok Chong, Longhao Qian, Hugh H.-T. Liu
Categories: cs.RO
Comments: 12 pages, 5 figures
\\
  Cooperatively Localizing robots should seek optimal control strategies to
maximize precision of position estimation and ensure safety in flight.
Observability-Aware Trajectory Optimization has strong potential to address
this issue, but no concrete link between observability and precision has been
proven yet. In this paper, we prove that improvement in positioning precision
inherently follows from optimizing observability. Based on this finding, we
develop an Observability-Aware Control principle to generate
observability-optimal control strategies. We implement this principle in a
Model Predictive Control framework, and we verify it on a team of quadrotor
Unmanned Aerial Vehicles comprising a follower vehicle localizing itself by
tracking a leader vehicle in both simulations and real-world flight tests. Our
results demonstrate that maximizing observability contributed to improving
global positioning precision for the quadrotor team.
\\ ( https://arxiv.org/abs/2411.03747 ,  1235kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03838
Date: Wed, 6 Nov 2024 11:18:13 GMT   (3772kb,D)

Title: Fundamental Three-Dimensional Configuration of Wire-Wound Muscle-Tendon
  Complex Drive
Authors: Yoshimoto Ribayashi, Yuta Sahara, Shogo Sawaguchi, Kazuhiro Miyama,
  Akihiro Miki, Kento Kawaharazuka, Kei Okada, Masayuki Inaba
Categories: cs.RO
Comments: Accepted at Humanoids2024, website -
  https://sites.google.com/view/yoshimoto-ribayashi/projects, YouTube -
  https://youtu.be/EDeAqg7aAb4
\\
  For robots to become more versatile and expand their areas of application,
their bodies need to be suitable for contact with the environment. When the
human body comes into contact with the environment, it is possible for it to
continue to move even if the positional relationship between muscles or the
shape of the muscles changes. We have already focused on the effect of
geometric deformation of muscles and proposed a drive system called wire-wound
Muscle-Tendon Complex (ww-MTC), an extension of the wire drive system. Our
previous study using a robot with a two-dimensional configuration demonstrated
several advantages: reduced wire loosening, interference, and wear; improved
robustness during environmental contact; and a muscular appearance. However,
this design had some problems, such as excessive muscle expansion that hindered
inter-muscle movement, and confinement to planar motion. In this study, we
develop the ww-MTC into a three-dimensional shape. We present a fundamental
construction method for a muscle exterior that expands gently and can be
contacted over its entire surface. We also apply the three-dimensional ww-MTC
to a 2-axis 3-muscle robot, and confirm that the robot can continue to move
while adapting to its environment.
\\ ( https://arxiv.org/abs/2411.03838 ,  3772kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03873
Date: Wed, 6 Nov 2024 12:40:59 GMT   (11000kb,D)

Title: Biomechanics-Aware Trajectory Optimization for Navigation during Robotic
  Physiotherapy
Authors: Italo Belli, J. Micah Prendergast, Ajay Seth, Luka Peternel
Categories: cs.RO cs.SY eess.SY
Comments: 13 pages, 9 figures, under review
\\
  Robotic devices hold promise for aiding patients in orthopedic
rehabilitation. However, current robotic-assisted physiotherapy methods
struggle including biomechanical metrics in their control algorithms, crucial
for safe and effective therapy. This paper introduces BATON, a
Biomechanics-Aware Trajectory Optimization approach to robotic Navigation of
human musculoskeletal loads. The method integrates a high-fidelity
musculoskeletal model of the human shoulder into real-time control of
robot-patient interaction during rotator cuff tendon rehabilitation. We extract
skeletal dynamics and tendon loading information from an OpenSim shoulder model
to solve an optimal control problem, generating strain-minimizing trajectories.
Trajectories were realized on a healthy subject by an impedance-controlled
robot while estimating the state of the subject's shoulder. Target poses were
prescribed to design personalized rehabilitation across a wide range of
shoulder motion avoiding high-strain areas. BATON was designed with real-time
capabilities, enabling continuous trajectory replanning to address unforeseen
variations in tendon strain, such as those from changing muscle activation of
the subject.
\\ ( https://arxiv.org/abs/2411.03873 ,  11000kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03928
Date: Wed, 6 Nov 2024 14:03:49 GMT   (13832kb,D)

Title: DEIO: Deep Event Inertial Odometry
Authors: Weipeng Guan, Fuling Lin, Peiyu Chen, Peng Lu
Categories: cs.RO
\\
  Event cameras are bio-inspired, motion-activated sensors that demonstrate
impressive potential in handling challenging situations, such as motion blur
and high-dynamic range. Despite their promise, existing event-based
simultaneous localization and mapping (SLAM) approaches exhibit limited
performance in real-world applications. On the other hand, state-of-the-art
SLAM approaches that incorporate deep neural networks for better robustness and
applicability. However, these is a lack of research in fusing learning-based
event SLAM methods with IMU, which could be indispensable to push the
event-based SLAM to large-scale, low-texture or complex scenarios. In this
paper, we propose DEIO, the first monocular deep event-inertial odometry
framework that combines learning-based method with traditional nonlinear
graph-based optimization. Specifically, we tightly integrate a trainable
event-based differentiable bundle adjustment (e-DBA) with the IMU
pre-integration in a factor graph which employs keyframe-based sliding window
optimization. Numerical Experiments in nine public challenge datasets show that
our method can achieve superior performance compared with the image-based and
event-based benchmarks. The source code is available at:
https://github.com/arclab-hku/DEIO.
\\ ( https://arxiv.org/abs/2411.03928 ,  13832kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03951
Date: Wed, 6 Nov 2024 14:33:30 GMT   (582kb,D)

Title: Continuous-Time State Estimation Methods in Robotics: A Survey
Authors: William Talbot, Julian Nubert, Turcan Tuna, Cesar Cadena, Frederike
  D\"umbgen, Jesus Tordesillas, Timothy D. Barfoot, Marco Hutter
Categories: cs.RO
Comments: Submitted to IEEE Transactions on Robotics (T-RO)
\\
  Accurate, efficient, and robust state estimation is more important than ever
in robotics as the variety of platforms and complexity of tasks continue to
grow. Historically, discrete-time filters and smoothers have been the dominant
approach, in which the estimated variables are states at discrete sample times.
The paradigm of continuous-time state estimation proposes an alternative
strategy by estimating variables that express the state as a continuous
function of time, which can be evaluated at any query time. Not only can this
benefit downstream tasks such as planning and control, but it also
significantly increases estimator performance and flexibility, as well as
reduces sensor preprocessing and interfacing complexity. Despite this,
continuous-time methods remain underutilized, potentially because they are less
well-known within robotics. To remedy this, this work presents a unifying
formulation of these methods and the most exhaustive literature review to date,
systematically categorizing prior work by methodology, application, state
variables, historical context, and theoretical contribution to the field. By
surveying splines and Gaussian processes together and contextualizing works
from other research domains, this work identifies and analyzes open problems in
continuous-time state estimation and suggests new research directions.
\\ ( https://arxiv.org/abs/2411.03951 ,  582kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03990
Date: Wed, 6 Nov 2024 15:30:42 GMT   (4892kb,D)

Title: ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy
Authors: Chenrui Tie, Yue Chen, Ruihai Wu, Boxuan Dong, Zeyi Li, Chongkai Gao,
  Hao Dong
Categories: cs.RO cs.CV cs.LG
Comments: Accept to CoRL 2024 Workshop on X-Embodiment Robot Learning
\\
  Imitation learning, e.g., diffusion policy, has been proven effective in
various robotic manipulation tasks. However, extensive demonstrations are
required for policy robustness and generalization. To reduce the demonstration
reliance, we leverage spatial symmetry and propose ET-SEED, an efficient
trajectory-level SE(3) equivariant diffusion model for generating action
sequences in complex robot manipulation tasks. Further, previous equivariant
diffusion models require the per-step equivariance in the Markov process,
making it difficult to learn policy under such strong constraints. We
theoretically extend equivariant Markov kernels and simplify the condition of
equivariant diffusion process, thereby significantly improving training
efficiency for trajectory-level SE(3) equivariant diffusion policy in an
end-to-end manner. We evaluate ET-SEED on representative robotic manipulation
tasks, involving rigid body, articulated and deformable object. Experiments
demonstrate superior data efficiency and manipulation proficiency of our
proposed method, as well as its ability to generalize to unseen configurations
with only a few demonstrations. Website: https://et-seed.github.io/
\\ ( https://arxiv.org/abs/2411.03990 ,  4892kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04005
Date: Wed, 6 Nov 2024 15:44:10 GMT   (12361kb,D)

Title: Object-Centric Dexterous Manipulation from Human Motion Data
Authors: Yuanpei Chen, Chen Wang, Yaodong Yang, C. Karen Liu
Categories: cs.RO
Comments: 20 pages, 7 figures
\\
  Manipulating objects to achieve desired goal states is a basic but important
skill for dexterous manipulation. Human hand motions demonstrate proficient
manipulation capability, providing valuable data for training robots with
multi-finger hands. Despite this potential, substantial challenges arise due to
the embodiment gap between human and robot hands. In this work, we introduce a
hierarchical policy learning framework that uses human hand motion data for
training object-centric dexterous robot manipulation. At the core of our method
is a high-level trajectory generative model, learned with a large-scale human
hand motion capture dataset, to synthesize human-like wrist motions conditioned
on the desired object goal states. Guided by the generated wrist motions, deep
reinforcement learning is further used to train a low-level finger controller
that is grounded in the robot's embodiment to physically interact with the
object to achieve the goal. Through extensive evaluation across 10 household
objects, our approach not only demonstrates superior performance but also
showcases generalization capability to novel object geometries and goal states.
Furthermore, we transfer the learned policies from simulation to a real-world
bimanual dexterous robot system, further demonstrating its applicability in
real-world scenarios. Project website:
https://cypypccpy.github.io/obj-dex.github.io/.
\\ ( https://arxiv.org/abs/2411.04005 ,  12361kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04006
Date: Wed, 6 Nov 2024 15:44:59 GMT   (27023kb,D)

Title: Select2Plan: Training-Free ICL-Based Planning through VQA and Memory
  Retrieval
Authors: Davide Buoso, Luke Robinson, Giuseppe Averta, Philip Torr, Tim
  Franzmeyer, Daniele De Martini
Categories: cs.RO cs.AI
\\
  This study explores the potential of off-the-shelf Vision-Language Models
(VLMs) for high-level robot planning in the context of autonomous navigation.
Indeed, while most of existing learning-based approaches for path planning
require extensive task-specific training/fine-tuning, we demonstrate how such
training can be avoided for most practical cases. To do this, we introduce
Select2Plan (S2P), a novel training-free framework for high-level robot
planning which completely eliminates the need for fine-tuning or specialised
training. By leveraging structured Visual Question-Answering (VQA) and
In-Context Learning (ICL), our approach drastically reduces the need for data
collection, requiring a fraction of the task-specific data typically used by
trained models, or even relying only on online data. Our method facilitates the
effective use of a generally trained VLM in a flexible and cost-efficient way,
and does not require additional sensing except for a simple monocular camera.
We demonstrate its adaptability across various scene types, context sources,
and sensing setups. We evaluate our approach in two distinct scenarios:
traditional First-Person View (FPV) and infrastructure-driven Third-Person View
(TPV) navigation, demonstrating the flexibility and simplicity of our method.
Our technique significantly enhances the navigational capabilities of a
baseline VLM of approximately 50% in TPV scenario, and is comparable to trained
models in the FPV one, with as few as 20 demonstrations.
\\ ( https://arxiv.org/abs/2411.04006 ,  27023kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04046
Date: Wed, 6 Nov 2024 16:51:30 GMT   (7439kb,D)

Title: Design and control of a robotic payload stabilization mechanism for
  rocket flights
Authors: Utkarsh Anand, Diya Parekh, Thakur Pranav G. Singh, Hrishikesh S.
  Yadav, Ramya S. Moorthy and Srinivas G
Categories: cs.RO cs.SY eess.SY
Comments: For code and design files, refer to
  https://github.com/utkarshanand140/Stewie-Robot
\\
  The use of parallel manipulators in aerospace engineering has gained
significant attention due to their ability to provide improved stability and
precision. This paper presents the design, control, and analysis of 'STEWIE',
which is a three-degree-of-freedom (DoF) parallel manipulator robot developed
by members of the thrustMIT rocketry team, as a payload stabilization mechanism
for their sounding rocket, 'Altair'. The goal of the robot was to demonstrate
the attitude control of the parallel plate against the continuous change in
orientation experienced by the rocket during its flight, stabilizing the
payloads. At the same time, the high gravitational forces (G-forces) and
vibrations experienced by the sounding rocket are counteracted. A novel design
of the mechanism, inspired by a standard Stewart platform, is proposed which
was down-scaled to fit inside a 4U CubeSat within its space constraints. The
robot uses three micro servo motors to actuate the links that control the
alignment of the parallel plate. In addition to the actuation mechanism, a
robust control system for its manipulation was developed for the robot. The
robot represents a significant advancement in the field of space robotics in
the aerospace industry by demonstrating the successful implementation of
complex robotic mechanisms in small, confined spaces such as CubeSats, which
are standard form factors for large payloads in the aerospace industry.
\\ ( https://arxiv.org/abs/2411.04046 ,  7439kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04050
Date: Wed, 6 Nov 2024 16:57:36 GMT   (6028kb,D)

Title: Memorized action chunking with Transformers: Imitation learning for
  vision-based tissue surface scanning
Authors: Bochen Yang, Kaizhong Deng, Christopher J Peters, George Mylonas,
  Daniel S. Elson
Categories: cs.RO
\\
  Optical sensing technologies are emerging technologies used in cancer
surgeries to ensure the complete removal of cancerous tissue. While point-wise
assessment has many potential applications, incorporating automated large area
scanning would enable holistic tissue sampling. However, such scanning tasks
are challenging due to their long-horizon dependency and the requirement for
fine-grained motion. To address these issues, we introduce Memorized Action
Chunking with Transformers (MACT), an intuitive yet efficient imitation
learning method for tissue surface scanning tasks. It utilizes a sequence of
past images as historical information to predict near-future action sequences.
In addition, hybrid temporal-spatial positional embeddings were employed to
facilitate learning. In various simulation settings, MACT demonstrated
significant improvements in contour scanning and area scanning over the
baseline model. In real-world testing, with only 50 demonstration trajectories,
MACT surpassed the baseline model by achieving a 60-80% success rate on all
scanning tasks. Our findings suggest that MACT is a promising model for
adaptive scanning in surgical settings.
\\ ( https://arxiv.org/abs/2411.04050 ,  6028kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04056
Date: Wed, 6 Nov 2024 17:05:58 GMT   (2678kb,D)

Title: Problem Space Transformations for Generalisation in Behavioural Cloning
Authors: Kiran Doshi, Marco Bagatella, Stelian Coros
Categories: cs.RO cs.LG
\\
  The combination of behavioural cloning and neural networks has driven
significant progress in robotic manipulation. As these algorithms may require a
large number of demonstrations for each task of interest, they remain
fundamentally inefficient in complex scenarios. This issue is aggravated when
the system is treated as a black-box, ignoring its physical properties. This
work characterises widespread properties of robotic manipulation, such as pose
equivariance and locality. We empirically demonstrate that transformations
arising from each of these properties allow neural policies trained with
behavioural cloning to better generalise to out-of-distribution problem
instances.
\\ ( https://arxiv.org/abs/2411.04056 ,  2678kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04073
Date: Wed, 6 Nov 2024 17:50:32 GMT   (4188kb,D)

Title: Rescheduling after vehicle failures in the multi-depot rural postman
  problem with rechargeable and reusable vehicles
Authors: Eashwar Sathyamurthy, Jeffrey W. Herrmann, Shapour Azarm
Categories: cs.RO cs.CC cs.MA
\\
  We present a centralized auction algorithm to solve the Multi-Depot Rural
Postman Problem with Rechargeable and Reusable Vehicles (MD-RPP-RRV), focusing
on rescheduling arc routing after vehicle failures. The problem involves
finding heuristically obtained best feasible routes for multiple rechargeable
and reusable vehicles with capacity constraints capable of performing multiple
trips from multiple depots, with the possibility of vehicle failures. Our
algorithm auctions the failed trips to active (non-failed) vehicles through
local auctioning, modifying initial routes to handle dynamic vehicle failures
efficiently. When a failure occurs, the algorithm searches for the best active
vehicle to perform the failed trip and inserts the trip into that vehicle's
route, which avoids a complete rescheduling and reduces the computational
effort. We compare the algorithm's solutions against offline optimal solutions
obtained from solving a Mixed Integer Linear Programming (MILP) formulation
using the Gurobi solver; this formulation assumes that perfect information
about the vehicle failures and failure times is given. The results demonstrate
that the centralized auction algorithm produces solutions that are, in some
cases, near optimal; moreover, the execution time for the proposed approach is
much more consistent and is, for some instances, orders of magnitude less than
the execution time of the Gurobi solver. The theoretical analysis provides an
upper bound for the competitive ratio and computational complexity of our
algorithm, offering a formal performance guarantee in dynamic failure
scenarios.
\\ ( https://arxiv.org/abs/2411.04073 ,  4188kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04112
Date: Wed, 6 Nov 2024 18:44:09 GMT   (6427kb,D)

Title: Fed-EC: Bandwidth-Efficient Clustering-Based Federated Learning For
  Autonomous Visual Robot Navigation
Authors: Shreya Gummadi, Mateus V. Gasparino, Deepak Vasisht, Girish Chowdhary
Categories: cs.RO cs.AI cs.CV cs.DC
\\
  Centralized learning requires data to be aggregated at a central server,
which poses significant challenges in terms of data privacy and bandwidth
consumption. Federated learning presents a compelling alternative, however,
vanilla federated learning methods deployed in robotics aim to learn a single
global model across robots that works ideally for all. But in practice one
model may not be well suited for robots deployed in various environments. This
paper proposes Federated-EmbedCluster (Fed-EC), a clustering-based federated
learning framework that is deployed with vision based autonomous robot
navigation in diverse outdoor environments. The framework addresses the key
federated learning challenge of deteriorating model performance of a single
global model due to the presence of non-IID data across real-world robots.
Extensive real-world experiments validate that Fed-EC reduces the communication
size by 23x for each robot while matching the performance of centralized
learning for goal-oriented navigation and outperforms local learning. Fed-EC
can transfer previously learnt models to new robots that join the cluster.
\\ ( https://arxiv.org/abs/2411.04112 ,  6427kb)
%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-%-
------------------------------------------------------------------------------
\\
arXiv:2411.03341 (*cross-listing*)
Date: Sat, 2 Nov 2024 11:21:33 GMT   (20176kb,D)

Title: Interpretable Embeddings for Segmentation-Free Single-Cell Analysis in
  Multiplex Imaging
Authors: Simon Gutwein, Daria Lazic, Thomas Walter, Sabine Taschner-Mandl,
  Roxane Licandro
Categories: eess.IV cs.CV
Comments: 5 Pages, 5 Figures, Submitted to ISBI 2025
\\
  Multiplex Imaging (MI) enables the simultaneous visualization of multiple
biological markers in separate imaging channels at subcellular resolution,
providing valuable insights into cell-type heterogeneity and spatial
organization. However, current computational pipelines rely on cell
segmentation algorithms, which require laborious fine-tuning and can introduce
downstream errors due to inaccurate single-cell representations. We propose a
segmentation-free deep learning approach that leverages grouped convolutions to
learn interpretable embedded features from each imaging channel, enabling
robust cell-type identification without manual feature selection. Validated on
an Imaging Mass Cytometry dataset of 1.8 million cells from neuroblastoma
patients, our method enables the accurate identification of known cell types,
showcasing its scalability and suitability for high-dimensional MI data.
\\ ( https://arxiv.org/abs/2411.03341 ,  20176kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03348 (*cross-listing*)
Date: Sun, 3 Nov 2024 18:44:28 GMT   (4814kb,D)

Title: Undermining Image and Text Classification Algorithms Using Adversarial
  Attacks
Authors: Langalibalele Lunga, Suhas Sreehari
Categories: cs.CR cs.AI cs.CV cs.LG
Comments: Accepted for presentation at Electronic Imaging Conference 2025
\\
  Machine learning models are prone to adversarial attacks, where inputs can be
manipulated in order to cause misclassifications. While previous research has
focused on techniques like Generative Adversarial Networks (GANs), there's
limited exploration of GANs and Synthetic Minority Oversampling Technique
(SMOTE) in text and image classification models to perform adversarial attacks.
Our study addresses this gap by training various machine learning models and
using GANs and SMOTE to generate additional data points aimed at attacking text
classification models. Furthermore, we extend our investigation to face
recognition models, training a Convolutional Neural Network(CNN) and subjecting
it to adversarial attacks with fast gradient sign perturbations on key features
identified by GradCAM, a technique used to highlight key image characteristics
CNNs use in classification. Our experiments reveal a significant vulnerability
in classification models. Specifically, we observe a 20 % decrease in accuracy
for the top-performing text classification models post-attack, along with a 30
% decrease in facial recognition accuracy. This highlights the susceptibility
of these models to manipulation of input data. Adversarial attacks not only
compromise the security but also undermine the reliability of machine learning
systems. By showcasing the impact of adversarial attacks on both text
classification and face recognition models, our study underscores the urgent
need for develop robust defenses against such vulnerabilities.
\\ ( https://arxiv.org/abs/2411.03348 ,  4814kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03445 (*cross-listing*)
Date: Tue, 5 Nov 2024 19:00:34 GMT   (1289kb,D)

Title: Solving Trojan Detection Competitions with Linear Weight Classification
Authors: Todd Huster, Peter Lin, Razvan Stefanescu, Emmanuel Ekwedike, Ritu
  Chadha
Categories: cs.LG cs.AI cs.CL cs.CR cs.CV
Comments: 9 pages, 4 Figures
\\
  Neural networks can conceal malicious Trojan backdoors that allow a trigger
to covertly change the model behavior. Detecting signs of these backdoors,
particularly without access to any triggered data, is the subject of ongoing
research and open challenges. In one common formulation of the problem, we are
given a set of clean and poisoned models and need to predict whether a given
test model is clean or poisoned. In this paper, we introduce a detector that
works remarkably well across many of the existing datasets and domains. It is
obtained by training a binary classifier on a large number of models' weights
after performing a few different pre-processing steps including feature
selection and standardization, reference model weights subtraction, and model
alignment prior to detection. We evaluate this algorithm on a diverse set of
Trojan detection benchmarks and domains and examine the cases where the
approach is most and least effective.
\\ ( https://arxiv.org/abs/2411.03445 ,  1289kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03456 (*cross-listing*)
Date: Tue, 5 Nov 2024 19:17:38 GMT   (1494kb,D)

Title: BOston Neonatal Brain Injury Data for Hypoxic Ischemic Encephalopathy
  (BONBID-HIE): II. 2-year Neurocognitive Outcome and NICU Outcome
Authors: Rina Bao, Yangming Ou
Categories: eess.IV cs.CV
Comments: Data description for BONBID-HIE 2024 Challenge on MICCAI 2024
\\
  Hypoxic Ischemic Encephalopathy (HIE) affects approximately 1-5/1000 newborns
globally and leads to adverse neurocognitive outcomes in 30% to 50% of cases by
two years of age. Despite therapeutic advances with Therapeutic Hypothermia
(TH), prognosis remains challenging, highlighting the need for improved
biomarkers. This paper introduces the second release of the Boston Neonatal
Brain Injury Dataset for Hypoxic-Ischemic Encephalopathy (BONBID-HIE), an
open-source, comprehensive MRI and clinical dataset featuring 237 patients,
including NICU outcomes and 2-year neurocognitive outcomes from Massachusetts
General Hospital and Boston Children's Hospital.
\\ ( https://arxiv.org/abs/2411.03456 ,  1494kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03464 (*cross-listing*)
Date: Tue, 5 Nov 2024 19:35:10 GMT   (6069kb,D)

Title: TopoTxR: A topology-guided deep convolutional network for breast
  parenchyma learning on DCE-MRIs
Authors: Fan Wang, Zhilin Zou, Nicole Sakla, Luke Partyka, Nil Rawal, Gagandeep
  Singh, Wei Zhao, Haibin Ling, Chuan Huang, Prateek Prasanna, Chao Chen
Categories: eess.IV cs.CV
Comments: 22 pages, 8 figures, 8 tables, accepted by Medical Image Analysis (
  https://www.sciencedirect.com/science/article/abs/pii/S1361841524002986 )
Journal-ref: Volume 99, 2025, 103373
DOI: 10.1016/j.media.2024.103373
\\
  Characterization of breast parenchyma in dynamic contrast-enhanced magnetic
resonance imaging (DCE-MRI) is a challenging task owing to the complexity of
underlying tissue structures. Existing quantitative approaches, like radiomics
and deep learning models, lack explicit quantification of intricate and subtle
parenchymal structures, including fibroglandular tissue. To address this, we
propose a novel topological approach that explicitly extracts multi-scale
topological structures to better approximate breast parenchymal structures, and
then incorporates these structures into a deep-learning-based prediction model
via an attention mechanism. Our topology-informed deep learning model,
\emph{TopoTxR}, leverages topology to provide enhanced insights into tissues
critical for disease pathophysiology and treatment response. We empirically
validate \emph{TopoTxR} using the VICTRE phantom breast dataset, showing that
the topological structures extracted by our model effectively approximate the
breast parenchymal structures. We further demonstrate \emph{TopoTxR}'s efficacy
in predicting response to neoadjuvant chemotherapy. Our qualitative and
quantitative analyses suggest differential topological behavior of breast
tissue in treatment-na\"ive imaging, in patients who respond favorably to
therapy as achieving pathological complete response (pCR) versus those who do
not. In a comparative analysis with several baselines on the publicly available
I-SPY 1 dataset (N=161, including 47 patients with pCR and 114 without) and the
Rutgers proprietary dataset (N=120, with 69 patients achieving pCR and 51 not),
\emph{TopoTxR} demonstrates a notable improvement, achieving a 2.6\% increase
in accuracy and a 4.6\% enhancement in AUC compared to the state-of-the-art
method.
\\ ( https://arxiv.org/abs/2411.03464 ,  6069kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03551 (*cross-listing*)
Date: Tue, 5 Nov 2024 23:11:26 GMT   (4979kb,D)

Title: Enhancing Weakly Supervised Semantic Segmentation for Fibrosis via
  Controllable Image Generation
Authors: Zhiling Yue, Yingying Fang, Liutao Yang, Nikhil Baid, Simon Walsh,
  Guang Yang
Categories: eess.IV cs.CV
\\
  Fibrotic Lung Disease (FLD) is a severe condition marked by lung stiffening
and scarring, leading to respiratory decline. High-resolution computed
tomography (HRCT) is critical for diagnosing and monitoring FLD; however,
fibrosis appears as irregular, diffuse patterns with unclear boundaries,
leading to high inter-observer variability and time-intensive manual
annotation. To tackle this challenge, we propose DiffSeg, a novel weakly
supervised semantic segmentation (WSSS) method that uses image-level
annotations to generate pixel-level fibrosis segmentation, reducing the need
for fine-grained manual labeling. Additionally, our DiffSeg incorporates a
diffusion-based generative model to synthesize HRCT images with different
levels of fibrosis from healthy slices, enabling the generation of the
fibrosis-injected slices and their paired fibrosis location. Experiments
indicate that our method significantly improves the accuracy of pseudo masks
generated by existing WSSS methods, greatly reducing the complexity of manual
labeling and enhancing the consistency of the generated masks.
\\ ( https://arxiv.org/abs/2411.03551 ,  4979kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03568 (*cross-listing*)
Date: Wed, 6 Nov 2024 00:16:16 GMT   (7133kb,D)

Title: The American Sign Language Knowledge Graph: Infusing ASL Models with
  Linguistic Knowledge
Authors: Lee Kezar, Nidhi Munikote, Zian Zeng, Zed Sehyr, Naomi Caselli, Jesse
  Thomason
Categories: cs.CL cs.CV
ACM-class: I.2.7
\\
  Language models for American Sign Language (ASL) could make language
technologies substantially more accessible to those who sign. To train models
on tasks such as isolated sign recognition (ISR) and ASL-to-English
translation, datasets provide annotated video examples of ASL signs. To
facilitate the generalizability and explainability of these models, we
introduce the American Sign Language Knowledge Graph (ASLKG), compiled from
twelve sources of expert linguistic knowledge. We use the ASLKG to train
neuro-symbolic models for 3 ASL understanding tasks, achieving accuracies of
91% on ISR, 14% for predicting the semantic features of unseen signs, and 36%
for classifying the topic of Youtube-ASL videos.
\\ ( https://arxiv.org/abs/2411.03568 ,  7133kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03569 (*cross-listing*)
Date: Wed, 6 Nov 2024 00:17:36 GMT   (1475kb,D)

Title: Towards Personalized Federated Learning via Comprehensive Knowledge
  Distillation
Authors: Pengju Wang and Bochao Liu and Weijia Guo and Yong Li and Shiming Ge
Categories: cs.LG cs.AI cs.CR cs.CV
Comments: Accepted by IEEE SMC 2024
\\
  Federated learning is a distributed machine learning paradigm designed to
protect data privacy. However, data heterogeneity across various clients
results in catastrophic forgetting, where the model rapidly forgets previous
knowledge while acquiring new knowledge. To address this challenge,
personalized federated learning has emerged to customize a personalized model
for each client. However, the inherent limitation of this mechanism is its
excessive focus on personalization, potentially hindering the generalization of
those models. In this paper, we present a novel personalized federated learning
method that uses global and historical models as teachers and the local model
as the student to facilitate comprehensive knowledge distillation. The
historical model represents the local model from the last round of client
training, containing historical personalized knowledge, while the global model
represents the aggregated model from the last round of server aggregation,
containing global generalized knowledge. By applying knowledge distillation, we
effectively transfer global generalized knowledge and historical personalized
knowledge to the local model, thus mitigating catastrophic forgetting and
enhancing the general performance of personalized models. Extensive
experimental results demonstrate the significant advantages of our method.
\\ ( https://arxiv.org/abs/2411.03569 ,  1475kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03615 (*cross-listing*)
Date: Wed, 6 Nov 2024 02:16:34 GMT   (1542kb,D)

Title: ADMIRE: a locally adaptive single-image, non-uniformity correction and
  denoising algorithm: application to uncooled IR camera
Authors: Yohann Tendero and Jerome Gilles
Categories: eess.IV cs.CV
Journal-ref: SPIE Defense, Security and Sensing conference,Proceedings Volume
  8353, Infrared Technology and Applications XXXVIII; 83531O, April 2012
DOI: 10.1117/12.912966
\\
  We propose a new way to correct for the non-uniformity (NU) and the noise in
uncooled infrared-type images. This method works on static images, needs no
registration, no camera motion and no model for the non uniformity. The
proposed method uses an hybrid scheme including an automatic locally-adaptive
contrast adjustment and a state-of-the-art image denoising method. It permits
to correct for a fully non-linear NU and the noise efficiently using only one
image. We compared it with total variation on real raw and simulated NU
infrared images. The strength of this approach lies in its simplicity, low
computational cost. It needs no test-pattern or calibration and produces no
"ghost-artefact".
\\ ( https://arxiv.org/abs/2411.03615 ,  1542kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03618 (*cross-listing*)
Date: Wed, 6 Nov 2024 02:23:38 GMT   (861kb,D)

Title: Cross Feature Fusion of Fundus Image and Generated Lesion Map for
  Referable Diabetic Retinopathy Classification
Authors: Dahyun Mok and Junghyun Bum and Le Duc Tai and Hyunseung Choo
Categories: eess.IV cs.AI cs.CV
Comments: ACCV 2024 accepted
\\
  Diabetic Retinopathy (DR) is a primary cause of blindness, necessitating
early detection and diagnosis. This paper focuses on referable DR
classification to enhance the applicability of the proposed method in clinical
practice. We develop an advanced cross-learning DR classification method
leveraging transfer learning and cross-attention mechanisms. The proposed
method employs the Swin U-Net architecture to segment lesion maps from DR
fundus images. The Swin U-Net segmentation model, enriched with DR lesion
insights, is transferred to generate a lesion map. Both the fundus image and
its segmented lesion map are used as complementary inputs for the
classification model. A cross-attention mechanism is deployed to improve the
model's ability to capture fine-grained details from the input pairs. Our
experiments, utilizing two public datasets, FGADR and EyePACS, demonstrate a
superior accuracy of 94.6%, surpassing current state-of-the-art methods by
4.4%. To this end, we aim for the proposed method to be seamlessly integrated
into clinical workflows, enhancing accuracy and efficiency in identifying
referable DR.
\\ ( https://arxiv.org/abs/2411.03618 ,  861kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03723 (*cross-listing*)
Date: Wed, 6 Nov 2024 07:40:27 GMT   (2183kb)

Title: Zero-shot Dynamic MRI Reconstruction with Global-to-local Diffusion
  Model
Authors: Yu Guan, Kunlong Zhang, Qi Qi, Dong Wang, Ziwen Ke, Shaoyu Wang, Dong
  Liang, Qiegen Liu
Categories: eess.IV cs.CV
Comments: 11 pages, 9 figures
\\
  Diffusion models have recently demonstrated considerable advancement in the
generation and reconstruction of magnetic resonance imaging (MRI) data. These
models exhibit great potential in handling unsampled data and reducing noise,
highlighting their promise as generative models. However, their application in
dynamic MRI remains relatively underexplored. This is primarily due to the
substantial amount of fully-sampled data typically required for training, which
is difficult to obtain in dynamic MRI due to its spatio-temporal complexity and
high acquisition costs. To address this challenge, we propose a dynamic MRI
reconstruction method based on a time-interleaved acquisition scheme, termed
the Glob-al-to-local Diffusion Model. Specifically, fully encoded
full-resolution reference data are constructed by merging under-sampled k-space
data from adjacent time frames, generating two distinct bulk training datasets
for global and local models. The global-to-local diffusion framework
alternately optimizes global information and local image details, enabling
zero-shot reconstruction. Extensive experiments demonstrate that the proposed
method performs well in terms of noise reduction and detail preservation,
achieving reconstruction quality comparable to that of supervised approaches.
\\ ( https://arxiv.org/abs/2411.03723 ,  2183kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03730 (*cross-listing*)
Date: Wed, 6 Nov 2024 07:51:19 GMT   (657kb,D)

Title: NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document
  VQA
Authors: Marlon Tobaben, Mohamed Ali Souibgui, Rub\`en Tito, Khanh Nguyen,
  Raouf Kerkouche, Kangsoo Jung, Joonas J\"alk\"o, Lei Kang, Andrey Barsky,
  Vincent Poulain d'Andecy, Aur\'elie Joseph, Aashiq Muhamed, Kevin Kuo,
  Virginia Smith, Yusuke Yamasaki, Takumi Fukami, Kenta Niwa, Iifan Tyou, Hiro
  Ishii, Rio Yokota, Ragul N, Rintu Kutum, Josep Llados, Ernest Valveny, Antti
  Honkela, Mario Fritz, Dimosthenis Karatzas
Categories: cs.LG cs.CR cs.CV
Comments: 27 pages, 6 figures
\\
  The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA)
competition challenged the community to develop provably private and
communication-efficient solutions in a federated setting for a real-life use
case: invoice processing. The competition introduced a dataset of real invoice
documents, along with associated questions and answers requiring information
extraction and reasoning over the document images. Thereby, it brings together
researchers and expertise from the document analysis, privacy, and federated
learning communities. Participants fine-tuned a pre-trained, state-of-the-art
Document Visual Question Answering model provided by the organizers for this
new domain, mimicking a typical federated invoice processing setup. The base
model is a multi-modal generative language model, and sensitive information
could be exposed through either the visual or textual input modality.
Participants proposed elegant solutions to reduce communication costs while
maintaining a minimum utility threshold in track 1 and to protect all
information from each document provider using differential privacy in track 2.
The competition served as a new testbed for developing and testing private
federated learning methods, simultaneously raising awareness about privacy
within the document image analysis and recognition community. Ultimately, the
competition analysis provides best practices and recommendations for
successfully running privacy-focused federated learning challenges in the
future.
\\ ( https://arxiv.org/abs/2411.03730 ,  657kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03752 (*cross-listing*)
Date: Wed, 6 Nov 2024 08:27:49 GMT   (2065kb,D)

Title: Deferred Poisoning: Making the Model More Vulnerable via Hessian
  Singularization
Authors: Yuhao He, Jinyu Tian, Xianwei Zheng, Li Dong, Yuanman Li, Leo Yu
  Zhang, Jiantao Zhou
Categories: cs.LG cs.CR cs.CV
\\
  Recent studies have shown that deep learning models are very vulnerable to
poisoning attacks. Many defense methods have been proposed to address this
issue. However, traditional poisoning attacks are not as threatening as
commonly believed. This is because they often cause differences in how the
model performs on the training set compared to the validation set. Such
inconsistency can alert defenders that their data has been poisoned, allowing
them to take the necessary defensive actions. In this paper, we introduce a
more threatening type of poisoning attack called the Deferred Poisoning Attack.
This new attack allows the model to function normally during the training and
validation phases but makes it very sensitive to evasion attacks or even
natural noise. We achieve this by ensuring the poisoned model's loss function
has a similar value as a normally trained model at each input sample but with a
large local curvature. A similar model loss ensures that there is no obvious
inconsistency between the training and validation accuracy, demonstrating high
stealthiness. On the other hand, the large curvature implies that a small
perturbation may cause a significant increase in model loss, leading to
substantial performance degradation, which reflects a worse robustness. We
fulfill this purpose by making the model have singular Hessian information at
the optimal point via our proposed Singularization Regularization term. We have
conducted both theoretical and empirical analyses of the proposed method and
validated its effectiveness through experiments on image classification tasks.
Furthermore, we have confirmed the hazards of this form of poisoning attack
under more general scenarios using natural noise, offering a new perspective
for research in the field of security.
\\ ( https://arxiv.org/abs/2411.03752 ,  2065kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03758 (*cross-listing*)
Date: Wed, 6 Nov 2024 08:33:07 GMT   (1417kb)

Title: Sub-DM:Subspace Diffusion Model with Orthogonal Decomposition for MRI
  Reconstruction
Authors: Yu Guan, Qinrong Cai, Wei Li, Qiuyun Fan, Dong Liang, Qiegen Liu
Categories: eess.IV cs.AI cs.CV
Comments: 10 pages, 11 figures
\\
  Diffusion model-based approaches recently achieved re-markable success in MRI
reconstruction, but integration into clinical routine remains challenging due
to its time-consuming convergence. This phenomenon is partic-ularly notable
when directly apply conventional diffusion process to k-space data without
considering the inherent properties of k-space sampling, limiting k-space
learning efficiency and image reconstruction quality. To tackle these
challenges, we introduce subspace diffusion model with orthogonal
decomposition, a method (referred to as Sub-DM) that restrict the diffusion
process via projections onto subspace as the k-space data distribution evolves
toward noise. Particularly, the subspace diffusion model circumvents the
inference challenges posed by the com-plex and high-dimensional characteristics
of k-space data, so the highly compact subspace ensures that diffusion process
requires only a few simple iterations to produce accurate prior information.
Furthermore, the orthogonal decomposition strategy based on wavelet transform
hin-ders the information loss during the migration of the vanilla diffusion
process to the subspace. Considering the strate-gy is approximately reversible,
such that the entire pro-cess can be reversed. As a result, it allows the
diffusion processes in different spaces to refine models through a mutual
feedback mechanism, enabling the learning of ac-curate prior even when dealing
with complex k-space data. Comprehensive experiments on different datasets
clearly demonstrate that the superiority of Sub-DM against state of-the-art
methods in terms of reconstruction speed and quality.
\\ ( https://arxiv.org/abs/2411.03758 ,  1417kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03855 (*cross-listing*)
Date: Wed, 6 Nov 2024 11:57:55 GMT   (454kb,D)

Title: MambaPEFT: Exploring Parameter-Efficient Fine-Tuning for Mamba
Authors: Masakazu Yoshimura, Teruaki Hayashi and Yota Maeda
Categories: cs.CL cs.AI cs.CV cs.LG
\\
  An ecosystem of Transformer-based models has been established by building
large models with extensive data. Parameter-efficient fine-tuning (PEFT) is a
crucial technology for deploying these models to downstream tasks with minimal
cost while achieving effective performance. Recently, Mamba, a State Space
Model (SSM)-based model, has attracted attention as a potential alternative to
Transformers. While many large-scale Mamba-based models have been proposed,
efficiently adapting pre-trained Mamba-based models to downstream tasks remains
unexplored. In this paper, we conduct an exploratory analysis of PEFT methods
for Mamba. We investigate the effectiveness of existing PEFT methods for
Transformers when applied to Mamba. We also modify these methods to better
align with the Mamba architecture. Additionally, we propose new Mamba-specific
PEFT methods that leverage the distinctive structure of Mamba. Our experiments
indicate that PEFT performs more effectively for Mamba than Transformers.
Lastly, we demonstrate how to effectively combine multiple PEFT methods and
provide a framework that outperforms previous works. To ensure reproducibility,
we will release the code after publication.
\\ ( https://arxiv.org/abs/2411.03855 ,  454kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04004 (*cross-listing*)
Date: Wed, 6 Nov 2024 15:43:51 GMT   (5632kb,D)

Title: Synomaly Noise and Multi-Stage Diffusion: A Novel Approach for
  Unsupervised Anomaly Detection in Ultrasound Imaging
Authors: Yuan Bi, Lucie Huang, Ricarda Clarenbach, Reza Ghotbi, Angelos Karlas,
  Nassir Navab, Zhongliang Jiang
Categories: eess.IV cs.CV
\\
  Ultrasound (US) imaging is widely used in routine clinical practice due to
its advantages of being radiation-free, cost-effective, and portable. However,
the low reproducibility and quality of US images, combined with the scarcity of
expert-level annotation, make the training of fully supervised segmentation
models challenging. To address these issues, we propose a novel unsupervised
anomaly detection framework based on a diffusion model that incorporates a
synthetic anomaly (Synomaly) noise function and a multi-stage diffusion
process. Synomaly noise introduces synthetic anomalies into healthy images
during training, allowing the model to effectively learn anomaly removal. The
multi-stage diffusion process is introduced to progressively denoise images,
preserving fine details while improving the quality of anomaly-free
reconstructions. The generated high-fidelity counterfactual healthy images can
further enhance the interpretability of the segmentation models, as well as
provide a reliable baseline for evaluating the extent of anomalies and
supporting clinical decision-making. Notably, the unsupervised anomaly
detection model is trained purely on healthy images, eliminating the need for
anomalous training samples and pixel-level annotations. We validate the
proposed approach on carotid US, brain MRI, and liver CT datasets. The
experimental results demonstrate that the proposed framework outperforms
existing state-of-the-art unsupervised anomaly detection methods, achieving
performance comparable to fully supervised segmentation models in the US
dataset. Additionally, ablation studies underline the importance of
hyperparameter selection for Synomaly noise and the effectiveness of the
multi-stage diffusion process in enhancing model performance.
\\ ( https://arxiv.org/abs/2411.04004 ,  5632kb)
------------------------------------------------------------------------------
\\
arXiv:2411.04055 (*cross-listing*)
Date: Wed, 6 Nov 2024 16:59:51 GMT   (469kb,D)

Title: Multi-branch Spatio-Temporal Graph Neural Network For Efficient Ice
  Layer Thickness Prediction
Authors: Zesheng Liu, Maryam Rahnemoonfar
Categories: cs.LG cs.CV
\\
  Understanding spatio-temporal patterns in polar ice layers is essential for
tracking changes in ice sheet balance and assessing ice dynamics. While
convolutional neural networks are widely used in learning ice layer patterns
from raw echogram images captured by airborne snow radar sensors, noise in the
echogram images prevents researchers from getting high-quality results.
Instead, we focus on geometric deep learning using graph neural networks,
aiming to build a spatio-temporal graph neural network that learns from
thickness information of the top ice layers and predicts for deeper layers. In
this paper, we developed a novel multi-branch spatio-temporal graph neural
network that used the GraphSAGE framework for spatio features learning and a
temporal convolution operation to capture temporal changes, enabling different
branches of the network to be more specialized and focusing on a single
learning task. We found that our proposed multi-branch network can consistently
outperform the current fused spatio-temporal graph neural network in both
accuracy and efficiency.
\\ ( https://arxiv.org/abs/2411.04055 ,  469kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03465 (*cross-listing*)
Date: Tue, 5 Nov 2024 19:35:23 GMT   (13243kb,D)

Title: Digital Twin for Autonomous Surface Vessels: Enabler for Safe Maritime
  Navigation
Authors: Daniel Menges, Adil Rasheed
Categories: eess.SY cs.RO cs.SY
\\
  Autonomous surface vessels (ASVs) are becoming increasingly significant in
enhancing the safety and sustainability of maritime operations. To ensure the
reliability of modern control algorithms utilized in these vessels, digital
twins (DTs) provide a robust framework for conducting safe and effective
simulations within a virtual environment. Digital twins are generally
classified on a scale from 0 to 5, with each level representing a progression
in complexity and functionality: Level 0 (Standalone) employs offline modeling
techniques; Level 1 (Descriptive) integrates sensors and online modeling to
enhance situational awareness; Level 2 (Diagnostic) focuses on condition
monitoring and cybersecurity; Level 3 (Predictive) incorporates predictive
analytics; Level 4 (Prescriptive) embeds decision-support systems; and Level 5
(Autonomous) enables advanced functionalities such as collision avoidance and
path following. These digital representations not only provide insights into
the vessel's current state and operational efficiency but also predict future
scenarios and assess life endurance. By continuously updating with real-time
sensor data, the digital twin effectively corrects modeling errors and enhances
decision-making processes. Since DTs are key enablers for complex autonomous
systems, this paper introduces a comprehensive methodology for establishing a
digital twin framework specifically tailored for ASVs. Through a detailed
literature survey, we explore existing state-of-the-art enablers across the
defined levels, offering valuable recommendations for future research and
development in this rapidly evolving field.
\\ ( https://arxiv.org/abs/2411.03465 ,  13243kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03815 (*cross-listing*)
Date: Wed, 6 Nov 2024 10:33:01 GMT   (5055kb,D)

Title: How to Drawjectory? -- Trajectory Planning using Programming by
  Demonstration
Authors: Leonhard Alkewitz and Timo Zuccarello and Alexander Raschke and
  Matthias Tichy
Categories: cs.SE cs.RO
ACM-class: D.1.7; D.2.3
\\
  A flight trajectory defines how exactly a quadrocopter moves in the
three-dimensional space from one position to another. Automatic flight
trajectory planning faces challenges such as high computational effort and a
lack of precision. Hence, when low computational effort or precise control is
required, programming the flight route trajectory manually might be preferable.
However, this requires in-depth knowledge of how to accurately plan flight
trajectories in three-dimensional space. We propose planning quadrocopter
flight trajectories manually using the Programming by Demonstration (PbD)
approach -- simply drawing the trajectory in the three-dimensional space by
hand. This simplifies the planning process and reduces the level of in-depth
knowledge required.
  We implemented the approach in the context of the Quadcopter Lab at Ulm
University. In order to evaluate our approach, we compare the precision and
accuracy of the trajectories drawn by a user using our approach as well as the
required time with those manually programmed using a domain specific language.
The evaluation shows that the Drawjectory workflow is, on average, 78.7 seconds
faster without a significant loss of precision, shown by an average deviation
6.67 cm.
\\ ( https://arxiv.org/abs/2411.03815 ,  5055kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03817 (*cross-listing*)
Date: Wed, 6 Nov 2024 10:35:11 GMT   (1222kb,D)

Title: From Novice to Expert: LLM Agent Policy Optimization via Step-wise
  Reinforcement Learning
Authors: Zhirui Deng, Zhicheng Dou, Yutao Zhu, Ji-Rong Wen, Ruibin Xiong, Mang
  Wang, Weipeng Chen
Categories: cs.AI cs.CL cs.HC cs.RO
\\
  The outstanding capabilities of large language models (LLMs) render them a
crucial component in various autonomous agent systems. While traditional
methods depend on the inherent knowledge of LLMs without fine-tuning, more
recent approaches have shifted toward the reinforcement learning strategy to
further enhance agents' ability to solve complex interactive tasks with
environments and tools. However, previous approaches are constrained by the
sparse reward issue, where existing datasets solely provide a final scalar
reward for each multi-step reasoning chain, potentially leading to
ineffectiveness and inefficiency in policy learning. In this paper, we
introduce StepAgent, which utilizes step-wise reward to optimize the agent's
reinforcement learning process. Inheriting the spirit of novice-to-expert
theory, we first compare the actions of the expert and the agent to
automatically generate intermediate rewards for fine-grained optimization.
Additionally, we propose implicit-reward and inverse reinforcement learning
techniques to facilitate agent reflection and policy adjustment. Further
theoretical analysis demonstrates that the action distribution of the agent can
converge toward the expert action distribution over multiple training cycles.
Experimental results across various datasets indicate that StepAgent
outperforms existing baseline methods.
\\ ( https://arxiv.org/abs/2411.03817 ,  1222kb)
%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%--%%
------------------------------------------------------------------------------
\\
arXiv:2209.05824
replaced with revised version Wed, 6 Nov 2024 07:23:52 GMT   (3381kb,D)

Title: CPnP: Consistent Pose Estimator for Perspective-n-Point Problem with
  Bias Elimination
Authors: Guangyang Zeng, Shiyu Chen, Biqiang Mu, Guodong Shi, and Junfeng Wu
Categories: cs.CV cs.RO
\\ ( https://arxiv.org/abs/2209.05824 ,  3381kb)
------------------------------------------------------------------------------
\\
arXiv:2310.06629
replaced with revised version Wed, 6 Nov 2024 13:29:57 GMT   (967kb,D)

Title: EViT: An Eagle Vision Transformer with Bi-Fovea Self-Attention
Authors: Yulong Shi, Mingwei Sun, Yongshuai Wang, Jiahao Ma, Zengqiang Chen
Categories: cs.CV
Comments: This work has been submitted to the IEEE for possible publication
\\ ( https://arxiv.org/abs/2310.06629 ,  967kb)
------------------------------------------------------------------------------
\\
arXiv:2311.04315
replaced with revised version Wed, 6 Nov 2024 05:35:40 GMT   (40920kb,D)

Title: A Data Perspective on Enhanced Identity Preservation for Diffusion
  Personalization
Authors: Xingzhe He, Zhiwen Cao, Nicholas Kolkin, Lantao Yu, Kun Wan, Helge
  Rhodin, Ratheesh Kalarot
Categories: cs.CV
Comments: WACV 2025
\\ ( https://arxiv.org/abs/2311.04315 ,  40920kb)
------------------------------------------------------------------------------
\\
arXiv:2312.05269
replaced with revised version Tue, 5 Nov 2024 22:08:14 GMT   (3667kb,D)

Title: LifelongMemory: Leveraging LLMs for Answering Queries in Long-form
  Egocentric Videos
Authors: Ying Wang, Yanlai Yang, Mengye Ren
Categories: cs.CV cs.LG
\\ ( https://arxiv.org/abs/2312.05269 ,  3667kb)
------------------------------------------------------------------------------
\\
arXiv:2401.11124
replaced with revised version Wed, 6 Nov 2024 11:40:50 GMT   (5661kb,D)

Title: Cross-Task Affinity Learning for Multitask Dense Scene Predictions
Authors: Dimitrios Sinodinos, Narges Armanfard
Categories: cs.CV cs.LG
Comments: Accepted for publication at the IEEE Winter Conference on
  Applications of Computer Vision (WACV) 2025
\\ ( https://arxiv.org/abs/2401.11124 ,  5661kb)
------------------------------------------------------------------------------
\\
arXiv:2402.08774
replaced with revised version Wed, 6 Nov 2024 16:57:36 GMT   (8827kb)

Title: LDTrack: Dynamic People Tracking by Service Robots using Diffusion
  Models
Authors: Angus Fung, Beno Benhabib, Goldie Nejat
Categories: cs.CV cs.RO
\\ ( https://arxiv.org/abs/2402.08774 ,  8827kb)
------------------------------------------------------------------------------
\\
arXiv:2403.20213
replaced with revised version Wed, 6 Nov 2024 07:09:03 GMT   (6749kb,D)

Title: VHM: Versatile and Honest Vision Language Model for Remote Sensing Image
  Analysis
Authors: Chao Pang, Xingxing Weng, Jiang Wu, Jiayu Li, Yi Liu, Jiaxing Sun,
  Weijia Li, Shuai Wang, Litong Feng, Gui-Song Xia, Conghui He
Categories: cs.CV
Comments: Equal contribution: Chao Pang, Xingxing Weng, Jiang Wu; Corresponding
  author: Gui-Song Xia, Conghui He
\\ ( https://arxiv.org/abs/2403.20213 ,  6749kb)
------------------------------------------------------------------------------
\\
arXiv:2404.03202
replaced with revised version Wed, 6 Nov 2024 09:26:23 GMT   (817kb,D)

Title: OmniGS: Fast Radiance Field Reconstruction using Omnidirectional
  Gaussian Splatting
Authors: Longwei Li, Huajian Huang, Sai-Kit Yeung, Hui Cheng
Categories: cs.CV
Comments: 8 pages, 6 figures, accepted by WACV 2025, project page:
  https://liquorleaf.github.io/research/OmniGS/
\\ ( https://arxiv.org/abs/2404.03202 ,  817kb)
------------------------------------------------------------------------------
\\
arXiv:2404.05997
replaced with revised version Wed, 6 Nov 2024 12:06:03 GMT   (1341kb,D)

Title: Concept-Attention Whitening for Interpretable Skin Lesion Diagnosis
Authors: Junlin Hou, Jilan Xu, Hao Chen
Categories: cs.CV
Comments: MICCAI 2024
\\ ( https://arxiv.org/abs/2404.05997 ,  1341kb)
------------------------------------------------------------------------------
\\
arXiv:2404.07724
replaced with revised version Wed, 6 Nov 2024 14:29:36 GMT   (21982kb,D)

Title: Applying Guidance in a Limited Interval Improves Sample and Distribution
  Quality in Diffusion Models
Authors: Tuomas Kynk\"a\"anniemi, Miika Aittala, Tero Karras, Samuli Laine,
  Timo Aila, Jaakko Lehtinen
Categories: cs.CV cs.AI cs.LG cs.NE stat.ML
Comments: NeurIPS 2024
\\ ( https://arxiv.org/abs/2404.07724 ,  21982kb)
------------------------------------------------------------------------------
\\
arXiv:2404.09633
replaced with revised version Wed, 6 Nov 2024 09:04:35 GMT   (18303kb,D)

Title: In-Context Translation: Towards Unifying Image Recognition, Processing,
  and Generation
Authors: Han Xue, Qianru Sun, Li Song, Wenjun Zhang, Zhiwu Huang
Categories: cs.CV
\\ ( https://arxiv.org/abs/2404.09633 ,  18303kb)
------------------------------------------------------------------------------
\\
arXiv:2405.15199
replaced with revised version Tue, 5 Nov 2024 16:40:01 GMT   (54658kb,D)

Title: ODGEN: Domain-specific Object Detection Data Generation with Diffusion
  Models
Authors: Jingyuan Zhu, Shiyu Li, Yuxuan Liu, Ping Huang, Jiulong Shan, Huimin
  Ma, Jian Yuan
Categories: cs.CV
Comments: Accepted by NeurIPS2024
\\ ( https://arxiv.org/abs/2405.15199 ,  54658kb)
------------------------------------------------------------------------------
\\
arXiv:2405.17705
replaced with revised version Tue, 5 Nov 2024 18:02:53 GMT   (34122kb,D)

Title: DC-Gaussian: Improving 3D Gaussian Splatting for Reflective Dash Cam
  Videos
Authors: Linhan Wang, Kai Cheng, Shuo Lei, Shengkun Wang, Wei Yin, Chenyang
  Lei, Xiaoxiao Long, Chang-Tien Lu
Categories: cs.CV
Comments: 10 pages,7 figures;project page:
  https://linhanwang.github.io/dcgaussian/; Accepted to NeurIPS 2024
\\ ( https://arxiv.org/abs/2405.17705 ,  34122kb)
------------------------------------------------------------------------------
\\
arXiv:2406.10057
replaced with revised version Tue, 5 Nov 2024 04:40:34 GMT   (2304kb,D)

Title: First Multi-Dimensional Evaluation of Flowchart Comprehension for
  Multimodal Large Language Models
Authors: Enming Zhang, Ruobing Yao, Huanyong Liu, Junhui Yu, Jiale Wang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2406.10057 ,  2304kb)
------------------------------------------------------------------------------
\\
arXiv:2406.15735
replaced with revised version Wed, 6 Nov 2024 03:53:13 GMT   (11303kb,D)

Title: Identifying and Solving Conditional Image Leakage in Image-to-Video
  Diffusion Model
Authors: Min Zhao, Hongzhou Zhu, Chendong Xiang, Kaiwen Zheng, Chongxuan Li,
  Jun Zhu
Categories: cs.CV cs.AI
Comments: NeurIPS 2024. Project page: https://cond-image-leak.github.io/
\\ ( https://arxiv.org/abs/2406.15735 ,  11303kb)
------------------------------------------------------------------------------
\\
arXiv:2406.16473
replaced with revised version Wed, 6 Nov 2024 02:17:05 GMT   (19501kb,D)

Title: D2SP: Dynamic Dual-Stage Purification Framework for Dual Noise
  Mitigation in Vision-based Affective Recognition
Authors: Haoran Wang, Xinji Mai, Zeng Tao, Xuan Tong, Junxiong Lin, Yan Wang,
  Jiawen Yu, Boyang Wang, Shaoqi Yan, Qing Zhao, Ziheng Zhou, Shuyong Gao,
  Wenqiang Zhang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2406.16473 ,  19501kb)
------------------------------------------------------------------------------
\\
arXiv:2407.02077
replaced with revised version Wed, 6 Nov 2024 05:11:24 GMT   (7662kb,D)

Title: Hierarchical Temporal Context Learning for Camera-based Semantic Scene
  Completion
Authors: Bohan Li, Jiajun Deng, Wenyao Zhang, Zhujin Liang, Dalong Du, Xin Jin,
  Wenjun Zeng
Categories: cs.CV
Comments: ECCV 2024
\\ ( https://arxiv.org/abs/2407.02077 ,  7662kb)
------------------------------------------------------------------------------
\\
arXiv:2407.09786
replaced with revised version Wed, 6 Nov 2024 14:22:28 GMT   (30851kb,D)

Title: Self-supervised 3D Point Cloud Completion via Multi-view Adversarial
  Learning
Authors: Lintai Wu, Xianjing Cheng, Yong Xu, and Huanqiang Zeng, Junhui Hou
Categories: cs.CV
Comments: 14 pages,10 figures
\\ ( https://arxiv.org/abs/2407.09786 ,  30851kb)
------------------------------------------------------------------------------
\\
arXiv:2407.10964
replaced with revised version Wed, 6 Nov 2024 18:58:03 GMT   (1010kb,D)

Title: No Train, all Gain: Self-Supervised Gradients Improve Deep Frozen
  Representations
Authors: Walter Simoncini, Spyros Gidaris, Andrei Bursuc, Yuki M. Asano
Categories: cs.CV cs.CL cs.LG
Comments: NeurIPS 2024. Code available at
  https://github.com/WalterSimoncini/fungivision
\\ ( https://arxiv.org/abs/2407.10964 ,  1010kb)
------------------------------------------------------------------------------
\\
arXiv:2407.15668
replaced with revised version Tue, 5 Nov 2024 18:38:08 GMT   (1566kb)

Title: SLVideo: A Sign Language Video Moment Retrieval Framework
Authors: Gon\c{c}alo Vinagre Martins, Jo\~ao Magalh\~aes, Afonso Quinaz, Carla
  Viegas, Sofia Cavaco
Categories: cs.CV cs.AI
Comments: 4 pages, 1 figure, 1 table
\\ ( https://arxiv.org/abs/2407.15668 ,  1566kb)
------------------------------------------------------------------------------
\\
arXiv:2407.17331
replaced with revised version Wed, 6 Nov 2024 08:22:33 GMT   (5963kb,D)

Title: Multi-label Cluster Discrimination for Visual Representation Learning
Authors: Xiang An and Kaicheng Yang and Xiangzi Dai and Ziyong Feng and
  Jiankang Deng
Categories: cs.CV
Comments: Accepted by ECCV2024
DOI: 10.1007/978-3-031-73383-3_25
\\ ( https://arxiv.org/abs/2407.17331 ,  5963kb)
------------------------------------------------------------------------------
\\
arXiv:2407.17952
replaced with revised version Wed, 6 Nov 2024 14:58:17 GMT   (47155kb,D)

Title: BetterDepth: Plug-and-Play Diffusion Refiner for Zero-Shot Monocular
  Depth Estimation
Authors: Xiang Zhang, Bingxin Ke, Hayko Riemenschneider, Nando Metzger, Anton
  Obukhov, Markus Gross, Konrad Schindler, Christopher Schroers
Categories: cs.CV
Comments: NeurIPS 2024
\\ ( https://arxiv.org/abs/2407.17952 ,  47155kb)
------------------------------------------------------------------------------
\\
arXiv:2408.00738
replaced with revised version Wed, 6 Nov 2024 14:45:58 GMT   (1272kb,D)

Title: Virchow2: Scaling Self-Supervised Mixed Magnification Models in
  Pathology
Authors: Eric Zimmermann, Eugene Vorontsov, Julian Viret, Adam Casson, Michal
  Zelechowski, George Shaikovski, Neil Tenenholtz, James Hall, David Klimstra,
  Razik Yousfi, Thomas Fuchs, Nicolo Fusi, Siqi Liu, Kristen Severson
Categories: cs.CV
\\ ( https://arxiv.org/abs/2408.00738 ,  1272kb)
------------------------------------------------------------------------------
\\
arXiv:2408.14789
replaced with revised version Wed, 6 Nov 2024 04:41:48 GMT   (5071kb,D)

Title: Revisiting Surgical Instrument Segmentation Without Human Intervention:
  A Graph Partitioning View
Authors: Mingyu Sheng, Jianan Fan, Dongnan Liu, Ron Kikinis, Weidong Cai
Categories: cs.CV
Comments: This paper is accepted by The 32nd ACM International Conference on
  Multimedia (ACM MM 2024) Workshop on Multimedia Computing for Health and
  Medicine (MCHM)
\\ ( https://arxiv.org/abs/2408.14789 ,  5071kb)
------------------------------------------------------------------------------
\\
arXiv:2409.08240
replaced with revised version Wed, 6 Nov 2024 13:03:20 GMT   (20872kb,D)

Title: IFAdapter: Instance Feature Control for Grounded Text-to-Image
  Generation
Authors: Yinwei Wu, Xianpan Zhou, Bing Ma, Xuefeng Su, Kai Ma, Xinchao Wang
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2409.08240 ,  20872kb)
------------------------------------------------------------------------------
\\
arXiv:2409.09774
replaced with revised version Wed, 6 Nov 2024 05:16:59 GMT   (19776kb,D)

Title: Generalizing Alignment Paradigm of Text-to-Image Generation with
  Preferences through $f$-divergence Minimization
Authors: Haoyuan Sun, Bo Xia, Yongzhe Chang, Xueqian Wang
Categories: cs.CV
Comments: 34 pages
\\ ( https://arxiv.org/abs/2409.09774 ,  19776kb)
------------------------------------------------------------------------------
\\
arXiv:2409.13941
replaced with revised version Wed, 6 Nov 2024 05:05:12 GMT   (601kb)

Title: TalkMosaic: Interactive PhotoMosaic with Multi-modal LLM Q&A
  Interactions
Authors: Kevin Li, Fulu Li
Categories: cs.CV cs.AI
Comments: 6 pages, 5 figures
\\ ( https://arxiv.org/abs/2409.13941 ,  601kb)
------------------------------------------------------------------------------
\\
arXiv:2409.16147
replaced with revised version Wed, 6 Nov 2024 18:08:23 GMT   (10303kb,D)

Title: Gaussian Deja-vu: Creating Controllable 3D Gaussian Head-Avatars with
  Enhanced Generalization and Personalization Abilities
Authors: Peizhi Yan, Rabab Ward, Qiang Tang, Shan Du
Categories: cs.CV
Comments: 11 pages, Accepted by WACV 2025 in Round 1
\\ ( https://arxiv.org/abs/2409.16147 ,  10303kb)
------------------------------------------------------------------------------
\\
arXiv:2409.17459
replaced with revised version Wed, 6 Nov 2024 09:50:06 GMT   (8563kb,D)

Title: TFS-NeRF: Template-Free NeRF for Semantic 3D Reconstruction of Dynamic
  Scene
Authors: Sandika Biswas, Qianyi Wu, Biplab Banerjee, Hamid Rezatofighi
Categories: cs.CV
Comments: Accepted in NeurIPS 2024
\\ ( https://arxiv.org/abs/2409.17459 ,  8563kb)
------------------------------------------------------------------------------
\\
arXiv:2410.05969
replaced with revised version Wed, 6 Nov 2024 16:28:58 GMT   (8606kb,D)

Title: Deep neural network-based detection of counterfeit products from
  smartphone images
Authors: Hugo Garcia-Cotte, Dorra Mellouli, Abdul Rehman, Li Wang, David G.
  Stork
Categories: cs.CV
\\ ( https://arxiv.org/abs/2410.05969 ,  8606kb)
------------------------------------------------------------------------------
\\
arXiv:2410.11666
replaced with revised version Wed, 6 Nov 2024 12:00:44 GMT   (2093kb,D)

Title: Degradation Oriented and Regularized Network for Blind Depth
  Super-Resolution
Authors: Zhengxue Wang and Zhiqiang Yan and Jinshan Pan and Guangwei Gao and
  Kai Zhang and Jian Yang
Categories: cs.CV
Comments: 10 pages
\\ ( https://arxiv.org/abs/2410.11666 ,  2093kb)
------------------------------------------------------------------------------
\\
arXiv:2410.13824
replaced with revised version Wed, 6 Nov 2024 08:29:22 GMT   (6937kb,D)

Title: Harnessing Webpage UIs for Text-Rich Visual Understanding
Authors: Junpeng Liu, Tianyue Ou, Yifan Song, Yuxiao Qu, Wai Lam, Chenyan
  Xiong, Wenhu Chen, Graham Neubig, Xiang Yue
Categories: cs.CV cs.CL
\\ ( https://arxiv.org/abs/2410.13824 ,  6937kb)
------------------------------------------------------------------------------
\\
arXiv:2410.21872
replaced with revised version Wed, 6 Nov 2024 02:52:47 GMT   (899kb)

Title: Advancing Efficient Brain Tumor Multi-Class Classification -- New
  Insights from the Vision Mamba Model in Transfer Learning
Authors: Yinyi Lai, Anbo Cao, Yuan Gao, Jiaqi Shang, Zongyu Li, Jia Guo
Categories: cs.CV cs.AI
\\ ( https://arxiv.org/abs/2410.21872 ,  899kb)
------------------------------------------------------------------------------
\\
arXiv:2411.01797
replaced with revised version Wed, 6 Nov 2024 03:45:13 GMT   (824kb)

Title: AIWR: Aerial Image Water Resource Dataset for Segmentation Analysis
Authors: Sangdaow Noppitak, Emmanuel Okafor, Olarik Surinta
Categories: cs.CV
Comments: 12 pages, 8 figures
\\ ( https://arxiv.org/abs/2411.01797 ,  824kb)
------------------------------------------------------------------------------
\\
arXiv:2411.01853
replaced with revised version Wed, 6 Nov 2024 12:27:27 GMT   (28199kb,D)

Title: GVKF: Gaussian Voxel Kernel Functions for Highly Efficient Surface
  Reconstruction in Open Scenes
Authors: Gaochao Song, Chong Cheng, Hao Wang
Categories: cs.CV
Comments: NeurIPS 2024
\\ ( https://arxiv.org/abs/2411.01853 ,  28199kb)
------------------------------------------------------------------------------
\\
arXiv:2411.02188
replaced with revised version Wed, 6 Nov 2024 06:38:47 GMT   (6366kb,D)

Title: Digi2Real: Bridging the Realism Gap in Synthetic Data Face Recognition
  via Foundation Models
Authors: Anjith George and Sebastien Marcel
Categories: cs.CV
Comments: The dataset would be available here:
  https://www.idiap.ch/paper/digi2real
\\ ( https://arxiv.org/abs/2411.02188 ,  6366kb)
------------------------------------------------------------------------------
\\
arXiv:2411.02229
replaced with revised version Tue, 5 Nov 2024 19:06:16 GMT   (17151kb,D)

Title: FewViewGS: Gaussian Splatting with Few View Matching and Multi-stage
  Training
Authors: Ruihong Yin, Vladimir Yugay, Yue Li, Sezer Karaoglu, Theo Gevers
Categories: cs.CV
Comments: Accepted by NeurIPS2024
\\ ( https://arxiv.org/abs/2411.02229 ,  17151kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03313
replaced with revised version Wed, 6 Nov 2024 12:07:08 GMT   (1936kb,D)

Title: Classification Done Right for Vision-Language Pre-Training
Authors: Zilong Huang, Qinghao Ye, Bingyi Kang, Jiashi Feng, Haoqi Fan
Categories: cs.CV
Comments: NeurIPS 2024
\\ ( https://arxiv.org/abs/2411.03313 ,  1936kb)
------------------------------------------------------------------------------
\\
arXiv:2111.08248
replaced with revised version Wed, 6 Nov 2024 12:20:10 GMT   (7409kb,D)

Title: Active Vapor-Based Robotic Wiper
Authors: Takuya Kiyokawa, Hiroki Katayama, Jun Takamatsu, Kensuke Harada
Categories: cs.RO
Comments: 4 pages, 8 figures
\\ ( https://arxiv.org/abs/2111.08248 ,  7409kb)
------------------------------------------------------------------------------
\\
arXiv:2209.02849
replaced with revised version Wed, 6 Nov 2024 04:07:16 GMT   (15984kb,D)

Title: Adaptive Complexity Model Predictive Control
Authors: Joseph Norby, Ardalan Tajbakhsh, Yanhao Yang, and Aaron M. Johnson
Categories: cs.RO
Comments: Published in Transactions on Robotics
Journal-ref: vol. 40, 2024, pp. 4615-4634
DOI: 10.1109/TRO.2024.3410408
\\ ( https://arxiv.org/abs/2209.02849 ,  15984kb)
------------------------------------------------------------------------------
\\
arXiv:2301.06668
replaced with revised version Wed, 6 Nov 2024 16:23:12 GMT   (4280kb,D)

Title: UMIRobot: An Open-{Software, Hardware} Low-Cost Robotic Manipulator for
  Education
Authors: Murilo M. Marinho, Hung-Ching Lin, Jiawei Zhao
Categories: cs.RO
Comments: Accepted on IROS 2023, 8 pages. Fixed a few typos
Journal-ref: 2023 IEEE/RSJ International Conference on Intelligent Robots and
  Systems (IROS), Detroit, MI, USA, 2023, pp. 4464-4471
DOI: 10.1109/IROS55552.2023.10341347
\\ ( https://arxiv.org/abs/2301.06668 ,  4280kb)
------------------------------------------------------------------------------
\\
arXiv:2309.13171
replaced with revised version Wed, 6 Nov 2024 14:46:04 GMT   (8344kb,D)

Title: Robust Perception-Informed Navigation using PAC-NMPC with a Learned
  Value Function
Authors: Adam Polevoy, Mark Gonzales, Marin Kobilarov, Joseph Moore
Categories: cs.RO
Comments: This work has been submitted to the IEEE for possible publication
\\ ( https://arxiv.org/abs/2309.13171 ,  8344kb)
------------------------------------------------------------------------------
\\
arXiv:2402.02989
replaced with revised version Wed, 6 Nov 2024 16:33:29 GMT   (4816kb,D)

Title: DexDiffuser: Generating Dexterous Grasps with Diffusion Models
Authors: Zehang Weng, Haofei Lu, Danica Kragic, Jens Lundell
Categories: cs.RO cs.LG
Comments: 7 pages
\\ ( https://arxiv.org/abs/2402.02989 ,  4816kb)
------------------------------------------------------------------------------
\\
arXiv:2403.16859
replaced with revised version Tue, 5 Nov 2024 19:34:22 GMT   (1493kb,D)

Title: A Semi-Lagrangian Approach for Time and Energy Path Planning
  Optimization in Static Flow Fields
Authors: V\'ictor C. da S. Campos, Armando A. Neto, and Douglas G. Macharet
Categories: cs.RO cs.SY eess.SY
Comments: 50 pages, reviewed version; Preprint submitted to Journal of the
  Franklin Institute (under review)
\\ ( https://arxiv.org/abs/2403.16859 ,  1493kb)
------------------------------------------------------------------------------
\\
arXiv:2404.09080
replaced with revised version Wed, 6 Nov 2024 10:23:59 GMT   (8881kb,D)

Title: Safe Reinforcement Learning on the Constraint Manifold: Theory and
  Applications
Authors: Puze Liu, Haitham Bou-Ammar, Jan Peters, Davide Tateo
Categories: cs.RO cs.LG
Comments: 19 pages; sumitted to IEEE Transactions on Robotics
\\ ( https://arxiv.org/abs/2404.09080 ,  8881kb)
------------------------------------------------------------------------------
\\
arXiv:2410.06192
replaced with revised version Wed, 6 Nov 2024 14:29:21 GMT   (8906kb,D)

Title: Hibikino-Musashi@Home 2024 Team Description Paper
Authors: Kosei Isomoto and Akinobu Mizutani and Fumiya Matsuzaki and Hikaru
  Sato and Ikuya Matsumoto and Kosei Yamao and Takuya Kawabata and Tomoya Shiba
  and Yuga Yano and Atsuki Yokota and Daiju Kanaoka and Hiromasa Yamaguchi and
  Kazuya Murai and Kim Minje and Lu Shen and Mayo Suzuka and Moeno Anraku and
  Naoki Yamaguchi and Satsuki Fujimatsu and Shoshi Tokuno and Tadataka Mizo and
  Tomoaki Fujino and Yuuki Nakadera and Yuka Shishido and Yusuke Nakaoka and
  Yuichiro Tanaka and Takashi Morie and Hakaru Tamukoh
Categories: cs.RO
\\ ( https://arxiv.org/abs/2410.06192 ,  8906kb)
------------------------------------------------------------------------------
\\
arXiv:2410.21845
replaced with revised version Wed, 6 Nov 2024 03:14:14 GMT   (39202kb,D)

Title: Precise and Dexterous Robotic Manipulation via Human-in-the-Loop
  Reinforcement Learning
Authors: Jianlan Luo, Charles Xu, Jeffrey Wu, Sergey Levine
Categories: cs.RO cs.AI
\\ ( https://arxiv.org/abs/2410.21845 ,  39202kb)
------------------------------------------------------------------------------
\\
arXiv:2410.22997
replaced with revised version Wed, 6 Nov 2024 16:57:03 GMT   (12553kb,D)

Title: A Comparison of Prompt Engineering Techniques for Task Planning and
  Execution in Service Robotics
Authors: Jonas Bode, Bastian P\"atzold, Raphael Memmesheimer, Sven Behnke
Categories: cs.RO cs.AI
Comments: 6 pages, 3 figures, 2 tables, to be published in the 2024 IEEE-RAS
  International Conference on Humanoid Robots, We make our code, including all
  prompts, available at https://github.com/AIS-Bonn/Prompt_Engineering
\\ ( https://arxiv.org/abs/2410.22997 ,  12553kb)
------------------------------------------------------------------------------
\\
arXiv:2410.23643
replaced with revised version Wed, 6 Nov 2024 08:41:50 GMT   (8445kb,D)

Title: SceneComplete: Open-World 3D Scene Completion in Complex Real World
  Environments for Robot Manipulation
Authors: Aditya Agarwal, Gaurav Singh, Bipasha Sen, Tom\'as Lozano-P\'erez,
  Leslie Pack Kaelbling
Categories: cs.RO
\\ ( https://arxiv.org/abs/2410.23643 ,  8445kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03294
replaced with revised version Wed, 6 Nov 2024 17:53:26 GMT   (9344kb,D)

Title: Out-of-Distribution Recovery with Object-Centric Keypoint Inverse Policy
  For Visuomotor Imitation Learning
Authors: George Jiayuan Gao, Tianyu Li, Nadia Figueroa
Categories: cs.RO cs.AI
Comments: Accepted for Spotlight (5 out of 21 papers) at CoRL 2024 Workshop on
  Lifelong Learning for Home Robots
\\ ( https://arxiv.org/abs/2411.03294 ,  9344kb)
------------------------------------------------------------------------------
\\
arXiv:2309.08798
replaced with revised version Tue, 5 Nov 2024 21:10:54 GMT   (11740kb,D)

Title: D3: Data Diversity Design for Systematic Generalization in Visual
  Question Answering
Authors: Amir Rahimi, Vanessa D'Amario, Moyuru Yamada, Kentaro Takemoto,
  Tomotake Sasaki, Xavier Boix
Categories: cs.AI cs.CV
Comments: TMLR (https://openreview.net/forum?id=ZAin13msOp)
\\ ( https://arxiv.org/abs/2309.08798 ,  11740kb)
------------------------------------------------------------------------------
\\
arXiv:2401.08426
replaced with revised version Tue, 5 Nov 2024 19:57:19 GMT   (8859kb,D)

Title: GD doesn't make the cut: Three ways that non-differentiability affects
  neural network training
Authors: Siddharth Krishna Kumar
Categories: cs.LG cs.CV
\\ ( https://arxiv.org/abs/2401.08426 ,  8859kb)
------------------------------------------------------------------------------
\\
arXiv:2403.05408 (*cross-listing*)
replaced with revised version Wed, 6 Nov 2024 07:08:58 GMT   (4048kb,D)

Title: FedFMS: Exploring Federated Foundation Models for Medical Image
  Segmentation
Authors: Yuxi Liu, Guibo Luo and Yuesheng Zhu
Categories: eess.IV cs.CV cs.DC
Comments: Accepted by MICCAI'2024
ACM-class: I.4.6; I.2.11
\\ ( https://arxiv.org/abs/2403.05408 ,  4048kb)
------------------------------------------------------------------------------
\\
arXiv:2403.19863
replaced with revised version Wed, 6 Nov 2024 18:29:38 GMT   (11665kb,D)

Title: DeNetDM: Debiasing by Network Depth Modulation
Authors: Silpa Vadakkeeveetil Sreelatha, Adarsh Kappiyath, Abhra Chaudhuri,
  Anjan Dutta
Categories: cs.LG cs.CV
Comments: Camera-ready version : NeurIPS 2024, * indicates these authors
  contributed equally
\\ ( https://arxiv.org/abs/2403.19863 ,  11665kb)
------------------------------------------------------------------------------
\\
arXiv:2405.07001
replaced with revised version Wed, 6 Nov 2024 13:56:28 GMT   (38543kb,D)

Title: ChartInsights: Evaluating Multimodal Large Language Models for Low-Level
  Chart Question Answering
Authors: Yifan Wu, Lutao Yan, Leixian Shen, Yunhai Wang, Nan Tang, Yuyu Luo
Categories: cs.CL cs.AI cs.CV
Comments: EMNLP 2024 Conference Paper
\\ ( https://arxiv.org/abs/2405.07001 ,  38543kb)
------------------------------------------------------------------------------
\\
arXiv:2405.15306
replaced with revised version Wed, 6 Nov 2024 09:49:31 GMT   (7377kb,D)

Title: DeTikZify: Synthesizing Graphics Programs for Scientific Figures and
  Sketches with TikZ
Authors: Jonas Belouadi, Simone Paolo Ponzetto, Steffen Eger
Categories: cs.CL cs.CV
Comments: Accepted at NeurIPS 2024 (spotlight); Project page:
  https://github.com/potamides/DeTikZify
\\ ( https://arxiv.org/abs/2405.15306 ,  7377kb)
------------------------------------------------------------------------------
\\
arXiv:2405.16749
replaced with revised version Wed, 6 Nov 2024 16:55:39 GMT   (91440kb,D)

Title: DMPlug: A Plug-in Method for Solving Inverse Problems with Diffusion
  Models
Authors: Hengkang Wang, Xu Zhang, Taihui Li, Yuxiang Wan, Tiancong Chen, Ju Sun
Categories: cs.LG cs.CV
Comments: Published in NeurIPS 2024
  (https://openreview.net/forum?id=81IFFsfQUj)
\\ ( https://arxiv.org/abs/2405.16749 ,  91440kb)
------------------------------------------------------------------------------
\\
arXiv:2405.17537
replaced with revised version Wed, 6 Nov 2024 15:56:04 GMT   (27878kb,D)

Title: CLIBD: Bridging Vision and Genomics for Biodiversity Monitoring at Scale
Authors: ZeMing Gong, Austin T. Wang, Xiaoliang Huo, Joakim Bruslund Haurum,
  Scott C. Lowe, Graham W. Taylor, Angel X. Chang
Categories: cs.AI cs.CL cs.CV
Comments: 25 pages with 11 figures
\\ ( https://arxiv.org/abs/2405.17537 ,  27878kb)
------------------------------------------------------------------------------
\\
arXiv:2406.04090
replaced with revised version Tue, 5 Nov 2024 20:51:06 GMT   (985kb,D)

Title: Interpretable Lightweight Transformer via Unrolling of Learned Graph
  Smoothness Priors
Authors: Tam Thuc Do, Parham Eftekhar, Seyed Alireza Hosseini, Gene Cheung,
  Philip Chou
Categories: cs.LG cs.CV eess.IV eess.SP
\\ ( https://arxiv.org/abs/2406.04090 ,  985kb)
------------------------------------------------------------------------------
\\
arXiv:2408.07079 (*cross-listing*)
replaced with revised version Tue, 5 Nov 2024 19:44:03 GMT   (1974kb,D)

Title: Anatomical Foundation Models for Brain MRIs
Authors: Carlo Alberto Barbano, Matteo Brunello, Benoit Dufumier, Marco
  Grangetto
Categories: eess.IV cs.AI cs.CV cs.LG
Comments: 12 pages; added source url
MSC-class: 68T07
ACM-class: I.2.6
\\ ( https://arxiv.org/abs/2408.07079 ,  1974kb)
------------------------------------------------------------------------------
\\
arXiv:2408.07832
replaced with revised version Tue, 5 Nov 2024 23:50:14 GMT   (64960kb,D)

Title: LADDER: Language Driven Slice Discovery and Error Rectification
Authors: Shantanu Ghosh, Rayan Syed, Chenyu Wang, Clare B. Poynton, Shyam
  Visweswaran, Kayhan Batmanghelich
Categories: cs.CL cs.CV
\\ ( https://arxiv.org/abs/2408.07832 ,  64960kb)
------------------------------------------------------------------------------
\\
arXiv:2408.13800 (*cross-listing*)
replaced with revised version Wed, 6 Nov 2024 12:10:54 GMT   (756kb,D)

Title: BCDNet: A Fast Residual Neural Network For Invasive Ductal Carcinoma
  Detection
Authors: Yujia Lin, Aiwei Lian, Mingyu Liao and Shuangjie Yuan
Categories: eess.IV cs.CV
Comments: 5 pages, 3 figures
\\ ( https://arxiv.org/abs/2408.13800 ,  756kb)
------------------------------------------------------------------------------
\\
arXiv:2410.13147
replaced with revised version Wed, 6 Nov 2024 05:18:04 GMT   (141kb,D)

Title: Utilizing Large Language Models in an iterative paradigm with Domain
  feedback for Zero-shot Molecule optimization
Authors: Khiem Le, Nitesh V. Chawla
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2410.13147 ,  141kb)
------------------------------------------------------------------------------
\\
arXiv:2410.21169
replaced with revised version Wed, 6 Nov 2024 00:11:08 GMT   (1803kb,D)

Title: Document Parsing Unveiled: Techniques, Challenges, and Prospects for
  Structured Information Extraction
Authors: Qintong Zhang, Victor Shea-Jay Huang, Bin Wang, Junyuan Zhang,
  Zhengren Wang, Hao Liang, Shawn Wang, Matthieu Lin, Conghui He, Wentao Zhang
Categories: cs.MM cs.AI cs.CL cs.CV
\\ ( https://arxiv.org/abs/2410.21169 ,  1803kb)
------------------------------------------------------------------------------
\\
arXiv:2410.23247 (*cross-listing*)
replaced with revised version Wed, 6 Nov 2024 17:07:53 GMT   (18394kb,D)

Title: bit2bit: 1-bit quanta video reconstruction via self-supervised photon
  prediction
Authors: Yehe Liu, Alexander Krull, Hector Basevi, Ales Leonardis, Michael W.
  Jenkins
Categories: eess.IV cs.CV cs.LG
Comments: NeurIPS 2024
MSC-class: 68T45
ACM-class: I.2.10
\\ ( https://arxiv.org/abs/2410.23247 ,  18394kb)
------------------------------------------------------------------------------
\\
arXiv:2411.00393
replaced with revised version Wed, 6 Nov 2024 13:25:42 GMT   (2137kb,D)

Title: Advantages of Neural Population Coding for Deep Learning
Authors: Heiko Hoffmann
Categories: cs.LG cs.AI cs.CV
\\ ( https://arxiv.org/abs/2411.00393 ,  2137kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03055
replaced with revised version Wed, 6 Nov 2024 13:24:10 GMT   (3330kb,D)

Title: ATM: Improving Model Merging by Alternating Tuning and Merging
Authors: Luca Zhou, Daniele Solombrino, Donato Crisostomi, Maria Sofia
  Bucarelli, Fabrizio Silvestri, Emanuele Rodol\`a
Categories: cs.LG cs.AI cs.CV
Comments: Main paper: 10 Pages, 11 figures, 2 tables
\\ ( https://arxiv.org/abs/2411.03055 ,  3330kb)
------------------------------------------------------------------------------
\\
arXiv:2411.03223
replaced with revised version Wed, 6 Nov 2024 09:10:46 GMT   (4364kb,D)

Title: Beyond Grid Data: Exploring Graph Neural Networks for Earth Observation
Authors: Shan Zhao, Zhaiyu Chen, Zhitong Xiong, Yilei Shi, Sudipan Saha, Xiao
  Xiang Zhu
Categories: cs.LG cs.AI cs.CV
Comments: Accepted for publication in Geoscience and Remote Sensing Magazine
  (GRSM)
\\ ( https://arxiv.org/abs/2411.03223 ,  4364kb)
------------------------------------------------------------------------------
\\
arXiv:2011.02073
replaced with revised version Wed, 6 Nov 2024 01:14:09 GMT   (11061kb,D)

Title: Optimal Control-Based Baseline for Guided Exploration in Policy Gradient
  Methods
Authors: Xubo Lyu, Site Li, Seth Siriya, Ye Pu, Mo Chen
Categories: cs.LG cs.AI cs.RO cs.SY eess.SY
\\ ( https://arxiv.org/abs/2011.02073 ,  11061kb)
------------------------------------------------------------------------------
\\
arXiv:2110.00675
replaced with revised version Wed, 6 Nov 2024 03:29:03 GMT   (8866kb,D)

Title: Contraction Theory for Nonlinear Stability Analysis and Learning-based
  Control: A Tutorial Overview
Authors: Hiroyasu Tsukamoto and Soon-Jo Chung and Jean-Jacques E. Slotine
Categories: cs.LG cs.RO cs.SY eess.SY math.OC
Comments: Annual Reviews in Control, Accepted, Oct. 1st
Journal-ref: Annual Reviews in Control; Volume 52; 2021; Pages 135-169; ISSN
  1367-5788,
DOI: 10.1016/j.arcontrol.2021.10.001
\\ ( https://arxiv.org/abs/2110.00675 ,  8866kb)
------------------------------------------------------------------------------
\\
arXiv:2406.03845
replaced with revised version Wed, 6 Nov 2024 14:11:05 GMT   (1864kb,D)

Title: Open Problem: Active Representation Learning
Authors: Nikola Milosevic, Gesine M\"uller, Jan Huisken, Nico Scherf
Categories: cs.LG cs.RO cs.SY eess.SY
\\ ( https://arxiv.org/abs/2406.03845 ,  1864kb)
------------------------------------------------------------------------------
\\
arXiv:2406.04815
replaced with revised version Wed, 6 Nov 2024 13:24:41 GMT   (19862kb,D)

Title: Skill-aware Mutual Information Optimisation for Generalisation in
  Reinforcement Learning
Authors: Xuehui Yu, Mhairi Dunion, Xin Li, Stefano V. Albrecht
Categories: cs.LG cs.AI cs.RO
Comments: The Thirty-eighth Annual Conference on Neural Information Processing
  Systems (NeurIPS), 2024
\\ ( https://arxiv.org/abs/2406.04815 ,  19862kb)
%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---%%%---
